---
title: "Debugging"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

# Simulation data

```{r simulation data 1, eval=TRUE}
library(compositions)
library(mvtnorm)
library(ggplot2)
library(tidyverse)
library(targets)
library(patchwork)
source("scripts/helper_functions.R")
# Create custom basis vectors that represent our desired contrasts
v1 <- c(0, 1/sqrt(2), -1/sqrt(2), 0, 0)  # Contrast between parts 2 and 3
v2 <- c(-1/3, -1/3, -1/3, 1, 0)  # Focus on part 4
v3 <- c(1/sqrt(2), 0, 0, -1/sqrt(2), 0)  # Additional contrast
v4 <- c(1/2, 1/2, 0, 0, -1)  # Additional contrast

# Combine into rotation matrix with each column representing a base
V <- cbind(v1, v2, v3, v4)

# Define eigenvalues to control strength of each pattern
eigenvalues <- c(0.6, 0.3, 0.05, 0.05)

# Construct covariance matrix
K <- V %*% diag(eigenvalues) %*% t(V)

```

```{r compute x_i, eval=TRUE}

# Generate samples
n_samples <- 500
mean <- c(0,2,0.5, -2, -0.5)
set.seed(123)
clr_coords <- rmvnorm(n_samples, mean = mean, sigma = K)
ilr_coords <- clr2ilr(clr_coords)

# Transform to compositions
compositions <- clrInv(clr_coords)
composition_list <- apply(compositions, 1, function(x) x, simplify = FALSE)

decomp_k <- prcomp(K)
plot_pca_rotation(decomp_k$rotation)

# Number of counts per sample
n_counts <- 2000

# Generate multinomial samples

x_data <- lapply(1:n_samples, function(i) {
  probs <- composition_list[[i]]
  rmultinom(1, n_counts, probs)[,1]
})
x_data_matrix <- do.call(rbind, x_data)
```

## targets

```{r targets, eval=TRUE}
sim1 <- tar_read(sim_comp_1)
x_sim1 <- sim1$x_data
x_counts <- tar_read(data_kl15_comp)
# make a list out of a matrix
x_cts_list <- apply(x_counts, 1, function(x) x, simplify = FALSE)
```

## Standard pca

```{r standard pca, eval=TRUE}
pca_clr <- prcomp(clr(x_data_matrix))
pca_ilr <- prcomp(ilr(x_data_matrix))

pca_clr_cts <- prcomp(clr(x_counts))
pca_ilr_cts <- prcomp(ilr(x_counts))

# initial estimates
D <- length(x_data[[1]])

basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

# initial estimates
nu <- rep(0, D - 1)
Sigma <- diag(D - 1)
pca_init <- prcomp(Sigma, center = FALSE)
pca_init$center <- nu
```

# Density-Implementierung

```{r fit density pca, eval=TRUE}
fit_density_pca <- function(x_data, x_grid = seq(min(unlist(x_data)), max(unlist(x_data)), length = 200),
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            bw = (max(x_grid) - min(x_grid))/10, eps = 0.01){
  # initial estimates
  # kernel density estimates
  densities_estimated <- lapply(1:length(x_data), function(i){
    density <- density(x_data[[i]], from = min(x_grid), to = max(x_grid), 
                       kernel = "gaussian", bw, 
                       n = length(x_grid))
    data.frame("x" = density$x, "y" = density$y)
  })
  # compute initial pca
  clr_densities_estimated <- lapply(densities_estimated, clr_trafo)
  clr_densities <- do.call("rbind", sapply(clr_densities_estimated, '[', 2))
  pca <- prcomp(na.omit(clr_densities))
  which_reduced <- rev(cumsum(rev(pca$sdev^2))/sum(pca$sdev^2) > dim_reduction)
  which_reduced <- which_reduced|c(TRUE, TRUE, rep(FALSE, length(which_reduced) - 2))
  pca$sdev <- pca$sdev[which_reduced]
  pca$rotation <- pca$rotation[,which_reduced, drop = FALSE]
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      # draw densities conditional on observations and current pca
      for(i in 1:nrow(clr_densities)){
        # find median of the posterior score distribution
        optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                              x_grid = x_grid, x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          # Formular 8
          conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      
      # update pca
      pca_old <- pca
      pca$center <- center_function(cbind(x_grid, pca$center + pca$rotation%*%mu_scores))[,2]
      
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      eigen_decomp <- eigen(Sigma)
      pca$sdev <- sqrt(eigen_decomp$values)
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      pca$rotation <- apply(pca$rotation, 2, function(g) center_function(cbind(x_grid, g))[,2])
      
      # check convergence
      critical_value_1 <- L_2_norm(cbind(x_grid, pca_old$center - pca$center))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      critical_value_2 <- L_2_norm(cbind(x_grid, sapply(1:nrow(K_old), function(k){
        L_2_norm(cbind(x_grid, K_old[k,] - K_new[k,]))
      })))
      
      if(max(critical_value_1, critical_value_2) < eps){
        # normalize result
        constant <- apply(pca$rotation, 2,  function(g){
          L_2_norm(cbind(x_grid, g))
        })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        return(list("iteration" = k, "pca" = pca, "x_grid" = x_grid, "x_data" = x_data))
      }
      which_reduced <- rev(cumsum(rev(pca$sdev^2))/sum(pca$sdev^2) > dim_reduction)
      which_reduced <- which_reduced|c(TRUE, TRUE, rep(FALSE, length(which_reduced) - 2))
      pca$sdev <- pca$sdev[which_reduced]
      pca$rotation <- pca$rotation[,which_reduced, drop = FALSE]
    }
  }
  # normalize result
  constant <- apply(pca$rotation, 2,  function(g){
    L_2_norm(cbind(x_grid, g))/(max(x_grid) - min(x_grid))
  })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_grid" = x_grid, "x_data" = x_data))
}

################################################################################
# objective function and gradient
conditional_scores_log_density <- function(scores, x_grid, x_data_i, pca){
  clr_density <- cbind(x_grid, pca$center + pca$rotation%*%scores)
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  
  sum(clr_density[idxs, 2]) - length(idxs)*log(f_integral) - sum(0.5*scores^2/(pca$sdev^2))
}

gradient_csld <- function(scores, x_grid, x_data_i, pca){
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  density <- inverse_clr_trafo(cbind(x_grid, pca$center + pca$rotation%*%scores))
  
  sapply(seq_along(scores), function(k){
    scalar_prod <- sum(density[,2]*pca$rotation[, k]*diff(mid_points))
    sum(pca$rotation[idxs, k]) - length(idxs)*scalar_prod - scores[k]/(pca$sdev[k]^2)
  })
}

################################################################################
# helper functions
center_function <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  f_integral <- sum(f_data[,2]*diff(mid_points))
  f_data[,2] <- f_data[,2] - f_integral/(mid_points[length(mid_points)] - mid_points[1])
  f_data
}
L_2_norm <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  sqrt(sum(f_data[,2]^2*diff(mid_points)))
}

clr_trafo <- function(f_data){
  f_data[,2] <- log(f_data[,2])
  center_function(f_data)
}

inverse_clr_trafo <- function(clr_density){
  mid_points <- c(clr_density[1, 1], clr_density[-1, 1] - 0.5*diff(clr_density[,1]),
                  clr_density[nrow(clr_density),1])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

predict_latent_densities <- function(density_pca){
  predicted_scores <- sapply(seq_along(density_pca$x_data), function(i){
    optim_result <- optim(rep(0, length = length(density_pca$pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                          x_grid = density_pca$x_grid, x_data_i = density_pca$x_data[[i]], pca = density_pca$pca,
                          control = list(fnscale = -1), method = "BFGS")
    as.vector(optim_result$par)
  })
  clr_densities <- density_pca$pca$center + density_pca$pca$rotation%*%predicted_scores
  return(list("clr_densities" = clr_densities, "x_grid" = density_pca$x_grid, "predicted_scores" = predicted_scores))
}

################################################################################
# plot functions

# plot all functional principal components
plot_pca <- function(pca, x_grid, dim = 2){
  pca_mean <- data.frame("x" = x_grid, "y" = pca$center)
  pcs <- lapply(1:dim, function(i){
    data.frame("x" = x_grid, "y" = pca$rotation[,i])
  })
  y_lim <- range(rbind(pca_mean, do.call("rbind", pcs))[,2])
  plot_function <- function() {
    plot(pca_mean, ylim = y_lim, type = "l", main = "Principal component decomposition")
    invisible(lapply(1:dim, function(i) lines(pcs[[i]], col = rainbow(dim)[i])))
    legend("bottom", legend = c("mean", paste("pc", 1:dim)), col = c("black", rainbow(dim)),
          lty = 1)
  }

  return(plot_function)
}

# obtain data to plot effect of one principal component on mean density
get_predicted_densities <- function(density_pca, idx = 1, fac = 0.2){
  clr_mean <- density_pca$pca$center
  clr_mean_minus <- clr_mean - fac*density_pca$pca$rotation[,idx]
  clr_mean_plus <- clr_mean + fac*density_pca$pca$rotation[,idx]
  
}

################################################################################
# Simulation Function
simulate_densities_greven <- function(n_data, n_samples, x_grid, lambda_1, lambda_2){
set.seed(12)
f_data <- data.frame("x" = x_grid, "y" = -20*(x_grid-0.5)^2+5/3)
clr_mean <- center_function(f_data)

pc_1 <- data.frame("x" = x_grid, "y" = -0.2*sin(10*(x_grid-0.5)))
pc_1 <- center_function(pc_1)
pc_1[,2] <- pc_1[,2]/L_2_norm(pc_1)

pc_2 <- data.frame("x" = x_grid, "y" = 0.1*cos(2*pi*(x_grid-0.5)))
pc_2 <- center_function(pc_2)
pc_2[,2] <- pc_2[,2]/L_2_norm(pc_2)

true_observed_clr_densities <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})
true_observed_densities <- lapply(1:n_data, function(i){
  clr_density <- data.frame(x_grid, true_observed_clr_densities[,i])
  inverse_clr_trafo(clr_density)
})

x_data_densities <- lapply(1:n_data, function(i){
  probs <- true_observed_densities[[i]][,2]
  x_grid <- true_observed_densities[[i]][,1]
  sample(x_grid, n_samples, replace = TRUE, prob = probs)
})

return(list("x_data_densities" = x_data_densities, "true_observed_densities" = true_observed_densities, "clr_mean" = clr_mean, "pc_1" = pc_1, "pc_2" = pc_2, "x_grid" = x_grid))
}
```

## Simulation setting

```{r simulation_setting for density, eval=TRUE}
######################################
x_grid <- seq(-5,5,0.1)
# x_grid <- seq(0,1,0.01)
#f_data <- data.frame("x" = x_grid, "y" = -0.2*x_grid^2)
f_data <- data.frame("x" = x_grid, "y" = -20*(x_grid-0.5)^2+5/3)
clr_mean <- center_function(f_data)

# pc_1 <- data.frame("x" = x_grid, "y" = 2*sin(x_grid))
pc_1 <- data.frame("x" = x_grid, "y" = -0.2*sin(10*(x_grid-0.5)))
pc_1 <- center_function(pc_1)
pc_1[,2] <- pc_1[,2]/L_2_norm(pc_1)
#####
# pc_2 <- data.frame("x" = x_grid, "y" = cos(0.3*x_grid))
pc_2 <- data.frame("x" = x_grid, "y" = 0.1*cos(2*pi*(x_grid-0.5)))
pc_2 <- center_function(pc_2)
pc_2[,2] <- pc_2[,2]/L_2_norm(pc_2)
lambda_1 <- 0.5
lambda_2 <- 0.2

# true densities
set.seed(12)
n_data <- 30
true_observed_clr_densities <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})
true_observed_densities <- lapply(1:n_data, function(i){
  clr_density <- data.frame(x_grid, true_observed_clr_densities[,i])
  inverse_clr_trafo(clr_density)
})

# draw data
x_data_densities <- lapply(1:n_data, function(i){
  probs <- true_observed_densities[[i]][,2]
  x_grid <- true_observed_densities[[i]][,1]
  sample(x_grid, 40, replace = TRUE, prob = probs)
})



```


## Gradient descent

### Gradient

### numerical

## Conditional Score 


# Clr-Implementierung

```{r fit fit_compositional_pca_vs1_0, eval=TRUE}
fit_compositional_pca_vs1_0 <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){
  start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  # initial estimates
  D <- length(x_data[[1]])
  nu <- rep(0, D)
  Sigma <- diag(D)
  # compute initial pca
  pca <- prcomp(Sigma)
  pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
  pca$center <- nu
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                              x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_clr_composition(scores, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
      }
      if (any(!is.finite(weights[[i]]))) {
         stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation%*%mu_scores
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)

      eigen_decomp <- eigen(Sigma)
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        # standard decomposition VLV
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data))
}

conditional_scores_log_clr_composition <- function(scores, x_data_i, pca){
clr_comp <- pca$center + pca$rotation%*%scores
norm_constant <- sum(exp(clr_comp))
log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant)
log_prior <- - sum(0.5*scores^2/(pca$sdev^2))
log_posterior <- log_likelihood + log_prior

return(log_posterior) 
}

gradient_cslc <- function(scores, x_data_i, pca){

  m_i <- sum(x_data_i)

  composition <- clrInv(pca$center + pca$rotation%*%scores)

  grad <- sapply(seq_along(scores), function(k) {
    e_k <- pca$rotation[, k]

    term1 <- sum(x_data_i * e_k)

    term2 <- m_i * sum(composition * e_k)

    grad_k <- term1 - term2 - scores[k] / (pca$sdev[k]^2)

    return(grad_k)
})

return(grad)
}


############ Simulate composition data ############ 
simulate_composition_1 <- function(n_components, n_data, n_counts, n_samples, lambda_1, lambda_2){
set.seed(123)
x_grid <- seq(1, n_components)
raw_data <- data.frame("x" = x_grid, "y" = rep(0, n_components))

clr_mean <- center_function_comp(raw_data)

pc_1 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(5/(5+1)*5^(-1)), 5),-1, rep(0, 7)))
pc_1 <- center_function_comp(pc_1)
pc_1[, 2] <- pc_1[, 2] / norm(pc_1[, 2], type = "2")

pc_2 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(12/(12+1)*12^(-1)), 12),0))
pc_2 <- center_function_comp(pc_2)
pc_2[, 2] <- pc_2[,2]/norm(pc_2[,2], type="2")

true_observed_clr_comp <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})

check_columns_sum_to(true_observed_clr_comp, 0.001)

true_observed_comp <- lapply(1:n_data, function(i){
  # TODO: remove x_grid
  clr_density <- data.frame(x_grid, true_observed_clr_comp[,i])
  inverse_clr_trafo(clr_density)
})

x_data <- unlist(lapply(1:n_data, function(i) {
  probs <- true_observed_comp[[i]][,2]
  samples <- t(rmultinom(n_samples, n_counts, probs))
  lapply(1:nrow(samples), function(j) samples[j,])
}), recursive = FALSE)

x_data_matrix <- do.call(rbind, x_data)

return(list(x_data = x_data, x_data_matrix = x_data_matrix, true_observed_comp = true_observed_comp))
}

center_function_comp <- function(comp_data){
  mean <- mean(comp_data[,2])
  comp_data[,2] <- comp_data[,2] - mean
  comp_data
}

check_columns_sum_to <- function(data, integer) {
  n_cols <- ncol(data)
  
  columns_sum_to_zero <- logical(n_cols)
  
  for (i in 1:n_cols) {
    column_sum <- sum(data[, i])
    columns_sum_to_zero[i] <- (column_sum < integer)
  }
  
  return(columns_sum_to_zero)
}

inverse_clr_trafo <- function(clr_density){
  f_integral <- sum(exp(clr_density[,2]))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

```

## Simulation setting

## Gradient descent

### Gradient

### numerical

## Conditional Score 

## Parameter choice

# Ilr-Implementierung

```{r fit compositional_pca_ilr_sc, eval=TRUE}
source("scripts/helper_functions.R")

# version mit ESS
fit_pca_vs_5 <- function(x_data,
                         max_iter = 50,
                         r = 10,
                         lambda = 1,
                         eps = 0.01,
                         sc_factor = 1,
                         scores = TRUE,
                         sum_exp = TRUE,
                         fix_sign = TRUE) {
  start_time <- Sys.time()
  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }
  if (is.list(x_data)) {
    x_df <- do.call(rbind, x_data)
  }
  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_df <- x_data
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }
  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  V <- get_helmert(x_df)
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))

  if (max_iter > 0) {
    for (k in 1L:max_iter) {
      for (i in seq_along(x_data)){
        optim_result <- optim(rep(0L, length = length(pca$sdev)),
                              conditional_scores_log_ilr,
                              gr = gradient_cslc_vs1,
                              x_data_i = x_data[[i]],
                              pca = pca,
                              basis_matrix = t(V),
                              sc_factor = sc_factor,
                              control = list(fnscale = -1),
                              method = "BFGS")

        scores_median <- as.vector(optim_result$par)
        proposal_scores[[i]] <- sapply(1L:(r * k), function(t) {
          matrix(rnorm(length(scores_median),
                       mean = scores_median,
                       sd = lambda * pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2L, function(scores) {
          conditional_scores_log_ilr(scores,
                                     x_data[[i]],
                                     pca,
                                     basis_matrix = t(V),
                                     sc_factor) -
            sum(dnorm(scores,
                      mean = scores_median,
                      sd = lambda * pca$sdev,
                      log = TRUE))
        })
        if (sum_exp == TRUE) {
          weights[[i]] <- stabilize_weights(log_weights)
        } else {
          max_log_weight <- max(log_weights)
          weights <- exp(log_weights - max_log_weight)
          weights[[i]] <- weights / sum(weights)
        }
      }
      # M-Step ###################
      scores_matrix <- sapply(seq_along(weights), function(i){
        proposal_scores[[i]] %*% weights[[i]]
      })
      mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation %*% mu_scores
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
        Reduce("+", lapply(1L:(r * k), function(t) {
          C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
            t((proposal_scores[[i]][, t] - mu_scores))
        }))
      })) / length(weights)
      edc <-  tryCatch({eigen(Sigma, symmetric = TRUE)}, error = function(e) {
        cat("error eigen() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
      })
      edc$vectors <- fix_sign(edc$vectors)
      ev <- edc$values
      if (any(neg <- ev < 0)) {
        if (any(ev[neg] < -9 * .Machine$double.eps * ev[1L])) 
          stop("covariance matrix is not non-negative definite")
        else ev[neg] <- 0
      }
      pca$sdev <- sqrt(edc$values)
      sdev_list[[k]] <- pca$sdev
      # TODO: Das ist der entscheidende Teil für die Konvergenz: herausarbeiten! -> wird hier eigentlich auch eine Vorzeichenkorrektur benötigt?
      pca$rotation <- pca$rotation %*% edc$vectors
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
        pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
      }))
      Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
        pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
      }))
      Sigma_diff <- Sigma_old - Sigma_new
      critical_value_2 <- norm(Sigma_diff, type = "F")
      if (max(critical_value_1, critical_value_2) < eps) {
        constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
        pca$rotation <- t(t(pca$rotation) / constant)
        pca$rotation_ilr <- pca$rotation
        pca$rotation <- V %*% pca$rotation_ilr
        pca$sdev <- pca$sdev / sum(pca$sdev)
        pca$scores_clr <- if (scores) {
          scale(clr(x_data),
                center = pca$center, scale = FALSE) %*% pca$rotation
        }
        rownames(pca$rotation) <- names(x_data[[1L]])
        cn <- paste0("Comp.", seq_len(ncol(pca$rotation)))
        colnames(pca$rotation) <- cn
        mean_ess <- mean(sapply(weights, function(w) 1 / sum(w^2)))
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, attr(elapsed_time, "units")))
        return(list("iteration" = k,
                    "pca" = pca,
                    "x_data" = x_data,
                    "time" = elapsed_time,
                    "ESS" = mean_ess))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) {
    sqrt(sum(g^2))
  })
  pca$rotation <- t(t(pca$rotation) / constant)
  pca$rotation_ilr <- pca$rotation
  pca$rotation <- V %*% pca$rotation_ilr
  pca$sdev <- pca$sdev / sum(pca$sdev)
  pca$scores_clr <- if (scores) {
    scale(clr(x_data),
          center = pca$center, scale = FALSE) %*% pca$rotation
  }
  rownames(pca$rotation) <- names(x_data[[1L]])
  cn <- paste0("Comp.", seq_len(ncol(pca$rotation)))
  colnames(pca$rotation) <- cn
  mean_ess <- mean(sapply(weights, function(w) 1 / sum(w^2)))
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter,
              "pca" = pca,
              "x_data" = x_data,
              "time" = elapsed_time,
              "ESS" = mean_ess))
}



fit_pca_ilr_vs_2_b <- function(x_data,
                             max_iter = 50,
                             r = 10,
                             lambda = 1,
                             eps = 0.01,
                             sc_factor = 0.001,
                             workers = 4) {
  start_time <- Sys.time()
  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }

  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }

  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  scores_median_list <- list(length(x_data))

  # Set up parallel workers
  plan(multisession, workers = workers)

  if (max_iter > 0) {
    for (k in 1:max_iter) {
      cat("Iteration:", k, "\n")

      # Parallel E-Step
      estep_results <- parallel_estep(x_data, pca, basis_matrix, r, k, lambda, sc_factor)

      # Extract results
      proposal_scores <- lapply(estep_results, `[[`, "proposal_scores")
      weights <- lapply(estep_results, `[[`, "weights")

      monitor_global_ess(weights, k)

      # M-Step ###################
      scores_matrix <- sapply(seq_along(weights), function(i){
          proposal_scores[[i]] %*% weights[[i]]
      })
      na_count <- sum(is.na(scores_matrix))  
      mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)  
      cat(sprintf("Removed %d NA values when calculating mu_scores\n", na_count))      
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation %*% mu_scores
      cat("center:", pca$center, "\n")
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
          Reduce("+", lapply(1:(r * k), function(t) {
          C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
              t((proposal_scores[[i]][, t] - mu_scores))
          }))
      })) / length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
          cat("error eigen() in iteration", k, "for observation", i, "\n")
          cat("error message:", e$message, "\n")
          print("pca$sdev:")
          print(pca$sdev)
      })
      negative_eigenvalues <- eigen_decomp$values < 0
      if (any(negative_eigenvalues)) {
          warning(sprintf("Warning: %d eigenvalues are negative.\n
          They have been set to zero.",
                          sum(negative_eigenvalues)))
      }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      pca$rotation <- pca$rotation %*% eigen_decomp$vectors
      clr_rotation <- t(basis_matrix) %*% pca$rotation %*% basis_matrix
      cat("clr-coordinates PCA1:", clr_rotation[ , 1], "\n")
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value center_diff:", critical_value_1, "\n")
      Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
          pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
      }))
      Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
        pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
      }))
      Sigma_diff <- Sigma_old - Sigma_new
      critical_value_2 <- norm(Sigma_diff, type = "F")
      cat("critical value Sigma_diff:", critical_value_2, "\n")  
      if (max(critical_value_1, critical_value_2) < eps) {
        constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
        pca$rotation <- t(t(pca$rotation) / constant)
        pca$sdev <- pca$sdev * constant  
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "minutes"))
        return(list("iteration" = k,
                    "pca" = pca,
                    "x_data" = x_data,
                    "list_center" = center_list,
                    "list_sdev" = sdev_list,
                    "time" = elapsed_time))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) {
    sqrt(sum(g^2))
  })
  pca$rotation <- t(t(pca$rotation) / constant)
  pca$sdev <- pca$sdev * constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter,
              "pca" = pca,
              "x_data" = x_data,
              "list_center" = center_list,
              "list_sdev" = sdev_list,
              "time" = elapsed_time))
}

parallel_estep <- function(x_data, pca, basis_matrix, r, k, lambda, sc_factor) {
  future_lapply(seq_along(x_data), function(i) {
    optim_result <- optim(rep(0, length = length(pca$sdev)),
                          conditional_scores_log_ilr,
                          gr = gradient_cslc_vs1,
                          x_data_i = x_data[[i]],
                          pca = pca,
                          basis_matrix = basis_matrix,
                          sc_factor = sc_factor,
                          control = list(fnscale = -1),
                          method = "BFGS")

    scores_median <- as.vector(optim_result$par)
    proposal_scores <- sapply(1:(r * k), function(t) {
      matrix(rnorm(length(scores_median),
                   mean = scores_median,
                   sd = lambda * pca$sdev))
    })

    log_weights <- apply(proposal_scores, 2, function(scores) {
      conditional_scores_log_ilr(scores,
                                      x_data[[i]],
                                      pca,
                                      basis_matrix,
                                      sc_factor) -
        sum(dnorm(scores,
                  mean = scores_median,
                  sd = lambda * pca$sdev,
                  log = TRUE))
    })

    weights <- stabilize_weights(log_weights)

    list(proposal_scores = proposal_scores,
         weights = weights)
  }, future.seed = TRUE)
}


conditional_scores_log_ilr <- function(scores,
                                          x_data_i,
                                          pca,
                                          basis_matrix,
                                          sc_factor) {
  scaling_factor <- sc_factor
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  norm_constant <- sum(exp(clr_comp))

  # Compute scaled log likelihood
  log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i) * log(norm_constant)
  # apply scaling when sc_factor ungleich 1
  if (scaling_factor != 1) {
    log_likelihood <- log_likelihood - scaling_factor
  }

  # Prior remains unchanged as it's already well-scaled
  log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))

  return(log_likelihood + log_prior)
}

gradient_cslc_vs1 <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix,
                                 sc_factor) {
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- v_k * (term1 - term2) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores / (pca$sdev^2)

  return(grad)
}

gradient_cslc_vs2 <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix,
                                 sc_factor) {
  scaling_factor <- sc_factor
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * v_k * (term1 - term2) - scores[k] / (pca$sdev[k]^2)

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  return(grad)
}


clrInv_long <- function(clr_coords) {
    # Exponentiate the clr coordinates
    exp_coords <- exp(clr_coords)
    
    # Calculate the geometric mean normalization constant
    norm_const <- sum(exp_coords)
    
    # Return normalized compositions
    exp_coords / norm_const
}

```


## Simulation setting

## Debug functions

```{r test objective and gradient, eval=FALSE}
conditional_scores_log_ilr_db <- function(scores,
                                          x_data_i,
                                          pca,
                                          basis_matrix,
                                          sc_factor) {
  scaling_factor <- sc_factor
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  norm_constant <- sum(exp(clr_comp))

  # Compute scaled log likelihood
  log_likelihood <- scaling_factor * sum(x_data_i * clr_comp)
                                      - sum(x_data_i) * log(norm_constant)

  # Prior remains unchanged as it's already well-scaled
  log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))

  return(log_likelihood + log_prior)
}

conditional_scores_log_ilr_db_2 <- function(scores,
                                          x_data_i,
                                          pca,
                                          basis_matrix,
                                          sc_factor) {
  scaling_factor <- sc_factor
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  norm_constant <- sum(exp(clr_comp))

  # Compute scaled log likelihood
  log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i) * log(norm_constant)
  # apply scaling when sc_factor ungleich 1
  if (scaling_factor != 1) {
    log_likelihood <- log_likelihood - scaling_factor
  }

  # Prior remains unchanged as it's already well-scaled
  log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))

  return(log_likelihood + log_prior)
}

gradient_cslc_ilr_db <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix,
                                 sc_factor) {
  scaling_factor <- sc_factor
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * v_k * (term1 - term2) - scores[k] / (pca$sdev[k]^2)

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  return(grad)
}

gradient_cslc_ilr_db_2 <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix,
                                 sc_factor) {
  scaling_factor <- sc_factor
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * v_k * (term1 - term2) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores / (pca$sdev^2)

  return(grad)
}

gradient_cslc_ilr_db_3 <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix) {
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * v_k * (term1 - term2) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores / (pca$sdev^2)

  return(grad)
}


```

### Gradient

#### optimal scores

$z_i = V (\theta_i - \mu)$

i.e. we can construct the optimal scores with the results from classical PCA.

```{r optimal scores, eval=FALSE}
x_data_i <- x_data[[1]]
rotation <- pca_ilr$rotation
center <- pca_ilr$center
scores_max <- rotation %*% (ilr(x_data_i) - center)
scores_max
```

Calculation of conditional scores with maximised scores:

```{r conditional scores max, eval=FALSE}
max_cond <- conditional_scores_log_ilr_db(scores_max,
                                                    x_data_i,
                                                    pca_ilr,
                                                    basis_matrix,
                                                    sc_factor = 1)

scores_min <- rep(0, length = length(pca_init$sdev))
min_cond <- conditional_scores_log_ilr_db(scores_min,
                                                    x_data_i,
                                                    pca_init,
                                                    basis_matrix,
                                                    sc_factor = 1)

c(max_cond, min_cond)
```

The range appears to be quite small, given the big scale of the conditional distribution.

Now, what do we get for the gradient in both cases?

```{r test gradient, eval=FALSE}
test_1 <- gradient_cslc_ilr_db(scores_min,
                                                    x_data_i,
                                                    pca_init,
                                                    basis_matrix,
                                                    sc_factor = 1)
```

Implementierung Gradient:

```{r gradient objects 1, eval=FALSE}
  # scaling_factor <- 0.1
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca_ilr$center + pca_ilr$rotation %*% scores_max)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores_min), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca_ilr$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * (v_k * (term1 - term2)) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores_max / (pca_ilr$sdev^2)

  return(grad)

grad
# compare results
(ilr_coords <- ilr(x_data_i))
ilr_comp
```

mit count compositions:


```{r gradient objects, eval=FALSE}
  # scaling_factor <- 0.1
m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca_count_ilr$center + pca_count_ilr$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca_count_ilr$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * (v_k * (term1 - term2)) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores / (pca_count_ilr$sdev^2)

  return(grad)

grad
# compare results
(ilr_coords <- ilr(x_data_i))
ilr_comp
```

### optim

```{r optim, eval=FALSE}
k <- 1
i <- 1
sc_factor <- 1
scores <- rep(0, length = length(pca_count$sdev))
x_data_i <- count_data[[1]]
optim_result <- tryCatch({
      optim(rep(0, length = length(pca_init$sdev)),
            conditional_scores_log_ilr_sc,
            gr = gradient_cslc_ilr_sc,
            x_data_i = count_data[[i]],
            pca = pca_init,
            basis_matrix = basis_matrix,
            sc_factor = sc_factor,
            control = list(fnscale = -1),
            method = "BFGS")
            # method = "Nelder-Mead")
    }, error = function(e) {
        cat("error optim() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
        print("pca$sdev:")
        print(pca$sdev)
        print("optim result:")
        print(optim_result)
        print("pca center:")
        print(pca$center)
    })
(scores_median <- as.vector(optim_result$par))
# Compare differenz of values estimated with score and real_center
(ilr_comp <- as.vector(pca_ilr$center + pca_ilr$rotation %*% scores_median))

```

The values are off but are best for E*(-1)

### numerical

## Conditional Score 

```{r conditional scores calculation, eval=FALSE}
# with initial values
  lengths <- unique(sapply(count_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca_init <- prcomp(Sigma, center = FALSE)
  pca_init$center <- nu
scores <- rep(0, length = length(pca_init$sdev))

scaling_factor <- 1
ilr_comp <- as.vector(pca_init$center + pca_init$rotation %*% scores)
clr_comp <- ilr2clr(ilr_comp)
norm_constant <- sum(exp(clr_comp))

# Compute scaled log likelihood
log_likelihood <- scaling_factor * (sum(x_data_i * clr_comp)
                                    - sum(x_data_i) * log(norm_constant))

# Prior remains unchanged as it's already well-scaled
log_prior <- - sum(0.5 * scores^2 / (pca_init$sdev^2))
c(log_likelihood, log_prior)
```


### Scaling factor

## Parameter choice

# ESS = 1

1. Find explanation for ESS = 1
Therefore we need to calculate a set of weights for one iteration, i.e. repeat the expectation step:

```{r ess expectation, eval=TRUE}
monitor_weights <- function(weights, iteration) {
  for (i in seq_along(weights)) {
    ess <- 1/sum(weights[[i]]^2)
    max_weight <- max(weights[[i]])
    cat(sprintf("Iteration %d:\n
     ESS: %.2f\n 
     Max weight: %.2e\n 
     Observation: %d\n",
                iteration,
                ess,
                max_weight,
                i))
  }
}

monitor_global_ess <- function(all_weights, k) {
  mean_ess <- mean(sapply(all_weights, function(w) 1/sum(w^2)))
  cat(sprintf("Iteration %d: Mean ESS = %.2f\n", k, mean_ess))
}

  lengths <- unique(sapply(x_cts_list, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE) # what happens if we scale the matrix?
  pca$center <- nu

  max_iter <- 10
  k <- 1
  r <- 100
  sc_factor <- 1
  lambda <- 1
  sum_exp = TRUE


iteration_check <- function(x_data,
                                        k = 1,
                                        r = 100,
                                         max_iter = 50,
                                         lambda = 1,
                                         eps = 0.01,
                                         sc_factor = 1,
                                         sum_exp = TRUE,
                                         pca = pca) {
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  scores_median_list <- list(length(x_data))
  log_weights_list <- list(length(x_data))
  aux_distr_list <- list(length(x_data)) 
  likelihood_term1_list <- list(length(x_data))
  likelihood_term2_list <- list(length(x_data))

# E-Step ###################
  for (i in seq_along(x_data)) {
    optim_result <- tryCatch({
      optim(rep(0, length = length(pca$sdev)),
            conditional_scores_log_ilr ,
            gr = gradient_cslc_vs1,
            x_data_i = x_data[[i]],
            pca = pca,
            basis_matrix = basis_matrix,
            sc_factor = sc_factor,
            control = list(fnscale = -1),
            method = "BFGS")
    }, error = function(e) {
        cat("error optim() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
        print("pca$sdev:")
        print(pca$sdev)
        print("optim result:")
        print(optim_result)
        print("pca center:")
        print(pca$center)
    })
    scores_median <- as.vector(optim_result$par)
    scores_median_list[[i]] <- scores_median
    # importance sampling
    proposal_scores[[i]] <- sapply(1:(r * k), function(t) {
      matrix(rnorm(length(scores_median),
                    mean = scores_median,
                    sd = lambda * pca$sdev))
    })

    conditional_scores_list[[i]] <-
      apply(proposal_scores[[i]], 2, function(scores) {
        conditional_scores_log_ilr(
                                      scores,
                                      x_data[[i]],
                                      pca,
                                      basis_matrix,
                                      sc_factor)
      })

    likelihood_term1_list[[i]] <- apply(proposal_scores[[i]], 2, function(scores) {
              ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
              clr_comp <- ilr2clr(ilr_comp)
              norm_constant <- sum(exp(clr_comp))

              # Compute scaled log likelihood
              sum(x_data[[i]] * clr_comp)
    })  

    likelihood_term2_list[[i]] <- apply(proposal_scores[[i]], 2, function(scores) {
              ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
              clr_comp <- ilr2clr(ilr_comp)
              norm_constant <- sum(exp(clr_comp))

              sum(x_data[[i]]) * log(norm_constant)
    })

    log_weights <- apply(proposal_scores[[i]], 2, function(scores) {
      conditional_scores_log_ilr(
                                    scores,
                                    x_data[[i]],
                                    pca,
                                    basis_matrix,
                                    sc_factor) -
        sum(dnorm(
                  scores,
                  mean = scores_median,
                  sd = lambda * pca$sdev,
                  log = TRUE))
    })

    aux_distr_list[[i]] <- apply(proposal_scores[[i]], 2, function(scores) {      sum(dnorm(
                  scores,
                  mean = scores_median,
                  sd = lambda * pca$sdev,
                  log = TRUE))
    })

    log_weights_list[[i]] <- log_weights
    # increase numerical stability
    if (sum_exp) {
      weights[[i]] <- stabilize_weights(log_weights)
    } else {
      log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
      weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
    }
  # End of E-Step ###################
  }
  return(list(weights = weights, conditional_scores_list = conditional_scores_list, log_weights_list = log_weights_list,
   aux_distr_list = aux_distr_list, scores_median_list = scores_median_list,
   likelihood_term1_list = likelihood_term1_list, likelihood_term2_list = likelihood_term2_list)) 
}


results <- iteration_check(x_cts_list, pca = pca_ilr_cts, sc_factor = -385500, r = 100, sum_exp = TRUE)


weights <- results$weights
monitor_global_ess(weights, k)

# check results
# turn list into matrix
# likelihood_term1 <- do.call(rbind, results$likelihood_term1_list)
# summary(likelihood_term1)
# likelihood_term2 <- do.call(rbind, results$likelihood_term2_list)
# summary(likelihood_term2)
# likelihood_term <- likelihood_term1 - likelihood_term2 + 390000
# summary(likelihood_term)
# scores_median <- do.call(rbind, results$scores_median_list)
# summary(scores_median)
scores_conditional <- do.call(rbind, results$conditional_scores_list)
summary(scores_conditional)
# log_weights <- do.call(rbind, results$log_weights_list)
# summary(log_weights)
# aux_distr <- do.call(rbind, results$aux_distr_list)
# summary(aux_distr)

# weights
# monitor_global_ess(weights, k)
```

Let's plot the data objects

```{r plot data objects, eval=TRUE}
plot_score_distributions <- function(scores_matrix) {
  # Convert to long format
  library(tidyr)
  scores_long <- as.data.frame(scores_matrix) %>%
    pivot_longer(cols = everything(), 
                names_to = "iteration",
                values_to = "scores")
  
  # Create plot
  library(ggplot2)
  ggplot(scores_long, aes(x = scores)) +
    geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
    geom_density(alpha = 0.2) +
    theme_minimal() +
    labs(title = "Distribution of Conditional Scores",
         x = "Score Value",
         y = "Count")
}

plot_multiple_distributions <- function(..., matrix_names = NULL) {
  # Get list of matrices
  matrices <- list(...)
  
  # If no names provided, create default names
  if (is.null(matrix_names)) {
    matrix_names <- paste("Matrix", seq_along(matrices))
  }
  
  # cat("Plotting the following matrices:" matrix_names, "\n")
  
  # Create list of plots
  plots <- lapply(seq_along(matrices), function(i) {
    scores_df <- as.data.frame(matrices[[i]]) %>%
      pivot_longer(cols = everything(), 
                  names_to = "iteration",
                  values_to = "scores")
    
    ggplot(scores_df, aes(x = scores)) +
      geom_histogram(bins = 30, fill = "steelblue", alpha = 0.7) +
      theme_minimal() +
      labs(title = matrix_names[i],
           x = "Value",
           y = "Count")
  })
  
  # Combine plots side by side
  wrap_plots(plots, ncol = length(matrices))
}

```

## Distributions for three scenarios

1. with initial parameters

```{r plot data objects 1, eval=TRUE}
# results <- iteration_check(x_cts_list, pca = pca_ilr_cts, sc_factor = 1)
results <- iteration_check(x_cts_list, pca = pca, sc_factor = 1)

likelihood_term1 <- do.call(rbind, results$likelihood_term1_list)
likelihood_term2 <- do.call(rbind, results$likelihood_term2_list)
likelihood_term <- likelihood_term1 - likelihood_term2 
scores_conditional <- do.call(rbind, results$conditional_scores_list)
aux_distr <- do.call(rbind, results$aux_distr_list)

plot_multiple_distributions(
  likelihood_term[1,], 
  aux_distr[1,],
  scores_conditional[1, ],
  matrix_names = c("Likelihood value", "Auxiliary distribution", "Conditional scores")
)

# statistical parameters for one observation and 100 samples
summary(likelihood_term[1, ])
```

2. with pca parameters

```{r plot data objects 2, eval=TRUE}
results <- iteration_check(x_cts_list, pca = pca_ilr_cts , sc_factor = 1)

likelihood_term1 <- do.call(rbind, results$likelihood_term1_list)
likelihood_term2 <- do.call(rbind, results$likelihood_term2_list)
likelihood_term <- likelihood_term1 - likelihood_term2 
scores_conditional <- do.call(rbind, results$conditional_scores_list)
aux_distr <- do.call(rbind, results$aux_distr_list)

plot_multiple_distributions(
  likelihood_term[1,], 
  aux_distr[1,],
  scores_conditional[1,],
  matrix_names = c("Likelihood value", "Auxiliary distribution", "Conditional scores")
)

```

3. with pca parameters and adjusted gradient

```{r plot data objects 3, eval=TRUE}
results <- iteration_check(x_cts_list, pca = pca_ilr_cts , sc_factor = -350000)

likelihood_term1 <- do.call(rbind, results$likelihood_term1_list)
likelihood_term2 <- do.call(rbind, results$likelihood_term2_list)
likelihood_term <- likelihood_term1 - likelihood_term2 
scores_conditional <- do.call(rbind, results$conditional_scores_list)
aux_distr <- do.call(rbind, results$aux_distr_list)

plot_multiple_distributions(
  likelihood_term[1,], 
  aux_distr[1,],
  scores_conditional[1,],
  matrix_names = c("Likelihood value", "Auxiliary distribution", "Conditional scores")
)

```

# DB test

## fit_pca_vs_5

```{r debug vs5, eval=TRUE}
sim_ls <- tar_read(sim_comp_2_smi_nSim_60)
x_data <- sim_ls[[1]]$x_data
max_iter = 50
r = 10
lambda = 1
eps = 0.01
sc_factor = 1
scores = TRUE
sum_exp = TRUE
fix_sign = TRUE

test <-     fit_pca_vs_5(x_data,
                   max_iter = 50,
                   r = 10,
                   lambda = 1,
                   eps = 0.03,
                   sc_factor = 1,
                   scores = TRUE,
                   sum_exp = TRUE,
                   fix_sign = TRUE)

  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }
  if (is.list(x_data)) {
    x_df <- do.call(rbind, x_data)
  }
  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_df <- x_data
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }
  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  V <- get_helmert(x_df)
  nu <- rep(0, ncol(x_df) - 1)
  Sigma <- diag(ncol(x_df) - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))

optim_result <- optim(rep(0L, length = length(pca$sdev)),
                              conditional_scores_log_ilr,
                              gr = gradient_cslc_vs1,
                              x_data_i = x_data[[1]],
                              pca = pca,
                              basis_matrix = t(V),
                              sc_factor = sc_factor,
                              control = list(fnscale = -1),
                              method = "BFGS")
scores_median <- as.vector(optim_result$par)

proposal_scores[[1]] <- sapply(1L:(r * 1), function(t) {
          matrix(rnorm(length(scores_median),
                       mean = scores_median,
                       sd = lambda * pca$sdev))
        })

log_weights <- apply(proposal_scores[[1]], 2L, function(scores) {
          conditional_scores_log_ilr(scores,
                                     x_data[[1]],
                                     pca,
                                     basis_matrix = t(V),
                                     sc_factor) -
            sum(dnorm(scores,
                      mean = scores_median,
                      sd = lambda * pca$sdev,
                      log = TRUE))
        })

 weights[[1]] <- stabilize_weights(log_weights)       

## that was the E-step
k <- 1
for (i in seq_along(x_data)){
        optim_result <- optim(rep(0L, length = length(pca$sdev)),
                              conditional_scores_log_ilr,
                              gr = gradient_cslc_vs1,
                              x_data_i = x_data[[i]],
                              pca = pca,
                              basis_matrix = t(V),
                              sc_factor = sc_factor,
                              control = list(fnscale = -1),
                              method = "BFGS")

        scores_median <- as.vector(optim_result$par)
        proposal_scores[[i]] <- sapply(1L:(r * k), function(t) {
          matrix(rnorm(length(scores_median),
                       mean = scores_median,
                       sd = lambda * pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2L, function(scores) {
          conditional_scores_log_ilr(scores,
                                     x_data[[i]],
                                     pca,
                                     basis_matrix = t(V),
                                     sc_factor) -
            sum(dnorm(scores,
                      mean = scores_median,
                      sd = lambda * pca$sdev,
                      log = TRUE))
        })
        if (sum_exp == TRUE) {
          weights[[i]] <- stabilize_weights(log_weights)
        } else {
          max_log_weight <- max(log_weights)
          weights <- exp(log_weights - max_log_weight)
          weights[[i]] <- weights / sum(weights)
        }
}

 scores_matrix <- sapply(seq_along(weights), function(i){
        proposal_scores[[i]] %*% weights[[i]]
      })

mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)

pca_old <- pca
pca$center <- pca$center + pca$rotation %*% mu_scores
Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
  Reduce("+", lapply(1L:(r * k), function(t) {
    C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
      t((proposal_scores[[i]][, t] - mu_scores))
  }))
})) / length(weights)
edc <-  tryCatch({eigen(Sigma, symmetric = TRUE)}, error = function(e) {
  cat("error eigen() in iteration", k, "for observation", i, "\n")
  cat("error message:", e$message, "\n")
})
edc$vectors <- fix_sign(edc$vectors)
ev <- edc$values
if (any(neg <- ev < 0)) {
  if (any(ev[neg] < -9 * .Machine$double.eps * ev[1L])) 
    stop("covariance matrix is not non-negative definite")
  else ev[neg] <- 0
}
pca$sdev <- sqrt(edc$values)
pca$rotation <- pca$rotation %*% edc$vectors
critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
  pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
}))
Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
  pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
}))
Sigma_diff <- Sigma_old - Sigma_new
critical_value_2 <- norm(Sigma_diff, type = "F")

# Convergence step
constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
pca$rotation <- t(t(pca$rotation) / constant)
pca$rotation_ilr <- pca$rotation
pca$rotation <- V %*% pca$rotation_ilr
pca$center <- V %*% pca$center
pca$sdev <- pca$sdev / sum(pca$sdev)
pca$scores_clr <- if (scores) {
  scale(clr(x_df),
        center = pca$center, scale = FALSE) %*% pca$rotation
}
rownames(pca$rotation) <- names(x_data[[1L]])
cn <- paste0("Comp.", seq_len(ncol(pca$rotation)))
colnames(pca$rotation) <- cn
mean_ess <- mean(sapply(weights, function(w) 1 / sum(w^2)))

```


## ilr version with adjusted gradient

```{r db test, eval=FALSE}

fit_compositional_pca_ilr_db <- function(x_data,
                                         max_iter = 50,
                                         r = 10,
                                         lambda = 1,
                                         eps = 0.01,
                                         sc_factor = 1,
                                         sum_exp = TRUE) {
  start_time <- Sys.time()
  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }

  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }

  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  scores_median_list <- list(length(x_data))
  if (max_iter > 0) {
    for (k in 1:max_iter) {
      cat("Iteration:", k, "\n")
      # E-Step ###################
      for (i in seq_along(x_data)) {
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)),
                conditional_scores_log_ilr_db_2,
                gr = gradient_cslc_ilr_db,
                x_data_i = x_data[[i]],
                pca = pca,
                basis_matrix = basis_matrix,
                sc_factor = sc_factor,
                control = list(fnscale = -1),
                method = "BFGS")
                # method = "Nelder-Mead")
        }, error = function(e) {
            cat("error optim() in iteration", k, "for observation", i, "\n")
            cat("error message:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            print("optim result:")
            print(optim_result)
            print("pca center:")
            print(pca$center)
        })
        scores_median <- as.vector(optim_result$par)
        scores_median_list[[i]] <- scores_median
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r * k), function(t) {
          matrix(rnorm(length(scores_median),
                       mean = scores_median,
                       sd = lambda * pca$sdev))
        })

        conditional_scores_list[[i]] <-
          apply(proposal_scores[[i]], 2, function(scores) {
            conditional_scores_log_ilr_db_2(
                                          scores,
                                          x_data[[i]],
                                          pca,
                                          basis_matrix,
                                          sc_factor)
          })

        log_weights <- apply(proposal_scores[[i]], 2, function(scores) {
          conditional_scores_log_ilr_db_2(
                                        scores,
                                        x_data[[i]],
                                        pca,
                                        basis_matrix,
                                        sc_factor) -
            sum(dnorm(
                      scores,
                      mean = scores_median,
                      sd = lambda * pca$sdev,
                      log = TRUE))
        })
        # increase numerical stability
        if (sum_exp) {
          weights[[i]] <- stabilize_weights(log_weights)
        } else {
          log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
          weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
        }
        # End of E-Step ###################
      }
      monitor_global_ess(weights, k)
      mean_conditional <- mean(unlist(conditional_scores_list), na.rm = TRUE)

      # cat(sprintf("Conditional score mean value %.2f:\n",
      #             mean_conditional))

      # M-Step ###################
      scores_matrix <- sapply(seq_along(weights), function(i){
        proposal_scores[[i]] %*% weights[[i]]
      })
      na_count <- sum(is.na(scores_matrix))

      # Calculate mu_scores with NA removal
      mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)

      # Print diagnostic message
      cat(sprintf("Removed %d NA values when calculating mu_scores\n", na_count))      
      # mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
      #   proposal_scores[[i]] %*% weights[[i]]
      # }), na.rm = TRUE)
      # cat("Mean scores:", mu_scores, "\n")
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation %*% mu_scores
      cat("center:", pca$center, "\n")
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
        Reduce("+", lapply(1:(r * k), function(t) {
          C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
            t((proposal_scores[[i]][, t] - mu_scores))
        }))
      })) / length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
        cat("error eigen() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
        print("pca$sdev:")
        print(pca$sdev)
      })
      negative_eigenvalues <- eigen_decomp$values < 0
      if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative.\n
        They have been set to zero.",
                        sum(negative_eigenvalues)))
      }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      pca$rotation <- pca$rotation %*% eigen_decomp$vectors
      clr_rotation <- t(basis_matrix) %*% pca$rotation %*% basis_matrix
      cat("PCA1:", clr_rotation[ , 1], "\n")
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value center_diff:", critical_value_1, "\n")
      Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
        pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
      }))
      Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
        pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
      }))
      Sigma_diff <- Sigma_old - Sigma_new
      critical_value_2 <- norm(Sigma_diff, type = "F")
      cat("critical value Sigma_diff:", critical_value_2, "\n")

      if (max(critical_value_1, critical_value_2) < eps) {
        constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
        pca$rotation <- t(t(pca$rotation) / constant)
        pca$sdev <- pca$sdev * constant

        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "minutes"))
        return(list("iteration" = k,
                    "pca" = pca,
                    "x_data" = x_data,
                    "list_center" = center_list,
                    "list_sdev" = sdev_list,
                    "time" = elapsed_time))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) {
    sqrt(sum(g^2))
  })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev * constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter,
              "pca" = pca,
              "x_data" = x_data,
              "list_center" = center_list,
              "list_sdev" = sdev_list,
              "time" = elapsed_time))
}

# use parallel processing 
library(future.apply)

# Parallel E-Step implementation
parallel_estep <- function(x_data, pca, basis_matrix, r, k, lambda, sc_factor) {
    future_lapply(seq_along(x_data), function(i) {
        optim_result <- optim(rep(0, length = length(pca$sdev)),
                            conditional_scores_log_ilr_db_2,
                            gr = gradient_cslc_ilr_db_2,
                            x_data_i = x_data[[i]],
                            pca = pca,
                            basis_matrix = basis_matrix,
                            sc_factor = sc_factor,
                            control = list(fnscale = -1),
                            method = "BFGS")
        
        scores_median <- as.vector(optim_result$par)
        proposal_scores <- sapply(1:(r * k), function(t) {
            matrix(rnorm(length(scores_median),
                        mean = scores_median,
                        sd = lambda * pca$sdev))
        })
        
        log_weights <- apply(proposal_scores, 2, function(scores) {
            conditional_scores_log_ilr_db_2(scores,
                                          x_data[[i]],
                                          pca,
                                          basis_matrix,
                                          sc_factor) -
                sum(dnorm(scores,
                         mean = scores_median,
                         sd = lambda * pca$sdev,
                         log = TRUE))
        })
        
        weights <- stabilize_weights(log_weights)
        
        list(proposal_scores = proposal_scores,
             weights = weights)
    }, future.seed = TRUE)
}

fit_compositional_pca_ilr_db_par <- function(x_data,
                                         max_iter = 50,
                                         r = 10,
                                         lambda = 1,
                                         eps = 0.01,
                                         sc_factor = 0.001,
                                         sum_exp = TRUE) {
  start_time <- Sys.time()
  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }

  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }

  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  scores_median_list <- list(length(x_data))

  # Set up parallel workers
  plan(multisession, workers = 4)

    if (max_iter > 0) {
      for (k in 1:max_iter) {
          cat("Iteration:", k, "\n")
          
          # Parallel E-Step
          estep_results <- parallel_estep(x_data, pca, basis_matrix, r, k, lambda, sc_factor)
          
          # Extract results
          proposal_scores <- lapply(estep_results, `[[`, "proposal_scores")
          weights <- lapply(estep_results, `[[`, "weights")
      }
      
      monitor_global_ess(weights, k)
      mean_conditional <- mean(unlist(conditional_scores_list), na.rm = TRUE)

      # cat(sprintf("Conditional score mean value %.2f:\n",
      #             mean_conditional))

      # M-Step ###################
      scores_matrix <- sapply(seq_along(weights), function(i){
        proposal_scores[[i]] %*% weights[[i]]
      })
      na_count <- sum(is.na(scores_matrix))

      # Calculate mu_scores with NA removal
      mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)

      # Print diagnostic message
      cat(sprintf("Removed %d NA values when calculating mu_scores\n", na_count))      
      # mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
      #   proposal_scores[[i]] %*% weights[[i]]
      # }), na.rm = TRUE)
      # cat("Mean scores:", mu_scores, "\n")
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation %*% mu_scores
      cat("center:", pca$center, "\n")
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
        Reduce("+", lapply(1:(r * k), function(t) {
          C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
            t((proposal_scores[[i]][, t] - mu_scores))
        }))
      })) / length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
        cat("error eigen() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
        print("pca$sdev:")
        print(pca$sdev)
      })
      negative_eigenvalues <- eigen_decomp$values < 0
      if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative.\n
        They have been set to zero.",
                        sum(negative_eigenvalues)))
      }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      pca$rotation <- pca$rotation %*% eigen_decomp$vectors
      clr_rotation <- t(basis_matrix) %*% pca$rotation %*% basis_matrix
      cat("PCA1:", clr_rotation[ , 1], "\n")
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value center_diff:", critical_value_1, "\n")
      Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
        pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
      }))
      Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
        pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
      }))
      Sigma_diff <- Sigma_old - Sigma_new
      critical_value_2 <- norm(Sigma_diff, type = "F")
      cat("critical value Sigma_diff:", critical_value_2, "\n")

      if (max(critical_value_1, critical_value_2) < eps) {
        constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
        pca$rotation <- t(t(pca$rotation) / constant)
        pca$sdev <- pca$sdev * constant

        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "minutes"))
        return(list("iteration" = k,
                    "pca" = pca,
                    "x_data" = x_data,
                    "list_center" = center_list,
                    "list_sdev" = sdev_list,
                    "time" = elapsed_time))
      }
    }
  constant <- apply(pca$rotation, 2, function(g) {
    sqrt(sum(g^2))
  })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev * constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter,
              "pca" = pca,
              "x_data" = x_data,
              "list_center" = center_list,
              "list_sdev" = sdev_list,
              "time" = elapsed_time))
}


# testrun with simulated data and standard parameters
testrun1 <- fit_compositional_pca_ilr_sc(x_data, max_iter = 30, sc_factor = 1, sum_exp = TRUE)
# numerical estimation in optim
testrun2 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE)
testrun3 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE, eps = 0.02)
testrun4 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE, eps = 0.02, max_iter = 80)
testrun5 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE, eps = 0.01, max_iter = 80, lamda = 0.8)
testrun6 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE, eps = 0.01, max_iter = 80, lambda = 0.8)
testrun7 <- fit_compositional_pca_ilr_db(x_data, sc_factor = 1, sum_exp = TRUE, eps = 0.01, max_iter = 80, lambda = 1.2)
testrun7 <- fit_compositional_pca_ilr_db(x_data, sc_factor = -100000,
                           sum_exp = TRUE, eps = 0.01, max_iter = 80, lambda = 1)

(testrun6$pca$sdev <- testrun6$pca$sdev / sum(testrun6$pca$sdev) )

testrun6$pca$center
plot(testrun1$pca$sdev)
plot_pca_rotation(testrun1$pca$rotation)

(clr_rotation <- t(basis_matrix) %*% testrun4$pca$rotation %*% basis_matrix)
pca_clr$rotation
```

> pca_ilr$center
[1]  1.4161564 -0.4162729 -2.4625888 -0.5572391


Test run with real data

```{r real test run, eval=FALSE}
x <- tar_read(data_kl15_comp)
x_acomp <- acomp(x)
x_clr <- clr(x_acomp)
x_ilr <- ilr(x_acomp)
pca_count_clr <- prcomp(na.omit(x_clr))
pca_count_ilr <- prcomp(na.omit(x_ilr))
biplot(pca_count)
plot(pca_count)

data_comp <- tar_read(data_kl15_comp)
colnames(data_comp)
data_comp <- unname(data_comp)
count_data <- apply(data_comp, 1, function(x) x, simplify = FALSE)
acomp_data <- acomp(data_comp)
count_data_acomp <-  apply(acomp_data, 1, function(x) x, simplify = FALSE)
# count_data[[1]]
# count_data_small <- count_data[1:10]
testrun_x_1 <- fit_compositional_pca_ilr_db(count_data, max_iter = 80, r = 10, lambda = 1, eps = 0.01, sc_factor = 1, sum_exp = TRUE)
testrun_x_2 <- fit_compositional_pca_ilr_db(count_data_acomp, sc_factor = 1,
                           sum_exp = TRUE, eps = 0.01, max_iter = 80, lambda = 1)
(testrun_x_1$pca$sdev <- testrun_x_1$pca$sdev / sum(testrun_x_1$pca$sdev) )

plot(testrun_x_1$pca$sdev)
plot_pca_rotation(testrun_x_1$pca$rotation)

testrun_x_2$pca$center
pca_count_ilr$center

(clr_rotation <- t(basis_matrix) %*% testrun_x_2$pca$rotation %*% basis_matrix)
pca_count_clr$rotation
```

pca_count_ilr$center
 [1] -0.3425493  2.6224834 -0.1471703  1.3770756 -3.1598049 -0.4763189
 [7]  1.7405216 -0.3022018  1.0637828  4.2967339 -0.3788770  1.5945849


# densities

Take an example observation and calculate its density with simplex and clr-coordinates:
(and ilr coordinates)

```{r density example, eval=TRUE}
x_data <- tar_read(data_kl15_comp)
x <- unname(as.numeric(x_data[1,]))
p <- as.vector(mean(acomp(x_data)))
n <- sum(x)
(dichte <- dmultinom(x, size = n, prob = p, log = TRUE))

x_total <- sum(x) 
x_props <- x/x_total
log_pi <- as.vector(log(x_props))
clr_x <- log(x_props) - mean(log_pi)

sum(x * clr_x)
clr_x^x # numerisch nicht lösbar
p^x
x*log(p)

# log density of clr components
x * clr_x
sum(x * clr_x)

pca_ilr <- prcomp(ilr(x_data))

# log density of scores_i (ilr-coordinates)
# 1. get optimal scores 
k <- 1
i <- 1
sc_factor <- 1
# scores <- rep(0, length = length(pca_count$sdev))
x_data_i <- x
optim_result <- tryCatch({
      optim(rep(0, length = length(pca_ilr$sdev)),
            conditional_scores_log_ilr,
            gr = gradient_cslc_vs1,
            x_data_i = x_data[[i]],
            pca = pca_ilr,
            basis_matrix = basis_matrix,
            sc_factor = sc_factor,
            control = list(fnscale = -1),
            method = "BFGS")
            # method = "Nelder-Mead")
    }, error = function(e) {
        cat("error optim() in iteration", k, "for observation", i, "\n")
        cat("error message:", e$message, "\n")
        print("pca$sdev:")
        print(pca$sdev)
        print("optim result:")
        print(optim_result)
        print("pca center:")
        print(pca$center)
    })
(scores_median <- as.vector(optim_result$par))

# get proposal scores
proposal_scores <- sapply(1:100, function(t) {
      matrix(rnorm(length(scores_median),
                   mean = scores_median,
                   sd = pca_ilr$sdev))
    })
scores <- proposal_scores[, 1]
aux_1 <- sum(dnorm(scores,
          mean = scores_median,
          sd = lambda * pca_ilr$sdev,
          log = TRUE))

# alternativ mit 
library(mvtnorm)
sigma <- diag(pca_ilr$sdev^2)
(density_mvn <- dmvnorm(t(scores), 
                      mean = scores_median,
                      sigma = sigma,
                      log = FALSE))

# Dimension der Daten
d <- length(scores)

# Kovarianzmatrix korrekt skalieren
sigma <- diag(pca$sdev^2)

# Normalisierungskonstante
(norm_const <- (2*pi)^(-d/2) * det(sigma)^(-1/2))

# Mahalanobis-Distanz zum Zentrum
(maha_dist <- mahalanobis(t(scores), scores_median, sigma))

# Exponentieller Term
(exp_term <- exp(-0.5 * maha_dist))

# Multivariate Normalverteilungsdichte
(density_mvn <- (2*pi)^(-d/2) * det(sigma)^(-1/2) * exp(-0.5 * maha_dist))

sigma <- diag(pca_ilr$sdev^2)
# Determinante
det(sigma)
# Eigenwerte
eigen(sigma)$values
```

## Verteilung Hilfsfunktion

Für r = 100 und Beobachtung 1 erhalten wir folgende Verteilung

```{r hilfsfunktion verteilung, eval=TRUE}
distr_aux <- list(100)
for (i in 1:100) {
  scores <- proposal_scores[, i]
  distr_aux[[i]] <- sum(dnorm(scores,
          mean = scores_median,
          sd = pca_ilr$sdev,
          log = TRUE))
}

df_aux <- data.frame(density = unlist(distr_aux))

# Create density plot
ggplot(df_aux, aes(x = density)) +
  geom_density(fill = "blue", alpha = 0.3) +
  geom_histogram(aes(y = ..density..), alpha = 0.5, bins = 30) +
  theme_minimal() +
  labs(x = "Log Density", 
       y = "Frequency",
       title = "Distribution of Log Densities from Proposal Distribution")
```

Was passiert, wenn wir nur die "relevanten" Componenten betrachten?

```{r dimension reduction}
pca_ilr$sdev <- pca_ilr$sdev[1:5]

distr_aux <- list(100)
for (i in 1:100) {
  scores <- proposal_scores[1:5, i]
  distr_aux[[i]] <- sum(dnorm(scores,
          mean = scores_median,
          sd = pca_ilr$sdev,
          log = TRUE))
}

df_aux <- data.frame(density = unlist(distr_aux))

# Create density plot
ggplot(df_aux, aes(x = density)) +
  geom_density(fill = "blue", alpha = 0.3) +
  geom_histogram(aes(y = ..density..), alpha = 0.5, bins = 30) +
  theme_minimal() +
  labs(x = "Log Density", 
       y = "Frequency",
       title = "Distribution of Log Densities from Proposal Distribution")

```

# helper functions debug

```{r functions debug, eval=FALSE}
sim <- tar_read(sim_comp_1_smi_nSim_300)
com_list <- list(
  Probe1 = c(Kohlenhydrate = 50, Proteine = 30, Fette = 20),
  Probe2 = c(Kohlenhydrate = 60, Proteine = 25, Fette = 15),
  Probe3 = c(Kohlenhydrate = 55, Proteine = 35, Fette = 10),
  Probe4 = c(Kohlenhydrate = 65, Proteine = 20, Fette = 15)
)
x_ls <- sim[[1]]$x_data
x_df <- sim[[1]]$x_data_matrix
x_df_bb <- zeroreplace(x_df, rep(0.1, ncol(x_df)))
x_rp <- replace_zeros(x_df)

x_clr <- lapply(x_ls, clr)
x_clr_a <- clr(com_list)
x_clr_2 <- clr(x_df)
x_clr_3 <- lapply(x_ls,clr_transform)
x_clr_4 <- clr_transform(x_df)

x_inv <- lapply(x_clr, clrInv)
x_inv_2 <- clrInv(x_clr_2)
x_inv_3 <- lapply(x_clr_3, inv_clr)
x_inv_4 <- inv_clr(x_clr_4)

x_ilr <- lapply(x_ls, ilr)
x_ilr_2 <- ilr(x_df)
x_ilr_3 <- lapply(x_ls, ilr_transform)
x_ilr_4 <- ilr_transform(x_df)

x_invI <- lapply(x_ilr, ilrInv)
x_invI_2 <- ilrInv(x_ilr_2)
x_invI_3 <- lapply(x_ilr_3, inv_ilr)
x_invI_4 <- inv_ilr(x_ilr_4)

D <- ncol(x_df)
V <- get_helmert(D)
V_inv <- MASS::ginv(V)
clr_x <- ilr_x %*% t(V)
clr_x_2 <- ilr_x %*% V_inv
clr_x_3 <- V %*% ilr_x 
```

# predict compositions with gradient descent

```{r predict compositions, eval=FALSE}
predict_latent_compositions_clr <- function(comp_pca, basis_matrix, sc_factor){
  predicted_scores <- sapply(seq_along(comp_pca$x_data), function(i){
    optim_result <- optim(rep(0, length = length(comp_pca$pca$sdev)), conditional_scores_log_ilr , gr = gradient_cslc_vs1,
                          x_data_i = comp_pca$x_data[[i]], pca = comp_pca$pca,
                          basis_matrix = basis_matrix,
                          sc_factor = sc_factor,
                          control = list(fnscale = -1), method = "BFGS")
    as.vector(optim_result$par)
  })
  
  compositions_list <- vector("list", ncol(predicted_scores))

  for(i in 1:ncol(predicted_scores)) {
      # Extract current score vector
      current_scores <- predicted_scores[,i]

      # Transform to CLR space
      ilr_center <- as.vector(comp_pca$pca$center)
      clr_center <- ilr_center %*% basis_matrix
      clr_rotation <- t(basis_matrix) %*% comp_pca$pca$rotation
      clr_composition <- clr_center + as.vector(clr_rotation %*% current_scores)

      # Transform to composition
      compositions_list[[i]] <- clrInv(clr_composition)
  }
  return(list("predicted_scores" = predicted_scores, "compositions" = compositions_list))
}

predict_latent_compositions_ilr <- function(comp_pca){
  predicted_scores <- sapply(seq_along(comp_pca$x_data), function(i){
    optim_result <- optim(rep(0, length = length(comp_pca$pca$sdev)), conditional_scores_log_ilr , gr = gradient_cslc_vs1,
                          x_data_i = comp_pca$x_data[[i]], pca = comp_pca$pca,
                          basis_matrix = basis_matrix,
                          sc_factor = sc_factor,
                          control = list(fnscale = -1), method = "BFGS")
    as.vector(optim_result$par)
  })
  clr_rotation <- t(basis_matrix) %*% comp_pca$rotation %*% basis_matrix
  clr_center <- t(basis_matrix) %*% simulation_results_list_2[[j]][[i]]$pca$center
  clr_compositions <- clr_center + comp_pca$pca$rotation%*%predicted_scores
  compositions <- clrInv(clr_compositions)
  return(list("clr_compositions" = clr_densities, "predicted_scores" = predicted_scores, "compositions" = compositions))
}


# example with sparsely sampled data
sim <- tar_read(sim_comp_2_smi_nSim_ebT_80_pi)
# observed data
x_df <- sim[[1]]$x_data_matrix
head(x_df)
head(acomp(x_df))
colMeans(acomp(x_df))
pi_ls <- sim[[1]]$composition_list
pi_df <- do.call(rbind, pi_ls)

D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

comp_pca_ls <- tar_read(pca_sim1_2_80_ebT)
comp_pca <- comp_pca_ls[[1]]

plot_pca_rotations(comp_pca$pca$rotation , main = "PCA MCEM", scale = 1, fixed = FALSE)
# test with one observation
optim_result <- optim(rep(0, length = length(comp_pca$pca$sdev)), conditional_scores_log_ilr , gr = gradient_cslc_vs1,
                          x_data_i = comp_pca$x_data[[1]], pca = comp_pca$pca,
                          basis_matrix = basis_matrix,
                          sc_factor = sc_factor,
                          control = list(fnscale = -1), method = "BFGS")
predicted_scores <- as.vector(optim_result$par)
ilr_center <- as.vector(comp_pca$pca$center)
clr_center <- ilr_center %*% basis_matrix
clr_rotation <- t(basis_matrix) %*% comp_pca$pca$rotation
clr_composition <- clr_center + as.vector(clr_rotation%*%predicted_scores)
(composition <- clrInv(clr_composition))
pi_df[1,]
sqrt(sum((pi_df[1,] - composition)^2))

# numerische alternative
optim_result <- optim(rep(0, length = length(comp_pca$pca$sdev)), 
                      conditional_scores_log_ilr,
                      x_data_i = comp_pca$x_data[[1]], 
                      pca = comp_pca$pca,
                      basis_matrix = basis_matrix,
                      sc_factor = sc_factor,
                      control = list(fnscale = -1), 
                      method = "Nelder-Mead")
predicted_scores <- as.vector(optim_result$par)
ilr_center <- as.vector(comp_pca$pca$center)
clr_center <- ilr_center %*% basis_matrix
clr_rotation <- t(basis_matrix) %*% comp_pca$pca$rotation
clr_composition <- clr_center + as.vector(clr_rotation%*%predicted_scores)
(composition <- clrInv(clr_composition))
pi_df[1,]
sqrt(sum((pi_df[1,] - composition)^2))
# is actually a bit better but that might depend on individual observation

# computation over eigendecomposition
ilr_center <- as.vector(comp_pca$pca$center)
clr_center <- ilr_center %*% basis_matrix
clr_rotation <- t(basis_matrix) %*% comp_pca$pca$rotation
predicted_scores <- comp_pca$pca$rotation %*% (ilr(pi_df[1,]) - ilr_center)
clr_composition <- clr_center + as.vector(clr_rotation%*%predicted_scores)
(composition <- clrInv(clr_composition))
pi_df[1,]
sqrt(sum((pi_df[1,] - composition)^2))
# thats very close to the first estimate

## final test
test <- predict_latent_compositions_clr(comp_pca, basis_matrix = basis_matrix, sc_factor = 1)
list <- test$compositions
length(list)
df <- do.call(rbind, list)
colMeans(df)
colMeans(pi_df)
cov(df)
cov(pi_df)
norm(cov(pi_df) - cov(df), type = "F")
# compare to standard and robust results
pca_clr_std <- prcomp(clr_transform(x_df, replace_zeros = "neutral"))
pca_clr_rob <- pcaCoDa(x_df, method = "robust")
clr_scores_std <- pca_clr_std$rotation %*% (clr(x_df - pca_clr_std$center))
compositions_std_list <- vector("list", nrow(clr_scores_std))
for(i in 1:nrow(clr_scores_std)) {
      # Extract current score vector
      current_scores <- clr_scores_std[i,]

      clr_composition <- pca_clr_std$center + as.vector(pca_clr_std$rotation  %*% current_scores)

      # Transform to composition
      compositions_std_list[[i]] <- clrInv(clr_composition)
  }
list <- compositions_std_list
length(list)
df <- do.call(rbind, list)
colMeans(df)
colMeans(pi_df)
cov(df)
cov(pi_df)
norm(cov(pi_df) - cov(df), type = "F")

clr_scores_rob <- pca_clr_rob$scores
rob_center <- pca_clr_std$center
compositions_rob_list <- vector("list", nrow(clr_scores_rob))
for(i in 1:nrow(clr_scores_rob)) {
      # Extract current score vector
      current_scores <- clr_scores_rob[i,]

      clr_composition <- rob_center + as.vector(pca_clr_rob$loadings  %*% current_scores)

      # Transform to composition
      compositions_rob_list[[i]] <- clrInv(clr_composition)
  }
list <- compositions_rob_list
length(list)
df <- do.call(rbind, list)
colMeans(df)
colMeans(pi_df)
cov(df)
cov(pi_df)
norm(cov(pi_df) - cov(df), type = "F")
# TODO: clean pca structure in mcem algorithm (scale and x)
# TODO: implement vector structure instead of matrix for the whole algorithm
# TODO: backtransform the principal components
# TODO: check scores calculation in pcaCoDa


optim_result <- optim(rep(0, length = length(comp_pca$pca$sdev)), conditional_scores_log_ilr_vs3 , gr = gradient_cslc_vs1,
                          x_data_i = comp_pca$x_data[[1]], pca = comp_pca$pca,
                          basis_matrix = t(V),
                          control = list(fnscale = -1), method = "BFGS")
# Expectation step
r <- 10
k <- 1
lambda <- 1
proposal_scores <- list(length(prepared_data$x_data))
weights <- list(length(prepared_data$x_data))
sum_exp <- TRUE

      for (i in seq_along(prepared_data$x_data)){
        optim_result <- optim(rep(0L, length = length(pca$sdev)),
                              conditional_scores_log_ilr_vs3b,
                              gr = gradient_cslc_vs1,
                              x_data_i = prepared_data$x_data[[i]],
                              pca = pca,
                              basis_matrix = t(prepared_data$H),
                              control = list(fnscale = -1),
                              method = "BFGS")
    
        scores_median <- as.vector(optim_result$par)
        proposal_scores[[i]] <- sapply(1L:(r * k), function(t) {
          matrix(rnorm(length(scores_median),
                       mean = scores_median,
                       sd = lambda * pca$sdev))
        })

        log_weights <- apply(proposal_scores[[i]], 2L, function(scores) {
          conditional_scores_log_ilr_vs3b(scores,
                                          prepared_data$x_data[[i]],
                                          pca,
                                          t(prepared_data$H)) -
            sum(dnorm(scores,
                      mean = scores_median,
                      sd = lambda * pca$sdev,
                      log = TRUE))
        })
        if (sum_exp == TRUE) {
          weights[[i]] <- stabilize_weights(log_weights)
        } else {
          max_log_weight <- max(log_weights)
          weights <- exp(log_weights - max_log_weight)
          normalized_weights <- weights / sum(weights)
        }
      }

```
