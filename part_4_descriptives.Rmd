---
title: "Descriptive analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(targets)
library(compositions)
library(dplyr)
library(ggplot2)
library(gridExtra)
```


# Core KL15 data

Für den Anwendungsteil der Masterarbeit wird der Datensatz des Marinekerns KL15 vom Gulf of Aden (WGS 84 Coordinates: 12.85738 Grad North, 47.41707
Grad East) genutzt. Dies geschah in Zusammenarbeit mit der working group
“paleoclimate dynamics” of the geoscientific institute of the University of Potsdam, welche die Arbeit mit den Sedimentdaten fachlich betreute. 
The core was already extracted in 1987, but new advantages in the
field of X-ray fluorescence (XRF) scanning caused a renewed interest in the sediment data recently.
In general the XRF scanning method produces counts of elements from the core that can be used as proxies
for for climate relevant processes, like marine productivity, calcium carbonate
deposition and dust deposition (Smol 2015).


The dataset contains 2157 observations of counts for 13 elements. Each observation covers a depth of 1 cm and is dated by an age model. Age control was provided by AMS 14C dating for the upper 3 m of the core and stable oxygen isotopes 
for the lower 19 m of the core. The oldest observation is dated 546,000 years BP (before present). The count data for each element are derived from XRF scans of the marine sediment resulting in a compositional representation of elements for each 
observation. **The data is rescaled to counts per centisecond** (TODO: explain) Each observation can be formalized as a compositional vector: 

$\boldsymbol{x}_i = [x_{i 1}, x_{i 2}, ..., x_{i D}]$ for $i = 1, \ldots, n$ and $D=13$.

The counts of elements are understood as proportions of a total. Accordingly to definition [TODO], each compositional part carries relative information and is a strictly positive number (Pawlowski-Glahn et al. 2015). 
<!-- The dataset is clearly of compositional character with the relative information of the elements in the composition being of greater importance than the absolute information of the element counts. -->

The XRF scanning technique allows for high resolution, non-destructive analysis of the marine core. The variations in the compositions of elements can be used to characterize climate-relavant processes over the past. 
However, the data by itself suffers from potential measurement and sampling errors. One obvious reason is that the sediment cores (cf. Appendix Figure 2) contain areas of a low density of elements, which results in 
low count totals for specific depths. Overall, the sum of the counts is subject to high variation, which is caused by the varying density of elements captured by the XRF scanner. Outliers in the sum of counts may indicate 
potential measurement errors (cf. Appendix Figure 1). Additionally, the age model does not result in constant time differences between each observation, but contains a lot of variation leading to time regions with a 
relative low number of observations. 

From this perspective, the data is "sparsely observed" in at least two dimensions. Firstly, it contains observations with a very low sum of counts, indicating problems with the reliability of the measurements. 
Secondly, the observations are sparsely sampled over time. Due to these factors, the dataset can be considered as "sparsely observed" discrete (count) compositions (Filzmoser et al. 2018, van den Boogart and Tolosana-Delgado 2013). 

In the case of "sparsely observed" compositional data, Steyer and Greven (2023) argue that a latent density model should be applied. With this approach, we are able to account for measurement errors that arise as a result of 
sampling errors, which negatively affect the estimation of principal components (Pavlu et al. 2024).

Following the sample size $m_i$ is plotted over time: TODO

## Element counts as proxies

Im Fachbereich der geowissenschaftlichen Paleoklimatischen Forschung ist es ein ausgewiesenes Ziel, die klimatologischen Prozesse in der Vergangenheit zu verstehen. Dafür werden u.a. Elemente oder Mineralien, die 
in Sedimenten gefunden werden, als Proxies für klimatologische Prozesse verwendet. In diesem Kapitel soll kurz erläutert werden, welche klimatologischen Prozesse bei der Betrachtung von marinen Sedimenten relevant sind 
und welche Elemente in der Forschung als Proxies für diese Prozesse verwendet werden.

In the field of geoscientific research that focuses on paleoclimatic phases, understanding climatic processes of the past is a well-established objective. For this purpose, elements or minerals found in marine sediments 
can be used as proxies for those processes. This section briefly explains which climatological processes are most relevant when examining marine sediments and which elements are commonly used as proxies for these processes 
in current research.

Im Folgenden sollen die für die Analyse von marinen Sedimenten relevanten Prozesse kurz erläutert werden. Diese können mit klimatologischen Prozessen, wie den Glazial-Interglazial Zyklen also den Veränderungen zwischen Eiszeiten und Warmzeiten, in Verbindung stehen, wobei 
die Verknüpfung nicht Teil dieser Arbeit ist. Wichtig für die Interpretation des Datensatzes ist lediglich, welche Elemente in der Forschung bisher als besonders relavant herausgestellt wurden.
Zu den relevanten Prozessen zählen biogene Produktivität, terrestrischer Eintrag und organische Produktion. In bisher erschienen Studien mariner Sedimente wurde insbesonder Calcium als Proxy für 
marine Produktivität herausgearbeitet. Im Gegensatz zur biogenischen Produktivität, also der Produktivität im Meer die insbesonder zu Ablagerungen von Calciumcarbonat im Meeresboden führt, wird oft der 
terrestrische Eintrag untersucht. Dieser beschreibt die Menge an terrestrischem Material, welches in den Ozean eingetragen wird. Für den Golf von Aden sind hierfür besonders Saharawinde verantwortlich, 
für die oft Eisen oder auch Titanium als Proxy verwendet werden. Bei der Dimension terrestrischer Eintrag wird darüber hinaus auch zwischen fluvialem und eolian Input unterschieden. Dafür wird im speziellen Fall die ratio von Eisen und Titanium 
als Proxy herangezogen. 


Darüber hinaus gibt es Studien, die Silicium und Kalium als Proxy für terrigenen Eintrag untersuchen. 
Calcium, Eisen und Titan zählen zu den am häufigsten untersuchten Proxies und sind in der Forschung auch die Elemente, welche am stärksten mit den Galzial-Interglazialen Zyklen in Verbindung gesetzt werden. 
Als weiteres wichtiges Element in der Zusammensetzung mariner Sedimente ist Bromium, das mit der Produktion von organischem Material in Verbindung gebracht wird. 
Es ist anzumerken, dass die Verbindung von Elementen und Proxies im Fachbereich ausführlich diskutiert wird und das sich hinter den hier aufgeführten Zusammenhängen eine hohe Komplexität von inhaltlichen Argumenten, sowie 
geologischen und chemischen Prozessen verbirgt. Zum Beispiel ist anzumerken, dass alle Element diagenitischen Prozessen der Zersetzung unterliegen, die selbst Bestandteil laufender Forschung sind. 
Als Orientierung für die explorative Analyse des Datensatzes ist es jedoch erforderlich einen kurzen Überblick über grundlegende, und weitesgehend akzeptierte Zusammenhänge 
zu geben. 

## Descriptive statistics

Im Folgenden werden für den kompositionalen Datensatz die deskriptiven Statistiken berechnet und erläutert. Das Ziel dabei ist einen Überblick über die Kompositionsanteile und ihr 
Verhältnis zueinander zu erhalten. Dies soll im Hinblick darauf geschehen, dass sowohl einzelne als auch Gruppen von Elementen als Proxies für klimatologische Prozesse angesehen werden 
können. Um die Variation dieser Prozesse besser verstehen zu können, ist es wichtig das Verhalten der vermeintlichen Proxies im Zusammenhang zu betrachten, d.h. als Komposition relativer 
Anteile. 

In the following section, descriptive statistics for the compositional dataset are calculated and explained. The aim is to obtain an overview of the compositional parts and their relationships 
to each other. This analysis is particularly relevant as both individual elements and groups of elements can serve as proxies for climatological processes. TODO: explain climatogical processes earlier (in count compositions) 
To better understand the variation of these processes, it is crucial to examine the behavior of the presumed proxies in context, that is, as a composition of relative proportions.

Der von der Forschungsgruppe "paleoclimatic dynamics" der Universität Potsdam frei zur Verfügung gestellte Datensatz enthält die Zähldaten von 13 Elemente, die in Fugure [..] aufgelistet werden. Dazu ist jede 
Beobachtung mit der Variable "Tiefe" und "Alter" versehen. Für ein grundlegendes Verständnis der relativen Bezüge der Elemente ist es wichtig den Kompositionsdatensatz descriptiv zu erfassen. Dafür werden 
im folgenden die bereits von Aitchison vorgeschlagenen Schätzungen (Vgl. Kapitel [...] für center, variation matrix und total variation berechnet.


```{r load data, eval=TRUE}
data <- tar_read(data_kl15)
data <- data*0.01

data_qf <- tar_read(missings_depth)
data_comp <- tar_read(data_kl15_comp)
data_acomp <- acomp(data_comp)

data_clr <- tar_read(data_kl15_comp_clr) %>% as.data.frame()
data_ilr <- tar_read(data_kl15_comp_ilr) %>% as.data.frame()
data_alr <- tar_read(data_kl15_comp_alr) %>% as.data.frame()

variables <- colnames(data_comp)
```

One fact that qualifies the *rescaled* kl15 dataset as a sparsely observed dataset is the high variation in the sum of counts that leads to compositional parts with a very low number
of counts.

```{r counts summary, eval=TRUE}
summary(data)

```


```{r plot line sum, eval=TRUE}
ggplot(data, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend compositional sum over Time") +
  theme_minimal()
```

```{r aggreagte summary, eval=TRUE}
summary(data$aggregate)
boxplot(data$aggregate)
hist(data$aggregate)
```

The idea is to draw samples from the empirical distribution of the density of all samples sizes to simulate the specific context.

```{r density estimation, eval=TRUE}
density_estimate <- density(data$aggregate)
plot(density_estimate)

# Create sampling function from density estimate
sample_from_density <- function(n) {
    sample(density_estimate$x, size = n, prob = density_estimate$y, replace = TRUE)
}

# Draw 1000 samples
new_samples <- sample_from_density(1000)
```

```{r boxplot for transformed coordinates, eval=TRUE,  fig.width=10, fig.height=30}

par(mfrow = c(3,1))
boxplot(data_clr,
        main = "Boxplots of the clr coordinates for each element", las = 2)
# boxplot(data_clr[, variables[variables != "Ca_Area"]], main="Boxplots of the clr coordinates for each element", las = 2)
boxplot(data_ilr,
        main = "Boxplots of the ilr coordinates for each element", las = 2)
boxplot(data_alr,
        main = "Boxplots of the alr coordinates for each element", las = 2)
par(mfrow = c(1,1))
```

TODO: Do I want to inlcude that? We can see that the transformed coordinates are much less diverged than the original count values.

```{r center of composition, eval=TRUE}
row <- mean(data_acomp)
row
df_row <- data.frame(name = names(row), value = as.vector(row))

# Create the bar plot
ggplot(df_row, aes(x = name, y = value)) +
  geom_bar(stat = "identity") +
  labs(x = "Element", y = "Value", title = "Sample Center") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Sample Variance
mvar(data_acomp) 

mean(clr(data_acomp))
mean(clr(data_acomp), robust=TRUE)
```

As we can see the mean composition is clearly dominated by the element counts of Calcium. If we would do our analysis in absolute counts the variation of this element would overshadow all
other releationships.But since we work in a compositional framework, we can use methods to put emphasis on the relative differences between the elements instead.

The ilr-mean (true) is 
 [1] -0.3422397  2.6225870 -0.1486677  1.3785359 -3.1594463 -0.4760258
 [7]  1.7429442 -0.3013453  1.0633686  4.2942048 -0.3787930  1.5942359

### Compositional mean

 See Boogart et al. 2013, p. 74:

\begin{equation}
\overline{\mathbf{x}}=\frac{1}{N} \odot \bigoplus_{n=1}^{N} \mathbf{x}_{n}=\operatorname{clr}^{-1}\left(\frac{1}{N} \sum_{n=1}^{N} \operatorname{clr}\left(\mathbf{x}_{n}\right)\right)=\mathscr{C}\left[\exp \left(\frac{1}{N} \sum_{n=1}^{N} \ln \left(\mathbf{x}_{n}\right)\right)\right]
\label{def:mean}
\end{equation}

citet[~p.74]{vandenBoogaart2013AnalyzingCD}

$$
\overline{\mathbf{x}}=\frac{1}{N} \odot \bigoplus_{n=1}^{N} \mathbf{x}_{n}=\operatorname{clr}^{-1}\left(\frac{1}{N} \sum_{n=1}^{N} \operatorname{clr}\left(\mathbf{x}_{n}\right)\right)=\mathscr{C}\left[\exp \left(\frac{1}{N} \sum_{n=1}^{N} \ln \left(\mathbf{x}_{n}\right)\right)\right], \tag{4.1}
$$

```{r compositional mean, eval=TRUE}
data <- tar_read(data_kl15)
data_sel <- data[4:ncol(data)-1]
colnames(data_sel) <- gsub("_cts", "", colnames(data_sel))
x <- acomp(data_sel)
x_m <- mean(x)
x_m_clr <- mean(clr(x))
# non compositional mean
colMeans(data_sel)
acomp(colMeans(data_sel))

df_row <- data.frame(name = names(x_m), value = as.vector(x_m), value_clr = as.vector(x_m_clr))

# Create the bar plot
plot1 <- ggplot(df_row, aes(x = name, y = value)) +
  geom_bar(stat = "identity", fill = "#9ebcda") +
  labs(x = "Element", y = "Percentage") +
  theme_grey() +
  # theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  # coord_flip() +
  theme(plot.title = element_text(hjust = 0.5),
  axis.text.x = element_text(angle = 45, hjust = 1, , color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title.x = element_text(color = "black"),
    axis.title.y = element_text(color = "black"))

# Create the bar plot
plot3 <- ggplot(df_row, aes(x = name, y = value_clr)) +
  geom_bar(stat = "identity", fill = "#9ebcda") +
  labs(x = "Element", y = "clr-coefficients") +
  theme_grey() +
  # theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  # coord_flip() +
  theme(plot.title = element_text(hjust = 0.5),
  axis.text.x = element_text(angle = 45, hjust = 1, , color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title.x = element_text(color = "black"),
    axis.title.y = element_text(color = "black"))


# over time
plot2 <- ggplot(data, aes(x = age, y = aggregate)) +
  geom_line(color = "#9ebcda") +
  geom_point(color = "#9ebcda") +
  theme_minimal() +
  labs(x = "Age", y = "Sample size", title = "Sample size per observation over time") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(plot1,plot3, ncol = 2)

summary(data)
```


```{r variation matrix, eval=TRUE}
var_matrix <- variation(x)
norm_var_matrix <- 1/sqrt(2) * var_matrix
tau_var_matrix <- exp(- norm_var_matrix)

# total variance
mvar(x)

xi_matrix <- (norm_var_matrix / (ncol(x) * mvar(x))) * (ncol(x)*(ncol(x)-1))
test <- (norm_var_matrix / mvar(x)) *(ncol(x)-1)
```

#### Centering, scaling and standardization

As in any other multivariate analysis, we can center the data by subtracting the center of composition from each row. This also gives us the neutral element of the simplex: $\mathbf{X}^{*}=\mathbf{X} \ominus \overline{\mathbf{x}}$

```{r centering, eval=TRUE}
mean(data_acomp-mean (data_acomp))
```

Usually we want to scale the data to have a variance of 1 and to be able to compare different variables.
Standardization in the compositional framework is done by:
$$
\mathbf{Z}=\frac{1}{\sqrt{\operatorname{mvar}(\mathbf{X})}} \odot(\mathbf{X} \ominus \overline{\mathbf{x}})
$$

```{r standardization, eval=TRUE}
mn = mean(data_acomp)
mvr = mvar(data_acomp)
x_std = (data_acomp-mn)/sqrt(mvr)
mvar(x_std)
```

Especially for ternary diagramms, it is often very helpful to center the data around one dimension.

#### Metric Variance and Standard deviation

$$
\begin{equation*}
\operatorname{mvar}(\mathbf{X})=\frac{1}{N-1} \sum_{n=1}^{N} d_{A}^{2}\left(\mathbf{x}_{n}, \overline{\mathbf{x}}\right) \tag{4.2}
\end{equation*}
$$

$$
\operatorname{msd}(X)=\sqrt{\frac{1}{D-1} \operatorname{mvar}(X)}
$$

```{r metric variance, eval=TRUE}
mvar(data_acomp)
msd(data_acomp)
```

These parameters can be used to compare subcompositions with the full composition.

Overall, we can not use typicall methods of explanatory analysis like correlations and covariances, since we always deal with **spurious** correlation due to the closure in the simplex.

Instead of a Covariance Matrix a **variation matrix** is proposed (Aitchison 1986):
$$
\begin{equation*}
\tau_{i j}=\operatorname{var}\left(\ln \frac{x_{i}}{x_{j}}\right) \tag{4.3}
\end{equation*}
$$

```{r variation matrix, eval=TRUE}
variation(data_acomp)
summary(data_acomp)$mean.ratio
```

This matrix is symmetric and a small variance implies good "proportionality". Boogart etl al. 2013 provide a very good visualisation for the interpretation of the mean and variation parameters
given a additive-logistic-normal distribution.

The mean ratio gives the center each pairwise ratio. We can also calculate the distribution of pairwise ratios for all observations. These statistics are shown in the following boxplot:

```{r boxplot of pairwise variations, eval=TRUE,  fig.width=10, fig.height=30}
boxplot(data_acomp)
abline(h = 0, col = "red", lty = 2)
```

### Projections and Balances

Balances are a way to compare subcompositions with each other. 

The orthonormal bases, we use for projections, can be of great importance for interpretation of the data. This becomes clear, when we use balances to construct an orthonormal basis. 


### PCA as exploratory tool

We can use `princomp`to compute PCA by singular value decomposition:

$$
\operatorname{clr}\left(\mathbf{X}^{*}\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^{t}
$$

The original count data is centered before applying the PCA. 

```{r PCA exploratory 1, eval=TRUE}
pcx <- princomp(data_acomp)
plot(pcx, type="screeplot")
# normalized eigenvalues
pcx$sdev^2/sum(pcx$sdev^2)
```

interestingly, the screeplot revails at least four singular values of importance.

```{r PCA results, eval=TRUE}
pcx$loadings[,1:4]
```


We can calculate the proportion of variance explained by the first two and the first four components:

```{r PCA exploratory 2, eval=TRUE}
sum(pcx$sdev[1:2]^2)/sum(pcx$sdev^2)
sum(pcx$sdev[1:4]^2)/sum(pcx$sdev^2)
```

That is 70% and 91% of the variance respectively and a strong argument to take the first four components into account (or that the data is flowed by structural measurement errors).

The biplot is one of the most helfpfull exploratory tools. For now, we plot observation numbers instead of points since we
want to preserve some information about the time dimension.


```{r PCA exploratory 3, eval=TRUE}
x <- data_acomp
opar <- par(mar=c(1,1,1,1))
dots = rep(".", times=nrow(x))
biplot(pcx, choices = 1:2, xlabs=dots)
# plot(pcx, type="biplot", scale=1)
par(opar)
```

We should analyse the relationships between the other components as well, f.e. PC1 and PC3:

```{r PCA exploratory 4, eval=TRUE}
biplot(pcx, choices = c(1,3), xlabs=dots)
```


One important step is to identify **links** between elements, i.e. shared dimensions. One interesting candidate could be:
- Br, S and Ca
- Si, Sr and Mg

Another is to identify low-variance subcompositions. Possible candidates are:
- Ca, Sr, Ru
- Si, Al, K
- Zr, Fe, Ti, Rb

Let's check of proportionality holds with these elements:

```{r PCA exploratory 5, eval=TRUE}
mvar( acomp(x[,c("Ca_cts","Sr_cts","Ru_cts")] ) )
mvar( acomp(x[,c("Si_cts","Al_cts","K_cts")] ) )
mvar( acomp(x[,c("Zr_cts","Fe_cts","Ti_cts","Rb_cts")] ) )
mvar(x)
```

They range between 4.4% of the total variance and 1.5% of the total variance. So these elements seem to be very good candidates for subcompositional analysis.

Another way to analyse subcompositions is to use the Singular Value Decomposition (SVD) of specific subsets. Following we will use 

1. Br, S and Ca
2. Si, Sr and Mg

TODO: plot the tenary diagrams for both subsets & check for orthogolity between Br and Sr, Ca and Zr, Fe, Ti, Rb & between Si, Al, K and Sr, Ca

```{r PCA exploratory 6, eval=TRUE}
elements <- c("Br_cts","S_cts","Ca_cts")
elements <- c("Si_cts","Sr_cts","Mg_cts")
s1 <- clr(x[,elements])
s1_c <- s1 - mean(s1)
# calculate the scala product between the clr coordinates and the second principal component
v <- svd(s1_c)$v[,2]
var(scalar(s1, v))
```

The variance of (1) is around 4% and the variance of (2) is around 1.5% of the total variance.

### Evolution Plots

```{r PCA exploratory 7, eval=TRUE}
loadings(pcx)[,1:2]
colSums( loadings(pcx) )
```

Be aware that we can only interpret coefficients as relative to each other.

```{r PCA exploratory 8, eval=TRUE}
comprel2d = function(data, fixedvar){
  diag(1/data[,fixedvar]) %*% unclass(data)
}

comprel1d = function(data, fixedvar){
  unclass(data)/data[fixedvar]
}

pivot <- "Br_cts"

fk = pcx$scores[,1]
vd = pcx$Loadings[1,]*pcx$sdev[1]

vd = comprel1d(vd, pivot)
mn = pcx$Center

mn = comprel1d(mn, pivot)
matplot(fk, log(comprel2d(x, pivot)), pch=19, col=rainbow(13))
for(i in 1:13){
  abline(a=log(mn[i]), b=log(vd[i]), col=rainbow(13)[i], lwd=2)
}
print(pivot)
```

```{r PCA exploratory 9, eval=TRUE}
fkdens = seq(from=min(fk)*1.1, to=max(fk)*1.1, length.out=200)
compdens = clrInv(outer(fkdens, clr(vd))) + pcx$Center
compdens = comprel2d(compdens,pivot)
etqy = compdens[length(fkdens),]
par(mfrow=c(1,2), mar=c(3,3,1,1))
for(logscale in c("","y")){
  matplot(fk, comprel2d(x,pivot),
  pch=19, col=rainbow(13), log=logscale, cex=0.75)
  matlines(fkdens, compdens, lty=1, col=rainbow(13))
}
text(x=fkdens[length(fkdens)], y=etqy,
labels=colnames(x), pos=2)
```

For Bromium the evolution plots display a clear compositional relationship between Br and most other elements, i.e. an increase in Bromium leads to a relative decline in all other parts.


## Normality Check

Um die gewünschten Prinzipien von Kompositionsanalyse (See chapter []) zu gewährleisten, sollten die Kompositionen in ihrer Koordinatenrepräsentation untersucht 
werden. Eine populäre Verteilung für Kompositionsdaten, die Dirichlet-Verteilung leidet z.B. darunter, dass sie nicht scale invariant ist. Um demenstprechende 
Nachteile zu vermeiden, ist es sinnvoll die Verteilung von Kompositionsdaten auf ihren orthonormalen Koordinaten zu betrachten (Filzmoser et al. 2019, p. 86).  
Eine Möglichkeit ist es, für die orthonormalen Koordinaten in $mathcal{R}^{D-1}$ eine multivariate Normalveretilung anzunehmen. 
Da es unendlich viele Möglichkeiten der Basisrepräsentation gibt, ist es wichtig zu betonen dass die Annahme von Normalität für einen Bestimmten Vektor von 
ilr Koordinaten gilt, diese Annahme sich auf alle Varianten der orthonormalen Koordinaten überträgt (Filzmoser et al. 2019, p. 87).
Hierdrin können wir einen weiteren Anwendungsfall für die Nutzung von ilr-Koordinaten sehen, die einen passenden Grundraum für das Testen 
auf bestimmte Verteilungen bieten, ohne dass die Daten weiteren vorgestellten Annahmen unterliegen und gleichzeitig den Prinzipien der Kompositionsanalyse folgen (Filzmoser et al. 2019, p. 87).  

Da jegliche beliebige orthonormale Basis für die Repräsentation der ilr-Koordinaten gewählt werden kann, ist es üblich die SVD zu nutzen, um ein 
unzweideutige Lösung zu erhalten, die nicht abhängig von der Wahl der Basis ist (Filzmoser et al. 2019, p. 87). TODO: test It
$$
\operatorname{clr}\left(\mathbf{X}^{*}\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^{t}
$$
Wenn wir die Scores in $\mathbf{U}$ als normalverteilt annehmen, folgt aus der SVD, dass die **zentrierten** ilr-Koordinaten einer (D-1)-standard 
Normalverteilung mit unabhängigen Komponenten folgen. 
TODO: Test mit Q-Q Plot

```{r test normality, eval=TRUE, fig.width=12, fig.height=5}
x_ilr <- tar_read(data_kl15_comp_ilr)
x_ilr_sc <- scale(x_ilr, center=TRUE, scale=FALSE)




create_qq_plot <- function(data, var_name) {
  ggplot(data.frame(x = data), aes(sample = x)) +
    stat_qq() +
    stat_qq_line() +
    ggtitle(paste("Q-Q Plot:", var_name)) +
    theme_minimal()
}

# Q-Q Plots für alle Variablen erstellen
qq_plots <- lapply(1:ncol(x_ilr_sc), function(i) {
  create_qq_plot(x_ilr_sc[,i], colnames(x_ilr_sc)[i])
})

grid.arrange(grobs = qq_plots, ncol = 4)
```


