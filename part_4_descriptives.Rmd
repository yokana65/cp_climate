---
title: "Descriptive analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(targets)
library(compositions)
library(dplyr)
library(ggplot2)
library(gridExtra)
```


# Core KL15 data

Für den Anwendungsteil der Masterarbeit wird der Datensatz des Marinekerns KL15 vom Gulf of Aden (WGS 84 Coordinates: 12.85738 Grad North, 47.41707
Grad East) genutzt. Dies geschah in Zusammenarbeit mit der working group
“paleoclimate dynamics” of the geoscientific institute of the University of Potsdam, welche die Arbeit mit den Sedimentdaten fachlich betreute. 
The core was already extracted in 1987, but new advantages in the
field of X-ray fluorescence (XRF) scanning caused a renewed interest in the sediment data recently.
In general the XRF scanning method produces counts of elements from the core that can be used as proxies
for for climate relevant processes, like marine productivity, calcium carbonate
deposition and dust deposition (Smol 2015).


The dataset contains 2157 observations of counts for 13 elements. Each observation covers a depth of 1 cm and is dated by an age model. Age control was provided by AMS 14C dating for the upper 3 m of the core and stable oxygen isotopes 
for the lower 19 m of the core. The oldest observation is dated 546,000 years BP (before present). The count data for each element are derived from XRF scans of the marine sediment resulting in a compositional representation of elements for each 
observation. **The data is rescaled to counts per centisecond** (TODO: explain) Each observation can be formalized as a compositional vector: 

$\boldsymbol{x}_i = [x_{i 1}, x_{i 2}, ..., x_{i D}]$ for $i = 1, \ldots, n$ and $D=13$.

The counts of elements are understood as proportions of a total. Accordingly to definition [TODO], each compositional part carries relative information and is a strictly positive number (Pawlowski-Glahn et al. 2015). 
<!-- The dataset is clearly of compositional character with the relative information of the elements in the composition being of greater importance than the absolute information of the element counts. -->

The XRF scanning technique allows for high resolution, non-destructive analysis of the marine core. The variations in the compositions of elements can be used to characterize climate-relavant processes over the past. 
However, the data by itself suffers from potential measurement and sampling errors. One obvious reason is that the sediment cores (cf. Appendix Figure 2) contain areas of a low density of elements, which results in 
low count totals for specific depths. Overall, the sum of the counts is subject to high variation, which is caused by the varying density of elements captured by the XRF scanner. Outliers in the sum of counts may indicate 
potential measurement errors (cf. Appendix Figure 1). Additionally, the age model does not result in constant time differences between each observation, but contains a lot of variation leading to time regions with a 
relative low number of observations. 

From this perspective, the data is "sparsely observed" in at least two dimensions. Firstly, it contains observations with a very low sum of counts, indicating problems with the reliability of the measurements. 
Secondly, the observations are sparsely sampled over time. Due to these factors, the dataset can be considered as "sparsely observed" discrete (count) compositions (Filzmoser et al. 2018, van den Boogart and Tolosana-Delgado 2013). 

In the case of "sparsely observed" compositional data, Steyer and Greven (2023) argue that a latent density model should be applied. With this approach, we are able to account for measurement errors that arise as a result of 
sampling errors, which negatively affect the estimation of principal components (Pavlu et al. 2024).

Following the sample size $m_i$ is plotted over time: TODO

## Descriptive statistics

Im Folgenden werden für den kompositionalen Datensatz die deskriptiven Statistiken berechnet und erläutert. 

```{r load data, eval=TRUE}
data <- tar_read(data_kl15)
data <- data*0.01

data_qf <- tar_read(missings_depth)
data_comp <- tar_read(data_kl15_comp)
data_acomp <- acomp(data_comp)

data_clr <- tar_read(data_kl15_comp_clr) %>% as.data.frame()
data_ilr <- tar_read(data_kl15_comp_ilr) %>% as.data.frame()
data_alr <- tar_read(data_kl15_comp_alr) %>% as.data.frame()

variables <- colnames(data_comp)
```

One fact that qualifies the *rescaled* kl15 dataset as a sparsely observed dataset is the high variation in the sum of counts that leads to compositional parts with a very low number
of counts.

```{r counts summary, eval=TRUE}
summary(data)

```


```{r plot line sum, eval=TRUE}
ggplot(data, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend compositional sum over Time") +
  theme_minimal()
```

```{r aggreagte summary, eval=TRUE}
summary(data$aggregate)
boxplot(data$aggregate)
hist(data$aggregate)
```

The idea is to draw samples from the empirical distribution of the density of all samples sizes to simulate the specific context.

```{r density estimation, eval=TRUE}
density_estimate <- density(data$aggregate)
plot(density_estimate)

# Create sampling function from density estimate
sample_from_density <- function(n) {
    sample(density_estimate$x, size = n, prob = density_estimate$y, replace = TRUE)
}

# Draw 1000 samples
new_samples <- sample_from_density(1000)
```

```{r boxplot for transformed coordinates, eval=TRUE,  fig.width=10, fig.height=30}

par(mfrow = c(3,1))
boxplot(data_clr,
        main = "Boxplots of the clr coordinates for each element", las = 2)
# boxplot(data_clr[, variables[variables != "Ca_Area"]], main="Boxplots of the clr coordinates for each element", las = 2)
boxplot(data_ilr,
        main = "Boxplots of the ilr coordinates for each element", las = 2)
boxplot(data_alr,
        main = "Boxplots of the alr coordinates for each element", las = 2)
par(mfrow = c(1,1))
```

TODO: Do I want to inlcude that? We can see that the transformed coordinates are much less diverged than the original count values.

```{r center of composition, eval=TRUE}
row <- mean(data_acomp)
df_row <- data.frame(name = names(row), value = as.vector(row))

# Create the bar plot
ggplot(df_row, aes(x = name, y = value)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Element", y = "Value", title = "Sample Center") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Sample Variance
mvar(data_acomp) 

mean(clr(data_acomp))
mean(clr(data_acomp), robust=TRUE)
```

As we can see the mean composition is clearly dominated by the element counts of Calcium. If we would do our analysis in absolute counts the variation of this element would overshadow all
other releationships.But since we work in a compositional framework, we can use methods to put emphasis on the relative differences between the elements instead.

The ilr-mean (true) is 
 [1] -0.3422397  2.6225870 -0.1486677  1.3785359 -3.1594463 -0.4760258
 [7]  1.7429442 -0.3013453  1.0633686  4.2942048 -0.3787930  1.5942359

#### Centering, scaling and standardization

As in any other multivariate analysis, we can center the data by subtracting the center of composition from each row. This also gives us the neutral element of the simplex: $\mathbf{X}^{*}=\mathbf{X} \ominus \overline{\mathbf{x}}$

```{r centering, eval=TRUE}
mean(data_acomp-mean (data_acomp))
```

Usually we want to scale the data to have a variance of 1 and to be able to compare different variables.
Standardization in the compositional framework is done by:
$$
\mathbf{Z}=\frac{1}{\sqrt{\operatorname{mvar}(\mathbf{X})}} \odot(\mathbf{X} \ominus \overline{\mathbf{x}})
$$

```{r standardization, eval=TRUE}
mn = mean(data_acomp)
mvr = mvar(data_acomp)
x_std = (data_acomp-mn)/sqrt(mvr)
mvar(x_std)
```

Especially for ternary diagramms, it is often very helpful to center the data around one dimension.

#### Metric Variance and Standard deviation

$$
\begin{equation*}
\operatorname{mvar}(\mathbf{X})=\frac{1}{N-1} \sum_{n=1}^{N} d_{A}^{2}\left(\mathbf{x}_{n}, \overline{\mathbf{x}}\right) \tag{4.2}
\end{equation*}
$$

$$
\operatorname{msd}(X)=\sqrt{\frac{1}{D-1} \operatorname{mvar}(X)}
$$

```{r metric variance, eval=TRUE}
mvar(data_acomp)
msd(data_acomp)
```

These parameters can be used to compare subcompositions with the full composition.

Overall, we can not use typicall methods of explanatory analysis like correlations and covariances, since we always deal with **spurious** correlation due to the closure in the simplex.

Instead of a Covariance Matrix a **variation matrix** is proposed (Aitchison 1986):
$$
\begin{equation*}
\tau_{i j}=\operatorname{var}\left(\ln \frac{x_{i}}{x_{j}}\right) \tag{4.3}
\end{equation*}
$$

```{r variation matrix, eval=TRUE}
variation(data_acomp)
summary(data_acomp)$mean.ratio
```

This matrix is symmetric and a small variance implies good "proportionality". Boogart etl al. 2013 provide a very good visualisation for the interpretation of the mean and variation parameters
given a additive-logistic-normal distribution.

The mean ratio gives the center each pairwise ratio. We can also calculate the distribution of pairwise ratios for all observations. These statistics are shown in the following boxplot:

```{r boxplot of pairwise variations, eval=TRUE,  fig.width=10, fig.height=30}
boxplot(data_acomp)
abline(h = 0, col = "red", lty = 2)
```

### Projections and Balances

Balances are a way to compare subcompositions with each other. 

The orthonormal bases, we use for projections, can be of great importance for interpretation of the data. This becomes clear, when we use balances to construct an orthonormal basis. 


### PCA as exploratory tool

We can use `princomp`to compute PCA by singular value decomposition:

$$
\operatorname{clr}\left(\mathbf{X}^{*}\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^{t}
$$

The original count data is centered before applying the PCA. 

```{r PCA exploratory 1, eval=TRUE}
pcx <- princomp(data_acomp)
plot(pcx, type="screeplot")
# normalized eigenvalues
pcx$sdev^2/sum(pcx$sdev^2)
```

interestingly, the screeplot revails at least four singular values of importance.

```{r PCA results, eval=TRUE}
pcx$loadings[,1:4]
```


We can calculate the proportion of variance explained by the first two and the first four components:

```{r PCA exploratory 2, eval=TRUE}
sum(pcx$sdev[1:2]^2)/sum(pcx$sdev^2)
sum(pcx$sdev[1:4]^2)/sum(pcx$sdev^2)
```

That is 70% and 91% of the variance respectively and a strong argument to take the first four components into account (or that the data is flowed by structural measurement errors).

The biplot is one of the most helfpfull exploratory tools. For now, we plot observation numbers instead of points since we
want to preserve some information about the time dimension.


```{r PCA exploratory 3, eval=TRUE}
x <- data_acomp
opar <- par(mar=c(1,1,1,1))
dots = rep(".", times=nrow(x))
biplot(pcx, choices = 1:2, xlabs=dots)
# plot(pcx, type="biplot", scale=1)
par(opar)
```

We should analyse the relationships between the other components as well, f.e. PC1 and PC3:

```{r PCA exploratory 4, eval=TRUE}
biplot(pcx, choices = c(1,3), xlabs=dots)
```


One important step is to identify **links** between elements, i.e. shared dimensions. One interesting candidate could be:
- Br, S and Ca
- Si, Sr and Mg

Another is to identify low-variance subcompositions. Possible candidates are:
- Ca, Sr, Ru
- Si, Al, K
- Zr, Fe, Ti, Rb

Let's check of proportionality holds with these elements:

```{r PCA exploratory 5, eval=TRUE}
mvar( acomp(x[,c("Ca_cts","Sr_cts","Ru_cts")] ) )
mvar( acomp(x[,c("Si_cts","Al_cts","K_cts")] ) )
mvar( acomp(x[,c("Zr_cts","Fe_cts","Ti_cts","Rb_cts")] ) )
mvar(x)
```

They range between 4.4% of the total variance and 1.5% of the total variance. So these elements seem to be very good candidates for subcompositional analysis.

Another way to analyse subcompositions is to use the Singular Value Decomposition (SVD) of specific subsets. Following we will use 

1. Br, S and Ca
2. Si, Sr and Mg

TODO: plot the tenary diagrams for both subsets & check for orthogolity between Br and Sr, Ca and Zr, Fe, Ti, Rb & between Si, Al, K and Sr, Ca

```{r PCA exploratory 6, eval=TRUE}
elements <- c("Br_cts","S_cts","Ca_cts")
elements <- c("Si_cts","Sr_cts","Mg_cts")
s1 <- clr(x[,elements])
s1_c <- s1 - mean(s1)
# calculate the scala product between the clr coordinates and the second principal component
v <- svd(s1_c)$v[,2]
var(scalar(s1, v))
```

The variance of (1) is around 4% and the variance of (2) is around 1.5% of the total variance.

### Evolution Plots

```{r PCA exploratory 7, eval=TRUE}
loadings(pcx)[,1:2]
colSums( loadings(pcx) )
```

Be aware that we can only interpret coefficients as relative to each other.

```{r PCA exploratory 8, eval=TRUE}
comprel2d = function(data, fixedvar){
  diag(1/data[,fixedvar]) %*% unclass(data)
}

comprel1d = function(data, fixedvar){
  unclass(data)/data[fixedvar]
}

pivot <- "Br_cts"

fk = pcx$scores[,1]
vd = pcx$Loadings[1,]*pcx$sdev[1]

vd = comprel1d(vd, pivot)
mn = pcx$Center

mn = comprel1d(mn, pivot)
matplot(fk, log(comprel2d(x, pivot)), pch=19, col=rainbow(13))
for(i in 1:13){
  abline(a=log(mn[i]), b=log(vd[i]), col=rainbow(13)[i], lwd=2)
}
print(pivot)
```

```{r PCA exploratory 9, eval=TRUE}
fkdens = seq(from=min(fk)*1.1, to=max(fk)*1.1, length.out=200)
compdens = clrInv(outer(fkdens, clr(vd))) + pcx$Center
compdens = comprel2d(compdens,pivot)
etqy = compdens[length(fkdens),]
par(mfrow=c(1,2), mar=c(3,3,1,1))
for(logscale in c("","y")){
  matplot(fk, comprel2d(x,pivot),
  pch=19, col=rainbow(13), log=logscale, cex=0.75)
  matlines(fkdens, compdens, lty=1, col=rainbow(13))
}
text(x=fkdens[length(fkdens)], y=etqy,
labels=colnames(x), pos=2)
```

For Bromium the evolution plots display a clear compositional relationship between Br and most other elements, i.e. an increase in Bromium leads to a relative decline in all other parts.


## Normality Check

Um die gewünschten Prinzipien von Kompositionsanalyse (See chapter []) zu gewährleisten, sollten die Kompositionen in ihrer Koordinatenrepräsentation untersucht 
werden. Eine populäre Verteilung für Kompositionsdaten, die Dirichlet-Verteilung leidet z.B. darunter, dass sie nicht scale invariant ist. Um demenstprechende 
Nachteile zu vermeiden, ist es sinnvoll die Verteilung von Kompositionsdaten auf ihren orthonormalen Koordinaten zu betrachten (Filzmoser et al. 2019, p. 86).  
Eine Möglichkeit ist es, für die orthonormalen Koordinaten in $mathcal{R}^{D-1}$ eine multivariate Normalveretilung anzunehmen. 
Da es unendlich viele Möglichkeiten der Basisrepräsentation gibt, ist es wichtig zu betonen dass die Annahme von Normalität für einen Bestimmten Vektor von 
ilr Koordinaten gilt, diese Annahme sich auf alle Varianten der orthonormalen Koordinaten überträgt (Filzmoser et al. 2019, p. 87).
Hierdrin können wir einen weiteren Anwendungsfall für die Nutzung von ilr-Koordinaten sehen, die einen passenden Grundraum für das Testen 
auf bestimmte Verteilungen bieten, ohne dass die Daten weiteren vorgestellten Annahmen unterliegen und gleichzeitig den Prinzipien der Kompositionsanalyse folgen (Filzmoser et al. 2019, p. 87).  

Da jegliche beliebige orthonormale Basis für die Repräsentation der ilr-Koordinaten gewählt werden kann, ist es üblich die SVD zu nutzen, um ein 
unzweideutige Lösung zu erhalten, die nicht abhängig von der Wahl der Basis ist (Filzmoser et al. 2019, p. 87). TODO: test It
$$
\operatorname{clr}\left(\mathbf{X}^{*}\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^{t}
$$
Wenn wir die Scores in $\mathbf{U}$ als normalverteilt annehmen, folgt aus der SVD, dass die **zentrierten** ilr-Koordinaten einer (D-1)-standard 
Normalverteilung mit unabhängigen Komponenten folgen. 
TODO: Test mit Q-Q Plot

```{r test normality, eval=TRUE, fig.width=12, fig.height=5}
x_ilr <- tar_read(data_kl15_comp_ilr)
x_ilr_sc <- scale(x_ilr, center=TRUE, scale=FALSE)




create_qq_plot <- function(data, var_name) {
  ggplot(data.frame(x = data), aes(sample = x)) +
    stat_qq() +
    stat_qq_line() +
    ggtitle(paste("Q-Q Plot:", var_name)) +
    theme_minimal()
}

# Q-Q Plots für alle Variablen erstellen
qq_plots <- lapply(1:ncol(x_ilr_sc), function(i) {
  create_qq_plot(x_ilr_sc[,i], colnames(x_ilr_sc)[i])
})

grid.arrange(grobs = qq_plots, ncol = 4)
```


