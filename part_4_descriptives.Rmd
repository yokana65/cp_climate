---
title: "Descriptive analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
library(targets)
library(compositions)
```


# Core KL15 data

The dataset contains 2157 observations of counts for 13 elements. Each observation covers a depth of 1 cm and is dated by an age model. Age control was provided by AMS 14C dating for the upper 3 m of the core and stable oxygen isotopes 
for the lower 19 m of the core. The oldest observation is dated 546,000 years BP (before present). The count data for each element are derived from XRF scans of the marine sediment resulting in a compositional representation of elements for each 
observation. Each observation can be formalized as a compositional vector: 

$\boldsymbol{x}_i = [x_{i 1}, x_{i 2}, ..., x_{i D}]$ for $i = 1, \ldots, n$ and $D=13$.

The counts of elements are understood as proportions of a total. Accordingly to definition [TODO], each compositional part carries relative information and is a strictly positive number (Pawlowski-Glahn et al. 2015). 
<!-- The dataset is clearly of compositional character with the relative information of the elements in the composition being of greater importance than the absolute information of the element counts. -->

The XRF scanning technique allows for high resolution, non-destructive analysis of the marine core. The variations in the compositions of elements can be used to characterize climate-relavant processes over the past. 
However, the data by itself suffers from potential measurement and sampling errors. One obvious reason is that the sediment cores (cf. Appendix Figure 2) contain areas of a low density of elements, which results in 
low count totals for specific depths. Overall, the sum of the counts is subject to high variation, which is caused by the varying density of elements captured by the XRF scanner. Outliers in the sum of counts may indicate 
potential measurement errors (cf. Appendix Figure 1). Additionally, the age model does not result in constant time differences between each observation, but contains a lot of variation leading to time regions with a 
relative low number of observations. 

From this perspective, the data is "sparsely observed" in at least two dimensions. Firstly, it contains observations with a very low sum of counts, indicating problems with the reliability of the measurements. 
Secondly, the observations are sparsely sampled over time. Due to these factors, the dataset can be considered as "sparsely observed" discrete (count) compositions (Filzmoser et al. 2018, van den Boogart and Tolosana-Delgado 2013). 

In the case of "sparsely observed" compositional data, Steyer and Greven (2023) argue that a latent density model should be applied. With this approach, we are able to account for measurement errors that arise as a result of 
sampling errors, which negatively affect the estimation of principal components (Pavlu et al. 2024).

Following the sample size $m_i$ is plotted over time: TODO

## Descriptive statistics

Im Folgenden werden für den kompositionalen Datensatz die deskriptiven Statistiken berechnet und erläutert. 

```{r load data, eval=TRUE}
data <- tar_read(data_kl15)
data_qf <- tar_read(missings_depth)
data_comp <- tar_read(data_kl15_comp)
data_acomp <- acomp(data_comp)

data_clr <- tar_read(data_kl15_comp_clr) %>% as.data.frame()
data_ilr <- tar_read(data_kl15_comp_ilr) %>% as.data.frame()
data_alr <- tar_read(data_kl15_comp_alr) %>% as.data.frame()

variables <- colnames(data_comp)
```

```{r plot line sum, eval=TRUE}
ggplot(data, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend compositional sum over Time") +
  theme_minimal()
```

TODO: Do I want to inlcude that? We can see that the transformed coordinates are much less diverged than the original count values.

```{r boxplot for transformed coordinates, eval=TRUE,  fig.width=10, fig.height=30}

par(mfrow = c(3,1))
boxplot(data_clr,
        main = "Boxplots of the clr coordinates for each element", las = 2)
# boxplot(data_clr[, variables[variables != "Ca_Area"]], main="Boxplots of the clr coordinates for each element", las = 2)
boxplot(data_ilr,
        main = "Boxplots of the ilr coordinates for each element", las = 2)
boxplot(data_alr,
        main = "Boxplots of the alr coordinates for each element", las = 2)
par(mfrow = c(1,1))
```


```{r center of composition, eval=TRUE}
row <- mean(data_acomp)
df_row <- data.frame(name = names(row), value = as.vector(row))

# Create the bar plot
ggplot(df_row, aes(x = name, y = value)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Element", y = "Value", title = "Sample Center") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Sample Variance
mvar(data_acomp) 

mean(x)
mean(x, robust=TRUE)
```

As we can see the mean composition is clearly dominated by the element counts of Calcium. If we would do our analysis in absolute counts the variation of this element would overshadow all
other releationships.But since we work in a compositional framework, we can use methods to put emphasis on the relative differences between the elements instead.



#### Centering, scaling and standardization

As in any other multivariate analysis, we can center the data by subtracting the center of composition from each row. This also gives us the neutral element of the simplex: $\mathbf{X}^{*}=\mathbf{X} \ominus \overline{\mathbf{x}}$

```{r centering, eval=TRUE}
mean(x-mean (x))
```

Usually we want to scale the data to have a variance of 1 and to be able to compare different variables.
Standardization in the compositional framework is done by:
$$
\mathbf{Z}=\frac{1}{\sqrt{\operatorname{mvar}(\mathbf{X})}} \odot(\mathbf{X} \ominus \overline{\mathbf{x}})
$$

```{r standardization, eval=TRUE}
mn = mean(x)
mvr = mvar(x)
x_std = (x-mn)/sqrt(mvr)
mvar(x_std)
```

Especially for ternary diagramms, it is often very helpful to center the data around one dimension.

#### Metric Variance and Standard deviation

$$
\begin{equation*}
\operatorname{mvar}(\mathbf{X})=\frac{1}{N-1} \sum_{n=1}^{N} d_{A}^{2}\left(\mathbf{x}_{n}, \overline{\mathbf{x}}\right) \tag{4.2}
\end{equation*}
$$

$$
\operatorname{msd}(X)=\sqrt{\frac{1}{D-1} \operatorname{mvar}(X)}
$$

```{r metric variance, eval=TRUE}
mvar(x)
msd(x)
```

These parameters can be used to compare subcompositions with the full composition.

Overall, we can not use typicall methods of explanatory analysis like correlations and covariances, since we always deal with **spurious** correlation due to the closure in the simplex.

Instead of a Covariance Matrix a **variation matrix** is proposed (Aitchison 1986):
$$
\begin{equation*}
\tau_{i j}=\operatorname{var}\left(\ln \frac{x_{i}}{x_{j}}\right) \tag{4.3}
\end{equation*}
$$

```{r variation matrix, eval=TRUE}
variation(x)
summary(x)$mean.ratio
```

This matrix is symmetric and a small variance implies good "proportionality". Boogart etl al. 2013 provide a very good visualisation for the interpretation of the mean and variation parameters
given a additive-logistic-normal distribution.

The mean ratio gives the center each pairwise ratio. We can also calculate the distribution of pairwise ratios for all observations. These statistics are shown in the following boxplot:

```{r boxplot of pairwise variations, eval=TRUE,  fig.width=10, fig.height=30}
boxplot(x, )
```

### Projections and Balances

Balances are a way to compare subcompositions with each other. 

The orthonormal bases, we use for projections, can be of great importance for interpretation of the data. This becomes clear, when we use balances to construct an orthonormal basis. 


### PCA as exploratory tool

We can use `princomp`to compute PCA by singular value decomposition:

$$
\operatorname{clr}\left(\mathbf{X}^{*}\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^{t}
$$

The original count data is centered before applying the PCA. 

```{r PCA exploratory 1, eval=TRUE}
pcx <- princomp(x)
plot(pcx, type="screeplot")
```

interestingly, the screeplot revails at least four singular values of importance.

We can calculate the proportion of variance explained by the first two and the first four components:

```{r PCA exploratory 2, eval=TRUE}
sum(pcx$sdev[1:2]^2)/sum(pcx$sdev^2)
sum(pcx$sdev[1:4]^2)/sum(pcx$sdev^2)
```

That is 70% and 91% of the variance respectively and a strong argument to take the first four components into account (or that the data is flowed by structural measurement errors).

The biplot is one of the most helfpfull exploratory tools. For now, we plot observation numbers instead of points since we
want to preserve some information about the time dimension.


```{r PCA exploratory 3, eval=TRUE}
opar <- par(mar=c(1,1,1,1))
dots = rep(".", times=nrow(x))
biplot(pcx, choices = 1:2, xlabs=dots)
# plot(pcx, type="biplot", scale=1)
par(opar)
```

We should analyse the relationships between the other components as well, f.e. PC1 and PC3:

```{r PCA exploratory 4, eval=TRUE}
biplot(pcx, choices = c(1,3), xlabs=dots)
```


One important step is to identify **links** between elements, i.e. shared dimensions. One interesting candidate could be:
- Br, S and Ca
- Si, Sr and Mg

Another is to identify low-variance subcompositions. Possible candidates are:
- Ca, Sr, Ru
- Si, Al, K
- Zr, Fe, Ti, Rb

Let's check of proportionality holds with these elements:

```{r PCA exploratory 5, eval=TRUE}
mvar( acomp(x[,c("Ca_cts","Sr_cts","Ru_cts")] ) )
mvar( acomp(x[,c("Si_cts","Al_cts","K_cts")] ) )
mvar( acomp(x[,c("Zr_cts","Fe_cts","Ti_cts","Rb_cts")] ) )
mvar(x)
```

They range between 4.4% of the total variance and 1.5% of the total variance. So these elements seem to be very good candidates for subcompositional analysis.

Another way to analyse subcompositions is to use the Singular Value Decomposition (SVD) of specific subsets. Following we will use 

1. Br, S and Ca
2. Si, Sr and Mg

TODO: plot the tenary diagrams for both subsets & check for orthogolity between Br and Sr, Ca and Zr, Fe, Ti, Rb & between Si, Al, K and Sr, Ca

```{r PCA exploratory 6, eval=TRUE}
elements <- c("Br_cts","S_cts","Ca_cts")
elements <- c("Si_cts","Sr_cts","Mg_cts")
s1 <- clr(x[,elements])
s1_c <- s1 - mean(s1)
# calculate the scala product between the clr coordinates and the second principal component
v <- svd(s1_c)$v[,2]
var(scalar(s1, v))
```

The variance of (1) is around 4% and the variance of (2) is around 1.5% of the total variance.

### Evolution Plots

```{r PCA exploratory 7, eval=TRUE}
loadings(pcx)[,1:2]
colSums( loadings(pcx) )
```

Be aware that we can only interpret coefficients as relative to each other.

```{r PCA exploratory 8, eval=TRUE}
comprel2d = function(data, fixedvar){
  diag(1/data[,fixedvar]) %*% unclass(data)
}

comprel1d = function(data, fixedvar){
  unclass(data)/data[fixedvar]
}

pivot <- "Al_cts"

fk = pcx$scores[,1]
vd = pcx$Loadings[1,]*pcx$sdev[1]

vd = comprel1d(vd, pivot)
mn = pcx$Center

mn = comprel1d(mn, pivot)
matplot(fk, log(comprel2d(x, pivot)), pch=19, col=rainbow(13))
for(i in 1:13){
  abline(a=log(mn[i]), b=log(vd[i]), col=rainbow(13)[i], lwd=2)
}
print(pivot)
```

```{r PCA exploratory 9, eval=TRUE}
fkdens = seq(from=min(fk)*1.1, to=max(fk)*1.1, length.out=200)
compdens = clrInv(outer(fkdens, clr(vd))) + pcx$Center
compdens = comprel2d(compdens,pivot)
etqy = compdens[length(fkdens),]
par(mfrow=c(1,2), mar=c(3,3,1,1))
for(logscale in c("","y")){
  matplot(fk, comprel2d(x,pivot),
  pch=19, col=rainbow(13), log=logscale, cex=0.75)
  matlines(fkdens, compdens, lty=1, col=rainbow(13))
}
text(x=fkdens[length(fkdens)], y=etqy,
labels=colnames(x), pos=2)
```

For Bromium the evolution plots display a clear compositional relationship between Br and most other elements, i.e. an increase in Bromium leads to a relative decline in all other parts.
