---
title: "Compositional Data Analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

# Introduction

This is where you can write an introduction to your document.

Install the required packages:

```{r eval=TRUE, include=FALSE}
if (!require(rmarkdown)) {
  install.packages("rmarkdown")
}

if (!require(targets)) {
  install.packages("targets")
}

if (!require(visNetwork)) {
  install.packages("visNetwork")
}

if (!require(ggplot2)) {
  install.packages("ggplot2")
}
if (!require(robCompositions)) {
  install.packages("robCompositions")
}

# call libraries
# TODO: make this part more lenient
library(ggplot2)
library(compositions)
library(dplyr)
```


## Arguments

The paper uses two pairs as proxy for aridity: Ti/Al and Ti/Ca. The first pair is used for the last 200kyr and the second pair for the last 1200kyr. The data is from the KL15 core.
A proposed alternative is the use of K as a proxy for aridity.

### environmental processes

with literature (from Croudace ez Rothwell, chapter 2). 

# Data overview

### Questions:

- [] In the KL15_xrf data, what is the difference between the Area estimates and the DArea? (it seems Area is used trough the analysis)

## ODP

This is the data from the Ocean Drilling Program close to Cyprus. Data contains several Cores, an age model and XRF data. XRF data is a proxy for the chemical composition of the sediment. 

The analysis is done with Ti.AL as a proxy for wetness (?). 
  

## CHB 14

## KL 15

In the XRF data we have the following variables:

```{r, eval=TRUE}
data <- tar_read(data_kl15)
colnames(data)
```

Variable selection is done in the target for data_kl15. Nevertheless, we need a list of element names for labeling.

```{r, eval=TRUE}
# data_select <- data[, c("depth", "age", "Br_Area", "Rb_Area", "Sr_Area", "Zr_Area", "Ru_Area", "Mg_Area",
#  "Al_Area", "Si_Area", "S_Area", "K_Area", "Ca_Area", "Ti_Area", "Fe_Area")]



variables <- c("Br_Area", "Rb_Area", "Sr_Area", "Zr_Area", "Ru_Area", "Mg_Area",
 "Al_Area", "Si_Area", "S_Area", "K_Area", "Ca_Area", "Ti_Area", "Fe_Area")

# # for compositional data, we are interested in the sum of all components
# data_select$aggregate <- rowSums(data_select[3:ncol(data_select)])
summary(data)
```


## Exploratory DA

Lets do some boxplots for our variables:

```{r boxplot counts, eval=TRUE}
par(mfrow = c(1,2))
boxplot(data[, variables], main="Boxplots of the Element counts", names = variables, las = 2)
boxplot(data[, variables[variables != "Ca_Area"]], main="Boxplots of the Element counts", names = variables[variables != "Ca_Area"], las = 2)
par(mfrow = c(1,1))
```

We do have dominant role of Ca. Mg and Rb don't show relevant variation (at first glance) and can probably be dropped in later steps of analysis (better argumentation is needed though).
Variance of Sr, Si and Fe is also more dominant than the other elements (is this reflected in the Simplex as well?). 

We want to analyse the Area minerals together with a time component, which gives us a Compositional Time Series (CTS).

The compositional data does not sum up to a constant value. In fact the spread of its sum is rather big.
We want to know how the sum of the compositional parts is behaving over time:

```{r plot line sum, eval=TRUE}
ggplot(data, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend compositional sum over Time") +
  theme_minimal()
```

It looks like its getting more compact with time until there is a turning point (which might make sense from a geological perspective?).
Anyway, the non constant makes sense since we probably don't measure all components of the sediment.

Due to the high variation of the sum, we could apply the concept of sparsely sampled data (Vgl. Greven et Steyer, 2023). The idea is that
we assume that we only pull a sample from the whole composition. 


Just for fun and orientation, let's have a look at the distribution of elements at a certain time point:
```{r plot observation, eval=TRUE}
row <- data[1, 3:(ncol(data)-1)]
# Convert the row to a data frame and reshape it to a long format
df <- as.data.frame(t(row))
df$Element <- rownames(df)
colnames(df)[1] <- "Value"
df$Value <- as.numeric(df$Value)

# Create the bar plot
ggplot(df, aes(x = Element, y = Value)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Element", y = "Value", title = "Values of Elements for 1 row") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

In addition, let's plot important compositional parts (not in the Simplex though!) over time:

```{r plot parts over time, eval=TRUE, fig.width=12, fig.height=6}
par(mfrow = c(4,1))

# plot data with age and Ca_Area
ggplot(data, aes(x = age, y = Ca_Area)) +
geom_line() +  geom_smooth(method = "loess") +
labs(x = "Time BP", y = "CA", title = "Count values for Ca over time") +
  theme_minimal()

ggplot(data, aes(x = age, y = Fe_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "Fe", title = "Count values for Fe over time") +
  theme_minimal()

ggplot(data, aes(x = age, y = Sr_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "Sr", title = "Count values for Sr over time") +
  theme_minimal()


ggplot(data, aes(x = age, y = K_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "K", title = "Count values for K over time") +
  theme_minimal()
```

There are a few hints for extraordinary events that might have a strong influence on the linear optimization problem:
- ~280 BP (and 410 BP) there is a Crash in Ca and Sr
Compare:
"Element ratios found useful as proxies by Rothwell et al. (2006) are:
â€¢ Sr/Ca (indicating high-Sr aragonite requiring a shallow-water source)"
=> Actually the ration does not change at all at this events. But the ratio of Sr & Ca to all other might change.
But when we take the compositional sum into account it looks like these events are characterized by low aggregated counts due to the crash in Ca and Sr counts, which might be a problem with the Cores
an measurement error of an area where the elementary density of the sediment is low (why?). In any case we should have in mind that these extreme count variations have a strong influence on the optimization algorithm of PCA.
But this effect is much stronger when dealing with absolute counts than with compositional parts. 
**That could be an example to show the effect of spurios correlation and the advantage of CoDa**. 


#### Further Covariates or additional flags


```{r, eval=FALSE}
data_kl15_agem <- read.table("data/Africa_NE_200/data/data_KL15-2_smooth_spline_ages.txt", header = TRUE, sep = "\t")

data_kl15_agem <- tar_read()
summary(data_kl15_agem)

```

This appears to be confidence intervals for the age data. "accrate" is the accumulation rate, which indicates the speed of sedimentation.

We also need the data with "quality flags"

```{r, eval=FALSE}
data_kl15_qf <- read.table("data/Africa_NE_200/data/data_KL15_qf.txt", header = TRUE, sep = "\t")

summary(data_kl15_qf)

```




- script_read_data_KL15_orbitalforcing
- script_read_data_KL15_odp967
- script_read_data_KL15_odp722
- script_read_data_KL15_odp721_722
- script_readco2_age
- script_read_data_KL15_CHB_K_RRM_MHT500



# Data Analysis

Data reading functionality is stored in the */scripts* directory. However, targets are used for the rendering process of this html. 

```{r, eval=FALSE, include=FALSE}
# run the read_data_200kyr_all.R script
source("scripts/read_data_200kyr_all.r")
```

## Compositional Data Analysis

As an entree point, we have to two R packages at hand for compositional data analysis: `compositions` and `robCompositions`. 
The first one is more general (and more popular)[https://www.rdocumentation.org/packages/compositions/versions/2.0-8] and the 
second one is more focused on robust statistics, including ("robust" PCA)[https://www.rdocumentation.org/packages/robCompositions/versions/2.4.1].

In the following we take a first look at centered log ratios (clr), which are the most recommended technique. Anyway, bafter some deliberation it seems that 
isometric log ratios (ilr) are more appropriate for our data and methods, that is for data centered around one dimension (Ca) ? and for PCA.

### Exploratory Analysis

Let's do the boxplot with clr coordinates:

```{r, eval=TRUE,  fig.width=10, fig.height=30}
data_clr <- tar_read(data_kl15_comp_clr) %>% as.data.frame()
data_ilr <- tar_read(data_kl15_comp_ilr) %>% as.data.frame()
data_alr <- tar_read(data_kl15_comp_alr) %>% as.data.frame()


par(mfrow = c(3,1))
boxplot(data_clr,
        main = "Boxplots of the clr coordinates for each element", las = 2)
# boxplot(data_clr[, variables[variables != "Ca_Area"]], main="Boxplots of the clr coordinates for each element", las = 2)
boxplot(data_ilr,
        main = "Boxplots of the ilr coordinates for each element", las = 2)
boxplot(data_alr,
        main = "Boxplots of the alr coordinates for each element", las = 2)
par(mfrow = c(1,1))
```

We can see that the transformed coordinates are much less diverged than the original count values.

### Centered log ratios

Without further ado, since this is the most robust transformation for compositional data.

```{r, eval=TRUE}
data_comp <- tar_read(data_kl15_comp)

# What happens with the relative information if we constrain each row to sum up to 1?
data_comp_1 <- constSum(data_comp, const = 1) 
# all(rowSums(data_comp_1) == 1)
# data_comp_1$aggregate <- rowSums(data_comp_1)
ratio_ti_al <- data_comp$Ti_Area / data_comp$Al_Area
head(ratio_ti_al)
ratio_ti_al_1 <- data_comp_1$Ti_Area / data_comp_1$Al_Area
head(ratio_ti_al_1)
# Bang!
```

That looks nice, but is the relative information (i.e. the ratio of one mineral to the others) more important than the absolute value (i.e. the amount of CA that is in the sediment)?

By the way, the transformation above proves that relative information is scale invariant while absolute information is not.

We can calculate the Aitchison distance between all elements for each point of time:
```{r, eval=TRUE}
aDist(data_comp[1:7, ])
```

Now we can calculate the clr transformation for the data:
```{r, eval=TRUE}
clr_coord <- cenLR(data_comp)$x.clr
ggplot(clr_coord, aes(x = Ti_Area, y = Al_Area)) +
  geom_point(size = 2) +
  xlab("Ti Area (clr)") +
  ylab("Al Area (clr)")
```

**Interpretation:** 
In very unprecise terms: The log of the ratio of equal components is zero, i.e. they are of equal importance.
The clr transformation takes the natural log of each value in a row and divides it by the square root of the products of the remaining parts.
In very unqualified language, that is the relevant information of one part in relation to all the others. If the
clr is positive, the part is more important than the average, if it is negative, it is less important.

So here Al and Ti clearly have a linear trend, i.e. if one is less important then so is the other. Also the spread is higher for more
more negative values, which makes sense since their importance goes into oblivion.

### Dendrogram

```{r, eval=TRUE}
# TODO: the dataset needs to be transformed to a pivot table?
res <- hclust(dist(data_comp_1), method = "single")
plot(res)
```

Thats not what we want since we calculate the distances between observations. But it is actually really interesting to see. What is wrong with **observation 17!**
And additionaly clusters of observations (that is times) are relevant as well. 

But now, how do we get the dendogram for the distances between the elements?

```{r, eval=TRUE}
res <- hclust(dist(t(data_comp_1)), method = "single")
plot(res)
```

So at first sight, that is disappointing. Even regarding relative information Ca clearly dominates the picture. 



Also we should try other clustering methods then single linkage.

```{r, eval=TRUE}
res <- hclust(dist(t(data_comp_1)), method = "ward.D2")
plot(res)
```

Does not change much.

There are ways out of that dilemma. We could use isometric transformations with Ca as pivot!

### PCA

Steps of analysis are build on: https://www.geo.fu-berlin.de/en/v/soga-r/Advances-statistics/Multivariate-approaches/Principal-Component-Analysis/PCA-the-basics/Data-preparation/index.html

Something to have in mind is that PCA is scale sensitive, while CoDa is scale invariant. That is an additional reason for the transformation in clr, alr or ilr. It also illustrates the fact that
any steps are unneccassary in CoDaPCA. 

With robCompositions: 

**Aim**: 
- to reduce our 14-D space without loosing too much of the relative information, that is the compositional aspects of the data.
- describe the relation between the observations (in time) and the compositional parts.


In the orthonormal space we calculated independent ilr coordinates (why not clr?).


#### methods

The general idea of PCA is a reduction in dimensionality. But from the persepctive of identifying relevant environmental processes it might be as well the deduction of non-relevant

There is a bunch of methods to calculate PCA.

- SVD: Single Vector Decomposition
- Eigenvector decomposition

With `robCompositions` we can use the `pcaCoDa` function, which uses eigen-decomposition of the covariates. 
The composition is represented in orthonormal coordinates, which are the ilr coordinates, and then standard PCA is applied.


We can calculate the eigenvalues and eigenvectors for CoDa:

```{r eigenvalues for euclidean data, eval=TRUE}
data_co <- tar_read(data_kl15_comp)
pca_cov <- cov(data_co)
pca_eigen <- eigen(pca_cov)
rownames(pca_eigen$vectors) <- colnames(data_co)
pca_eigen$values

```

The eigenvalues are an indication of th-e explained variance from each component and thus used to order the components. The sum of the eigenvalues is equal to the 
the total variance of the data (cp. chapter 28 SODA).

The eigenvectors of the count data are as follows:

```{r display result, eval=TRUE}
pca_eigen$vectors
```

If we compare them with the eigenvectors of CoDa, we see clear differences and from the first point of view a lineare Deconstruction is only possible in CoDa (f.e. with absolute counts the first component is only Ca).

Interpretation for values in euclidean space:
We can already infer from the numerical values that the first component is almost exclusively composed of Ca (with roughly 50% of explained variance). Depending on the geological interpretation, it might be usefull to visNetwork
Ca as a basic component that we are not that interested in and want to take out of the equation to put a focus on the other compositional parts?

If we compare that with our results from the transformed coordinates we can clearly see the difference between variation in absolute count numbers vs. variation of transformed coordinates in the simplex, what in my point of view, is a strong argument 
that PCA should only be considered for transformed data. All in all variations in relative information are very different from absolute variations (also see the boxplots).

#### Ternary Diagram

We can use ternary Diagrams to explore relative information for each component: 

```{r ternaryDiag, eval=TRUE}

data_3_comp <- data[, c("Br_Area", "Sr_Area", "Fe_Area")]
ternaryDiag(data_3_comp, line = "pca")
```

There is something off with this diagram. TODO: investigate

#### PCA calculation

The 'robCompositions' package gives a practical functionlaity to do PCA, but its inner workings are not very clear (yet).

```{r pcaCoDa, eval=TRUE}


## compositional PCA, non-robust
p_comp <- pcaCoDa(data[, variables], method = "classical")
## compositional PCA, robust
set.seed(234) # to reproduce the result exactly
p_comp_rob <- pcaCoDa(data[, variables], method = "robust")
summary(p_comp)
summary(p_comp_rob)
plot(p_comp_rob, type = "l")
```

We can see that there is no big jump in the explanatory power of the components. For later analysis, we need to qualify the meaningfulness of each component (See chapter environmental processes).
Also the Kaiser criterion can be used to determine the number of components. 

The 'robCompositions' package seems to make high level transformations if the input. 

```{r str(pcaCoDa), eval=FALSE}
str(p_comp_rob)
```


The PCA is calculated (with 'compositions') with a Singular Value Decomposition:
 $$
\operatorname{clr}\left(\mathbf{X}^*\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^t
$$

The columns of $V$ are the *loadings* or *principle components* of the clr-transformed data points. $D$ is a diagonal matrix with the "eigenvalues", which values can be interpretated as the sd of the coordinates on the new orthonormal basis ($V$).
$U$ contains the scores or standardized coordinates, which we can use to derive synthetic data. (Cp. Boogart et al., page 177)


```{r pca_rs, eval=TRUE, warning=FALSE}
acomp <- acomp(data_co)
pca_rs <- princomp(acomp)
# str(pca_rs)
pca_rs$loadings
pca_rs$sdev
```


Scores: Interpretation(?), but can be used to reconstruct the project values in $\boldsymbol{x}_{\mathrm{i}}^{\prime}$
**Relevant for time analysis**: We can identify how much each obersavtion contributes to a component. In case we can actually identify the latent processes, we can observe the time frames when they are most relevant in the data.

```{r display scores, eval=FALSE}
head(p_comp_rob$scores)
head(pca_rs$scores)
```


Loadings: In euclidean space indication of the contribution of each compositional part to the respective component

```{r display loadings, eval=FALSE}
p_comp_rob$loadings
pca_rs$loadings
```

For 'compositios' results non significant values are not shown. But the explained variance can't be right. 
We can calculate the explained variance for the first three components:

```{r explained variance, eval=TRUE}
sum(pca_rs$sdev[1:3]^2/mvar(acomp))
```
83% is consistent with the results from 'robCompositions'.

Eigenvalues: sum up to one and indicate the contribution of each component to the total variance

```{r display eigenvalues, eval=TRUE}
p_comp_rob$eigenvalues
```


#### Exploratory Analysis of Components

Let's plot the components in relation to the (dominant) first component:

TODO! => check robCompositions to reconstruct the plots below since the choice argument does not work as intended.

```{r biplot pcaCoDa, eval=TRUE, fig.width=10, fig.height=30}
par(mfrow=c(5,1), mar = c(4,4,2,2))
# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob)

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(2,3))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(2,2))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(1,5), xlabs = rownames(data))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(1,6), xlabs = rownames(data))

par(mfrow=c(1,1))

```

That also looks like we can work with. While the first component seems to be associated with Ca (no surprise) and Sr, the second is a combination of Zr, Ti and Fe and maybe most interestingly Br seems to take
a special place and is probably dominant in the third component.

TODO: **Calculate scores for the components and plot them over time.**
TODO: Validate result and compare it with euclidean distances -> explain the difference!



#### Calculation of $\boldsymbol{x}_{\mathrm{i}}^{\prime}$

We can use principal component to reconstruct the data with all or some of the components. One idea would be calculate an "average" composition and show the influence of every component on the compositional parts (over time).

```{r loadings pcaCoda, eval=TRUE}
(pca_loading <- pca_rs$loadings[, 1:2])

# calculate squared sum of loadings to check if the sum is 1
apply(pca_loading^2, 2, sum)
```

Since  $\boldsymbol{x}_{\mathrm{i}}^{\prime}$ is a linear combination of the observations and their loadings, we can use them to reconstruct the original data (plus error).

Remember that we need the transformed variables in the Simplex. To get the variables in Euclidean space (i.e. the original count space we need to retransform the result).


```{r calc synthetic observations, eval=TRUE}
# first observation projected on PC1
# remember that we need the transformed variables in the Simplex. To get the variables in Euclidean space (i.e. the original count space we need to retransform the result)
obs1 <- acomp[2, ]
obs1_matrix <- matrix(as.numeric(obs1), nrow = 1)
obs1_matrix %*% pca_loading[, 1]
```
This value is the projection of our 13-dimensional transformed composition (i.e the first observation) of it onto the first component. It is also the score value for the first observation and PC1, which is nothing else 
than the linear transformation of the first observation:
$$
z_{i 1}=\phi_{11} \mathbf{x}_{i 1}+\phi_{21} \mathbf{x}_{i 2}, \ldots, \phi_{d 1} \mathbf{x}_{i d} i=1,2, \ldots, n
$$

Those linear transformations can be very helpful to analyse compositional variations over time (f.e. which component is more dominant over time).

Unfortunately, it is not possible to clearly interpret the clr-coordinates (real or synthetic). If we deal with synthetic data, we need to transform them back to euclidean space (which is not possible for ilr? -> check!)


```{r head scores, eval=TRUE}
# head(pca_rs$scores)
Z <- matrix(as.numeric(acomp), nrow = 2119) %*% pca_rs$loadings
```

The scores in the pca object are not correct!


```{r time analysis scores, eval=TRUE}
data <- tar_read(data_kl15)
agem <- data$age
Z <- cbind(Z, agem)

ggplot(Z, aes(x = agem, y = Z[, 1])) +
    geom_line() +
    geom_smooth(method = "loess") +
    labs(x = "Time BP", y = "Scores PC1", title = "Scores for PC1 over time") +
    theme_minimal()
```



### Theorie

A clr coordinate contains the relative information of one part related to all the others. I am not a geologist, but if that is what we are looking for, might depend on 
the process of sedimentation. And to be honest, I have no idea about that one.

## DA Functions

```{r, eval=TRUE, include=FALSE}
graph_ts_200ka <- function(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1) {
    # with targets we can use read_target to load the data
    d <- get(datasets[index])

    #*******************************200ka Plot of wentness index*************************************
    # Create the plot
    if (!display) {
      png(filename = paste0("graphs/200ka_polygonplot", datasets[index],".png"))
    }

    yt <- d[,2] # your time series y values
    xt <- d[,1] # your time series age values

    # Assuming d is a matrix with two columns
    mean_yt <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # Define ylim based on 3-sigma range
    # y_min <- mean_yt - 5 * std_dev
    # y_max <- mean_yt + 5 * std_dev
    
    #***************************Plotting the graph********************************
    plot(xt, yt, type = "n", xlab = "", ylab = "", xlim = range(xt), ylim = range(yt), main = paste0("200ka Plot of ", datasets[index]))

    # Create the filled areas
    # age matrix
    fill_x <- c(xt, rev(xt))
    # Terrigenous matrix
    fill_y <- c(yt, rep(mean_yt, length(yt)))
    fill_y[yt < mean_yt] <- mean_yt
    fill_y[1] <- mean_yt
    fill_y[length(fill_y)] <- mean_yt
    color_rgb <- col2rgb('#D95319') / 255
    polygon(fill_x, fill_y, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    fill_y2 <- c(yt, rep(mean_yt, length(yt)))
    fill_y2[yt > mean_yt] <- mean_yt
    fill_y2[1] <- mean_yt
    fill_y2[length(fill_y2)] <- mean_yt
    color_rgb <- col2rgb('#0072BD') / 255
    polygon(fill_x, fill_y2, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    if (!display) {
        # Close the graphics device
        dev.off()
        return()
    }
    
    # #*******************alternative ggplot graph********************************
    # library(ggplot2)

    # # Assuming d is a data frame with two columns
    # mean_val <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # # Define ylim based on 3-sigma range
    # y_min <- mean_val - 5 * std_dev
    # y_max <- mean_val + 5 * std_dev

    # # Create a data frame for the filled areas
    # fill_pos_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # negative values are replaced with the mean value
    # fill_pos_data$y[fill_data$y < mean_val] <- mean_val

    # # Create a data frame for the filled areas
    # fill_neg_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # positive values are replaced with the mean value
    # fill_neg_data$y[fill_data$y >= mean_val] <- mean_val


    # # Create the plot
    # p <- ggplot(d, aes(x = yt, y = xt)) +
    # geom_line() +
    # geom_polygon(data = fill_pos_data, aes(x = x, y = y), 
    #             fill = rgb(217/255, 83/255, 25/255, alpha = 0.9), 
    #             color = NA) +
    # geom_polygon(data = fill_neg_data, aes(x = x, y = y), 
    #             fill = rgb(173/255, 216/255, 230/255, alpha = 0.9), 
    #             color = NA) +
    # ylim(y_min, y_max) +
    # theme_minimal() +
    # labs(x = "Time (kyrs BP)", y = "", title = paste0("200ka Plot of Wentness Index of ", datasets[1]))

    # ggsave(filename = paste0("200ka_plot_",datasets[index],".png"), plot = p, path = "graphs/", width = 10, height = 6, dpi = 300)
}

```


  ```{r, eval=FALSE}
  # this function is an artifact of the translation of mathplot functions and requires the read script to run beforehand
  graph_ts_200ka(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1)
  ```

  ToDo: It would be more  informative to have point data instead of polygons.


### spearman cross correlation analysis

The first step is to reproduce the cross correlation analysis from the paper (especially figure 3).





# Convert Matlab files and reproduce Paper graphs


## Read the files

The following reads data objects from matlab files.

```{r, eval=FALSE}
# Install the R.matlab package if it's not already installed
if (!require(R.matlab)) {
  install.packages("R.matlab")
}

# Load the R.matlab package
library(R.matlab)

# Use the readMat() function to read a .mat file
data <- readMat("data/Africa_NE_200/data_pentagramm_5_1200_025kyr_XCorr.mat")
```

# targets

Visualize the status of the targets pipeline:

```{r, eval=TRUE}
tar_visnetwork()
```