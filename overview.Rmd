---
title: "Compositional Data Analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

# Introduction

This is where you can write an introduction to your document.

Install the required packages:

```{r preinstall libraries, eval=TRUE, include=FALSE}
sapply(
        c("rmarkdown", "targets", "visNetwork", "ggplot2", "robCompositions", "compositions"), 
          function(pkg) if (!require(pkg, character.only = TRUE)) install.packages(pkg)
          )

# call libraries
library(ggplot2)
library(compositions)
library(dplyr)
```


## Arguments

The paper (Markus F. et al.) uses two pairs as proxy for aridity: Ti/Al and Ti/Ca. The first pair is used for the last 200kyr and the second pair for the last 1200kyr. The data is from the KL15 core.
A proposed alternative is the use of K as a proxy for aridity.

### environmental processes

with literature (from Croudace ez Rothwell, chapter 2). 

### Sparse sampling

It makes sense to assume that the observations are not equally distributed over the whole sample space and that we have areas where observations are sparse (from a density perspective: that would mean where we have very low count numbers OR very low counts overall for specific depths). This has a few consequences: TODO

### Assumptions

1. Discrete distributions of Elements are not directly observed, but trough a sample.

We can account for the sampling error (which is highly relevant for PCA) by substracting the variance-covariance estimated for the error (by bootstraping from the sample -> see Doobs delta-method) from the sample covariance-variance.

2. We observe "outliers" which diffuse the time-dependend processes and are results of specific natural phenomones like a tsunami or else (?).

3. The relative information stored in proportions between elements is more important than the absolute count number of an element.

# Data overview

Get the data objects from targets:

```{r load data objects, eval=TRUE}
data <- tar_read(data_kl15)
data_qf <- tar_read(missings_depth)
data_comp <- tar_read(data_kl15_comp)

data_clr <- tar_read(data_kl15_comp_clr) %>% as.data.frame()
data_ilr <- tar_read(data_kl15_comp_ilr) %>% as.data.frame()
data_alr <- tar_read(data_kl15_comp_alr) %>% as.data.frame()

variables <- colnames(data_comp)

```

### Questions:

- [] In the KL15_xrf data, what is the difference between the Area estimates and the DArea? (it seems Area is used trough the analysis)


## KL 15

In the XRF data we have the following variables:

```{r read data, eval=TRUE}

colnames(data)
```

Variable selection is done in the target for data_kl15. Nevertheless, we need a list of element names for labeling.

```{r data summary, eval=TRUE}
# data_select$aggregate <- rowSums(data_select[3:ncol(data_select)])
summary(data)
```


## Exploratory DA

Lets do some boxplots for our variables:

```{r boxplot counts, eval=TRUE}
par(mfrow = c(1,2))
boxplot(data[, variables], main="Boxplots of the Element counts", names = variables, las = 2)
boxplot(data[, variables[variables != "Ca_Area"]], main="Boxplots of the Element counts", names = variables[variables != "Ca_Area"], las = 2)
par(mfrow = c(1,1))
```

We do have dominant role of Ca. Mg and Rb don't show relevant variation (at first glance) and can probably be dropped in later steps of analysis (better argumentation is needed though).
Variance of Sr, Si and Fe is also more dominant than the other elements (is this reflected in the Simplex as well?). 

We want to analyse the Area minerals together with a time component, which gives us a Compositional Time Series (CTS).

The compositional data does not sum up to a constant value. In fact the spread of its sum is rather big.
We want to know how the sum of the compositional parts is behaving over time:

```{r plot line sum, eval=TRUE}
ggplot(data, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend compositional sum over Time") +
  theme_minimal()
```

It looks like its getting more compact with time until there is a turning point (which might make sense from a geological perspective?).
Anyway, the non constant makes sense since we probably don't measure all components of the sediment.

Due to the high variation of the sum, we could apply the concept of sparsely sampled data (Vgl. Greven et Steyer, 2023). The idea is that
we assume that we only pull a sample from the whole composition. 


Just for fun and orientation, let's have a look at the distribution of elements at a certain time point:
```{r plot observation, eval=TRUE}
row <- data[1, 3:(ncol(data)-1)]
# Convert the row to a data frame and reshape it to a long format
df <- as.data.frame(t(row))
df$Element <- rownames(df)
colnames(df)[1] <- "Value"
df$Value <- as.numeric(df$Value)

# Create the bar plot
ggplot(df, aes(x = Element, y = Value)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Element", y = "Value", title = "Values of Elements for 1 row") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

In addition, let's plot important compositional parts (not in the Simplex though!) over time:

```{r plot parts over time, eval=TRUE, fig.width=12, fig.height=6}
par(mfrow = c(4,1))

# plot data with age and Ca_Area
ggplot(data, aes(x = age, y = Ca_Area)) +
geom_line() +  geom_smooth(method = "loess") +
labs(x = "Time BP", y = "CA", title = "Count values for Ca over time") +
  theme_minimal()

ggplot(data, aes(x = age, y = Fe_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "Fe", title = "Count values for Fe over time") +
  theme_minimal()

ggplot(data, aes(x = age, y = Sr_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "Sr", title = "Count values for Sr over time") +
  theme_minimal()


ggplot(data, aes(x = age, y = K_Area)) +
geom_line() +
geom_smooth(method = "loess") +
labs(x = "Time BP", y = "K", title = "Count values for K over time") +
  theme_minimal()
```

There are a few hints for extraordinary events that might have a strong influence on the linear optimization problem:
- ~280 BP (and 410 BP) there is a Crash in Ca and Sr
Compare:
"Element ratios found useful as proxies by Rothwell et al. (2006) are:
• Sr/Ca (indicating high-Sr aragonite requiring a shallow-water source)"
=> Actually the ration does not change at all at this events. But the ratio of Sr & Ca to all other might change.
But when we take the compositional sum into account it looks like these events are characterized by low aggregated counts due to the crash in Ca and Sr counts, which might be a problem with the Cores
an measurement error of an area where the elementary density of the sediment is low (why?). In any case we should have in mind that these extreme count variations have a strong influence on the optimization algorithm of PCA.
But this effect is much stronger when dealing with absolute counts than with compositional parts. 
**That could be an example to show the effect of spurios correlation and the advantage of CoDa**. 

### Distribution of elements

To get an idea about the multivariate distribution of our observations, we want to have a more detailed look at the distribution of each element:

```{r plot distribution, eval=TRUE}
# make a historgram for each column in data

for (var in variables) {
  hist(data[, var], main = paste("Histogram of", var),
       xlab = var, probability = TRUE, col = "blue")
}
```

#### Further Covariates or additional flags


```{r summary agem, eval=FALSE}
data_kl15_agem <- read.table("data/Africa_NE_200/data/data_KL15-2_smooth_spline_ages.txt", header = TRUE, sep = "\t")

summary(data_kl15_agem)
```

This appears to be confidence intervals for the age data. "accrate" is the accumulation rate, which indicates the speed of sedimentation.

We also need the data with "quality flags"

```{r summary quality flags, eval=TRUE}

summary(data_qf)

```




- script_read_data_KL15_orbitalforcing
- script_read_data_KL15_odp967
- script_read_data_KL15_odp722
- script_read_data_KL15_odp721_722
- script_readco2_age
- script_read_data_KL15_CHB_K_RRM_MHT500



# Data Analysis

TODO: do we need z-transformations after the projection onto the Simplex? 
TODO: Plots: differentiate cts, arl, clr, ilr

Data reading functionality is stored in the */scripts* directory. However, targets are used for the rendering process of this html. 

```{r read script, eval=FALSE, include=FALSE}
# run the read_data_200kyr_all.R script
source("scripts/read_data_200kyr_all.r")
```

## Compositional Data Analysis

As an entree point, we have to two R packages at hand for compositional data analysis: `compositions` and `robCompositions`. 
The first one is more general (and more popular)[https://www.rdocumentation.org/packages/compositions/versions/2.0-8] and the 
second one is more focused on robust statistics, including ("robust" PCA)[https://www.rdocumentation.org/packages/robCompositions/versions/2.4.1].

In the following we take a first look at centered log ratios (clr), which are the most recommended technique. Anyway, bafter some deliberation it seems that 
isometric log ratios (ilr) are more appropriate for our data and methods, that is for data centered around one dimension (Ca) ? and for PCA.

### Theory 

The aim of aitchison geometry is to measure dissimilarity between compositions. Since compositions are scale invariant that has to be a measure that accounts for changes in 
relative information rather than obsolute information.

The neutral element on the simplex is the vector with all elements set to 1.

### Exploratory Analysis

Let's do the boxplot with clr coordinates:

```{r boxplot for transformed coordinates, eval=TRUE,  fig.width=10, fig.height=30}



par(mfrow = c(3,1))
boxplot(data_clr,
        main = "Boxplots of the clr coordinates for each element", las = 2)
# boxplot(data_clr[, variables[variables != "Ca_Area"]], main="Boxplots of the clr coordinates for each element", las = 2)
boxplot(data_ilr,
        main = "Boxplots of the ilr coordinates for each element", las = 2)
boxplot(data_alr,
        main = "Boxplots of the alr coordinates for each element", las = 2)
par(mfrow = c(1,1))
```

We can see that the transformed coordinates are much less diverged than the original count values.

### Parameter Estimation of the discrete distribution



Der **Erwartungswert** einer Komposition kann wie folgt berechnet werden:

Die Komposition ist ein Vektor $\mathbf{x} = (x_1, \ldots, x_D)$, wobei $D$ die Anzahl der Komponenten (Elemente) ist und $\sum_{i=1}^D x_i = 1$ (or a arbitrary constant due to the scale invariance of CoDa). 
Die clr-Transformation ist dann definiert als:
$\text{clr}(\mathbf{x}) = \left(\ln\frac{x_1}{g(\mathbf{x})}, \ldots, \ln\frac{x_D}{g(\mathbf{x})}\right)$
wobei $g(\mathbf{x}) = \left(\prod_{i=1}^D x_i\right)^{1/D}$ das geometrische Mittel der Komponenten ist.

Für die clr-transformierten Daten ist der Erwartungswert jeder Komponente berechnen:

$\mathbb{E}[\text{clr}(\mathbf{X})_i] = \mathbb{E}\left[\ln\frac{X_i}{g(\mathbf{X})}\right]$

Dabei ist $\mathbf{X}$ eine Zufallsvariable, die Werte im Simplex annimmt, und $\mathbb{E}$ bezeichnet den Erwartungswert.

TODO: Übertragung auf ilr-Transformation (Isometrie between Euclidean space and Simplex)


Die **Varianz** kann analog dazu berechnet werden:

$\text{Var}[\text{clr}(\mathbf{X})_i] = \mathbb{E}\left[\left(\ln\frac{X_i}{g(\mathbf{X})} - \mathbb{E}\left[\ln\frac{X_i}{g(\mathbf{X})}\right]\right)^2\right]$



### Centered log ratios

Without further ado, since this is the most robust transformation for compositional data.

```{r show scale invariance, eval=TRUE}

# What happens with the relative information if we constrain each row to sum up to 1?
data_comp_1 <- constSum(data_comp, const = 1) 
# all(rowSums(data_comp_1) == 1)
# data_comp_1$aggregate <- rowSums(data_comp_1)
ratio_ti_al <- data_comp$Ti_Area / data_comp$Al_Area
head(ratio_ti_al)
ratio_ti_al_1 <- data_comp_1$Ti_Area / data_comp_1$Al_Area
head(ratio_ti_al_1)
# Bang!
```

That looks nice, but is the relative information (i.e. the ratio of one mineral to the others) more important than the absolute value (i.e. the amount of CA that is in the sediment)?

By the way, the transformation above proves that relative information is scale invariant while absolute information is not.

We can calculate the Aitchison distance between all elements for each point of time:
```{r Aitchison distances, eval=TRUE}
aDist(data_comp[1:7, ])
```

Now we can calculate the clr transformation for the data:
```{r plot clr, eval=TRUE}
clr_coord <- cenLR(data_comp)$x.clr
ggplot(clr_coord, aes(x = Ti_Area, y = Al_Area)) +
  geom_point(size = 2) +
  xlab("Ti Area (clr)") +
  ylab("Al Area (clr)")
```

**Interpretation:** 
In very unprecise terms: The log of the ratio of equal components is zero, i.e. they are of equal importance.
The clr transformation takes the natural log of each value in a row and divides it by the square root of the products of the remaining parts.
In very unqualified language, that is the relevant information of one part in relation to all the others. If the
clr is positive, the part is more important than the average, if it is negative, it is less important.

So here Al and Ti clearly have a linear trend, i.e. if one is less important then so is the other. Also the spread is higher for more
more negative values, which makes sense since their importance goes into oblivion.

### Dendrogram

```{r dendrogram single, eval=TRUE}
# TODO: the dataset needs to be transformed to a pivot table?
res <- hclust(dist(data_comp_1), method = "single")
plot(res)
```

Thats not what we want since we calculate the distances between observations. But it is actually really interesting to see. What is wrong with **observation 17!**
And additionaly clusters of observations (that is times) are relevant as well. 

But now, how do we get the dendogram for the distances between the elements?


So at first sight, that is disappointing. Even regarding relative information Ca clearly dominates the picture. 



Also we should try other clustering methods then single linkage.

```{r dendrogram ward, eval=TRUE}
res <- hclust(dist(t(data_comp_1)), method = "ward.D2")
plot(res)
```

Does not change much.

There are ways out of that dilemma. We could use isometric transformations with Ca as pivot!

### PCA

Steps of analysis are build on: https://www.geo.fu-berlin.de/en/v/soga-r/Advances-statistics/Multivariate-approaches/Principal-Component-Analysis/PCA-the-basics/Data-preparation/index.html

Something to have in mind is that PCA is scale sensitive, while CoDa is scale invariant. That is an additional reason for the transformation in clr, alr or ilr. It also illustrates the fact that
any steps are unneccassary in CoDaPCA. 

With robCompositions: 

**Aim**: 
- to reduce our 14-D space without loosing too much of the relative information, that is the compositional aspects of the data.
- describe the relation between the observations (in time) and the compositional parts.


In the orthonormal space we calculated independent ilr coordinates (why not clr?).


#### methods

The general idea of PCA is a reduction in dimensionality. But from the persepctive of identifying relevant environmental processes it might be as well the deduction of non-relevant

There is a bunch of methods to calculate PCA.

- SVD: Single Vector Decomposition
- Eigenvector decomposition

With `robCompositions` we can use the `pcaCoDa` function, which uses eigen-decomposition of the covariates. 
The composition is represented in orthonormal coordinates, which are the ilr coordinates, and then standard PCA is applied.


We can calculate the eigenvalues and eigenvectors for CoDa:

```{r eigenvalues for euclidean data, eval=TRUE}

pca_cov <- cov(data_comp)
pca_eigen <- eigen(pca_cov)
rownames(pca_eigen$vectors) <- colnames(data_comp)
pca_eigen$values

```

The eigenvalues are an indication of th-e explained variance from each component and thus used to order the components. The sum of the eigenvalues is equal to the 
the total variance of the data (cp. chapter 28 SODA).

The eigenvectors of the count data are as follows:

```{r display result, eval=TRUE}
pca_eigen$vectors
```

If we compare them with the eigenvectors of CoDa, we see clear differences and from the first point of view a lineare Deconstruction is only possible in CoDa (f.e. with absolute counts the first component is only Ca).

Interpretation for values in euclidean space:
We can already infer from the numerical values that the first component is almost exclusively composed of Ca (with roughly 50% of explained variance). Depending on the geological interpretation, it might be usefull to visNetwork
Ca as a basic component that we are not that interested in and want to take out of the equation to put a focus on the other compositional parts?

If we compare that with our results from the transformed coordinates we can clearly see the difference between variation in absolute count numbers vs. variation of transformed coordinates in the simplex, what in my point of view, is a strong argument 
that PCA should only be considered for transformed data. All in all variations in relative information are very different from absolute variations (also see the boxplots).

#### Ternary Diagram

We can use ternary Diagrams to explore relative information for each component: 

```{r ternaryDiag, eval=TRUE}

data_3_comp <- data[, c("Br_Area", "Sr_Area", "Fe_Area")]
ternaryDiag(data_3_comp, line = "pca")
```

There is something off with this diagram. TODO: investigate

#### PCA calculation

The 'robCompositions' package gives a practical functionlaity to do PCA, but its inner workings are not very clear (yet).

```{r pcaCoDa, eval=TRUE}


## compositional PCA, non-robust
p_comp <- pcaCoDa(data[, variables], method = "classical")
## compositional PCA, robust
set.seed(234) # to reproduce the result exactly
p_comp_rob <- pcaCoDa(data[, variables], method = "robust")
summary(p_comp)
summary(p_comp_rob)
plot(p_comp_rob, type = "l")
```

We can see that there is no big jump in the explanatory power of the components. For later analysis, we need to qualify the meaningfulness of each component (See chapter environmental processes).
Also the Kaiser criterion can be used to determine the number of components. 

The 'robCompositions' package seems to make high level transformations if the input. 

```{r str(pcaCoDa), eval=FALSE}
str(p_comp_rob)
```


The PCA is calculated (with 'compositions') with a Singular Value Decomposition:
 $$
\operatorname{clr}\left(\mathbf{X}^*\right)=\mathbf{U} \cdot \mathbf{D} \cdot \mathbf{V}^t
$$

The columns of $V$ are the *loadings* or *principle components* of the clr-transformed data points. $D$ is a diagonal matrix with the "eigenvalues", which values can be interpretated as the sd of the coordinates on the new orthonormal basis ($V$).
$U$ contains the scores or standardized coordinates, which we can use to derive synthetic data. (Cp. Boogart et al., page 177)


```{r pca_rs, eval=TRUE, warning=FALSE}
acomp <- acomp(data_comp)
pca_rs <- princomp(acomp)
# str(pca_rs)
pca_rs$loadings
pca_rs$sdev
```


Scores: Interpretation(?), but can be used to reconstruct the project values in $\boldsymbol{x}_{\mathrm{i}}^{\prime}$
**Relevant for time analysis**: We can identify how much each obersavtion contributes to a component. In case we can actually identify the latent processes, we can observe the time frames when they are most relevant in the data.

```{r display scores, eval=FALSE}
head(p_comp_rob$scores)
head(pca_rs$scores)
```


Loadings: In euclidean space indication of the contribution of each compositional part to the respective component

```{r display loadings, eval=FALSE}
p_comp_rob$loadings
pca_rs$loadings
```

For 'compositios' results non significant values are not shown. But the explained variance can't be right. 
We can calculate the explained variance for the first three components:

```{r explained variance, eval=TRUE}
sum(pca_rs$sdev[1:3]^2/mvar(acomp))
```
83% is consistent with the results from 'robCompositions'.

Eigenvalues: sum up to one and indicate the contribution of each component to the total variance

```{r display eigenvalues, eval=TRUE}
p_comp_rob$eigenvalues
```


#### Exploratory Analysis of Components

Let's plot the components in relation to the (dominant) first component:

TODO! => check robCompositions to reconstruct the plots below since the choice argument does not work as intended.

```{r biplot pcaCoDa, eval=TRUE, fig.width=10, fig.height=30}
par(mfrow=c(5,1), mar = c(4,4,2,2))
# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob)

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(2,3))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(2,2))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(1,5), xlabs = rownames(data))

# biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, choices = c(1,6), xlabs = rownames(data))

par(mfrow=c(1,1))

```

That also looks like we can work with. While the first component seems to be associated with Ca (no surprise) and Sr, the second is a combination of Zr, Ti and Fe and maybe most interestingly Br seems to take
a special place and is probably dominant in the third component.

TODO: **Calculate scores for the components and plot them over time.**
TODO: Validate result and compare it with euclidean distances -> explain the difference!


We can use the loadings to characterize the components.


```{r loadings by elements, eval=TRUE, fig.width=10, fig.height=16}
loadings <- matrix(pca_rs$loadings, nrow = 13, ncol = 12)
rownames(loadings) <- variables

par(mfrow = c(3,2), mar = c(4, 8, 2, 2))

for (i in 1:6) {
  df <- as.data.frame(loadings[,i])
  df$rowname <- rownames(loadings)
  
  barplot(df[,1], 
          horiz = TRUE, 
          names.arg = df$rowname, 
          las = 1, 
          col = ifelse(df[,1] > 0, "#b0451a", "#1fd844"),
          main = paste("Loadings of PC", i),
          xlab = "Loadings")
  
  abline(v = 0, lty = 2)
}

par(mfrow = c(1,1))
```

This graphic gives a very rough idea of the characteristica of the components. A better method would be to construct prototype compositions.

By the way, in first sight it seems like it is not Ca that dominates the variation in the (transformed) data, but Br. What does Br do?
Wikipedia => the high solubility of the bromide ion (Br−) has caused its accumulation in the oceans (Yeah that makes sense, but probably does not tell us much about
specific clima-relevant processes?).

##### Score plots

We can use score plots to further investigate clusters of time dependend observations on a two dimensional plane with two components (and that is if possible two environmental processes).

TODO


#### Calculation of $\boldsymbol{x}_{\mathrm{i}}^{\prime}$

We can use principal component to reconstruct the data with all or some of the components. One idea would be calculate an "average" composition and show the influence of every component on the compositional parts (over time).

```{r loadings pcaCoda, eval=TRUE}
(pca_loading <- pca_rs$loadings[, 1:2])

# calculate squared sum of loadings to check if the sum is 1
apply(pca_loading^2, 2, sum)
```

Since  $\boldsymbol{x}_{\mathrm{i}}^{\prime}$ is a linear combination of the observations and their loadings, we can use them to reconstruct the original data (plus error).

Remember that we need the transformed variables in the Simplex. To get the variables in Euclidean space (i.e. the original count space we need to retransform the result).


```{r calc synthetic observations, eval=TRUE}
# first observation projected on PC1
# remember that we need the transformed variables in the Simplex. To get the variables in Euclidean space (i.e. the original count space we need to retransform the result)
obs1 <- acomp[2, ]
obs1_matrix <- matrix(as.numeric(obs1), nrow = 1)
obs1_matrix %*% pca_loading[, 1]
```
This value is the projection of our 13-dimensional transformed composition (i.e the first observation) of it onto the first component. It is also the score value for the first observation and PC1, which is nothing else 
than the linear transformation of the first observation:
$$
z_{i 1}=\phi_{11} \mathbf{x}_{i 1}+\phi_{21} \mathbf{x}_{i 2}, \ldots, \phi_{d 1} \mathbf{x}_{i d} i=1,2, \ldots, n
$$

Those linear transformations can be very helpful to analyse compositional variations over time (f.e. which component is more dominant over time).

Unfortunately, it is not possible to clearly interpret the clr-coordinates (real or synthetic). If we deal with synthetic data, we need to transform them back to euclidean space (which is not possible for ilr? -> check!)


```{r head scores, eval=TRUE}
# head(pca_rs$scores)
Z <- matrix(as.numeric(acomp), nrow = 2119) %*% pca_rs$loadings
```

The scores in the pca object are not correct!

##### Time series of scores

```{r time analysis scores, eval=TRUE}
data <- tar_read(data_kl15)
agem <- data$age
Z <- cbind(Z, agem)

ggplot(Z, aes(x = agem, y = Z[, 1])) +
    geom_line() +
    geom_smooth(method = "loess") +
    labs(x = "Time BP", y = "Scores PC1", title = "Scores for PC1 over time") +
    theme_minimal()
```

So that shows us some clear trends. If we would have a clear interpretation of the component (f.e. proxy for aridity), we can show a transformed picture that reduces the relative information to just this process and eliminates the "noise" of other processes.
Additionally we can show the impact of the component onto an "average" composition to construct prototypical compositions of different processes.

Further analysis should visualize a familiy of discrete distributions of elements with the "average" distribution being pertubated by the principal component of choice.

### EMMA

The idea is quite similar to PCA with a decomposition of the data into loadings and scores:

 $$
\mathbf{X}_{n \times q}=\mathbf{M}_{n \times q} \mathbf{V}_{q \times q}^T
$$

Trough the separation of principal components and error we can separate signal and noise within the data to identify natural processes:

"In EMMA, in addition to implementing dimensionality reduction through PCA, factor analysis is applied to simplify the structure of the eigenvector matrix, V, by an orthogonal rotation of the axes in the feature space. This rotation removes the order of the eigenvectors and redistributes the loadings more evenly − a condition often used to decipher natural processes (Dietze el al. 2012)."

EMMA makes use of an iterative algorithm to find the best rotation of the axes in the feature space. The application of EMMA within the compositional framework offers some interesting perspectives:

1. Can the rescaling steps 1 and 2 of the EMMA algorith be replaced by a transformation of the data to a simplex?
2. Calculate the Eigenvectors and Eigenvalues of the covariance matrix of the data.
3. Apply factor rotation 
4. Normalize the loadings
5. Apply inverse transformation to the eigenvectors and scores
6. Calculate the model error: $E = X^{\prime} - X$ with $X^{\prime} = \mathbf{M}_{n \times r} \mathbf{V}_{r \times r}^T$
7. iterate until optimal $r$ is reached (there shouldn't be a optimal transformation, but we should check that)

But since one model is just a random selection from the sample space, we need a large set of models to get reliable results with EMMA (which we don't have? =>
**We can treat each time dependend observation as a sample** -> that's interesting and probly allows for time dependend components).

#### SOGA example

with synthetic data resulting from 4 natural processes (r=4) that are randomly resampled. 

We have 45 observations of 20 grain size classes in our sample (from a population of 99 classes). Each class value indicates the volumne and the volumne of all classes adds
up to a constant. 

Since it is simulated data we know that each natural process has a specific profile (thats what I called prototype earlier).

By the way, since we have a continous space for grain sizes, what we are observing are densities and the data can be used to replicate the study from Prof. Greven.

For our data, we have the disadvantage that elements are not ordered. Therefore the identification of profiles is more complicated. But the main idea remains the same.
The idea we are simulating is that each natural process has an ideal profile (here it is the four different densities - with discrete data it would be specific compositions). 
The data consisting of our 2117 observations is a mixture of the natural processes (and the error component -> which is only contained in the removed components => realistic?). 
If we use this concept, we need to introduce a time component since the processes we are interested in are dynamically changing over time.

```{r emma example, eval=FALSE}
load(url("http://www.userpage.fu-berlin.de/~soga/300/30100_data_sets/data_emma.RData"))
str(df.soil.samples[1:20, 1:20])

# first sample (observation)
phi <- colnames(df.soil.samples)
plot(phi, df.soil.samples[1, ], type = "l", ylab = "")

# Step 1
X <- as.matrix(df.soil.samples) # convert data.frame object to matrix object
c <- 1 # set constant sum, can be 1, or 100 (%), or any other meaningful constant
X <- X / apply(X, 1, sum) * c

# Step 2
lw <- 0.05 # set percentile
qts <- function(X, lw) quantile(X, c(lw, 1 - lw), type = 5) # define function
ls <- t(apply(X, 2, qts, lw = lw)) # apply function column-wise
W <- t((t(X) - ls[, 1]) / (ls[, 2] - ls[, 1])) # calculate weight transformation

# Step 3
## Eigen space extraction
A <- t(W) %*% W # calculate minor product
EIG <- eigen(A) # calculate eigenvectors and eigenvalues

V <- EIG$vectors[, order(seq(ncol(A), 1, -1))]
Vf <- V[, order(seq(ncol(A), 1, -1))] # eigenvector matrix

L <- EIG$values[order(seq(ncol(A), 1, -1))]
Lv <- cumsum(sort(L / sum(L), decreasing = TRUE)) # normalized eigenvalues

# Step 4
r <- 4
# since factor analysis is deterministic regarding the number of components, we either need a very good argument or an iterative process to find optimal r
Vr <- do.call(varimax, list(Vf[, 1:r])) 

# Step 5
# apply varimax function
library(limSolve)
# calculate the end-member loadings
Vq <- Vr$loadings[, order(seq(r, 1, -1))] # extract and sort factor loadings
Vqr <- t(t(Vq) / apply(Vq, 2, sum)) * c # rescale
Vqr <- t(Vqr)
Vqn <- t((Vqr - apply(Vqr, 1, min)) / # normalize factor loadings column-wise
  (apply(Vqr, 1, max) - apply(Vqr, 1, min)))

# calculate the end-member scores matrix by non-negative least square fitting
Mq <- matrix(nrow = nrow(X), ncol = r)
for (i in 1:nrow(X)) {
  Mq[i, ] <- nnls(Vqn, as.vector(t(W[i, ])))$X
}

# Step 6: Rescale
s <- (c - sum(ls[, 1])) / apply(Vqn * unname(ls[, 2] - ls[, 1]), 2, sum)
Vqs <- Vqn
for (i in 1:r) {
  Vqs[, i] <- t(s[i] * t(Vqn[, i]) * (ls[, 2] - ls[, 1]) + ls[, 1])
}
Vqsn <- t(t(Vqs) / apply(Vqs, 2, sum)) * c # rescale loading matrix

# Visualisation of the synthetic prototypes:
library(RColorBrewer)

par(mfrow = c(1, 1))
cols <- brewer.pal(r, "Set2")

## Modeled data ##
plot(phi, Vqsn[, 1],
  type = "l",
  ylab = "Volume proportion",
  main = "Modeled loadings",
  col = cols[1], ylim = c(0, 0.10), lwd = 2
)
lines(phi, Vqsn[, 2], col = cols[2], lwd = 2)
lines(phi, Vqsn[, 3], col = cols[3], lwd = 2)
lines(phi, Vqsn[, 4], col = cols[4], lwd = 2)

## natural end members ##
lines(phi, EMa[[1]], col = "grey", lwd = 2)
lines(phi, EMa[[2]], col = "grey", lwd = 2)
lines(phi, EMa[[3]], col = "grey", lwd = 2)
lines(phi, EMa[[4]], col = "grey", lwd = 2)

legend("topright",
  legend = c("EM 1", "EM 2", "EM 3", "EM 4", "natural end members"),
  lwd = 2,
  col = c(cols, "grey"),
  cex = 0.85
)

```

The result underestimates the natural processes (which can be expected with a linear fit), but clearly identifies them.
The simulated data has a lot of advantages:
- known number of processes
- no noise
- equally distributed processes
- stable processes over all observations

We should be able to reproduce that with an iterative process to identify the optimal number of processes.
**Additionally, we need an idea about the seperation of signal and noise and their relations to the elements in the data!**
Also there should be an argument to get rid of the rescaling steps when using the compositional framework. We will compare both ways.

#### example with EMMAgeo

### Time Series contributions

### Predictive Modelling (PCA and Regression: Scalar on Composition)


### Theorie CoDa

A clr coordinate contains the relative information of one part related to all the others. I am not a geologist, but if that is what we are looking for, might depend on 
the process of sedimentation. And to be honest, I have no idea about that one.

## DA Functions

```{r mathplot graph, eval=FALSE, include=FALSE}
graph_ts_200ka <- function(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1) {
    # with targets we can use read_target to load the data
    d <- get(datasets[index])

    #*******************************200ka Plot of wentness index*************************************
    # Create the plot
    if (!display) {
      png(filename = paste0("graphs/200ka_polygonplot", datasets[index],".png"))
    }

    yt <- d[,2] # your time series y values
    xt <- d[,1] # your time series age values

    # Assuming d is a matrix with two columns
    mean_yt <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # Define ylim based on 3-sigma range
    # y_min <- mean_yt - 5 * std_dev
    # y_max <- mean_yt + 5 * std_dev
    
    #***************************Plotting the graph********************************
    plot(xt, yt, type = "n", xlab = "", ylab = "", xlim = range(xt), ylim = range(yt), main = paste0("200ka Plot of ", datasets[index]))

    # Create the filled areas
    # age matrix
    fill_x <- c(xt, rev(xt))
    # Terrigenous matrix
    fill_y <- c(yt, rep(mean_yt, length(yt)))
    fill_y[yt < mean_yt] <- mean_yt
    fill_y[1] <- mean_yt
    fill_y[length(fill_y)] <- mean_yt
    color_rgb <- col2rgb('#D95319') / 255
    polygon(fill_x, fill_y, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    fill_y2 <- c(yt, rep(mean_yt, length(yt)))
    fill_y2[yt > mean_yt] <- mean_yt
    fill_y2[1] <- mean_yt
    fill_y2[length(fill_y2)] <- mean_yt
    color_rgb <- col2rgb('#0072BD') / 255
    polygon(fill_x, fill_y2, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    if (!display) {
        # Close the graphics device
        dev.off()
        return()
    }
    
    # #*******************alternative ggplot graph********************************
    # library(ggplot2)

    # # Assuming d is a data frame with two columns
    # mean_val <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # # Define ylim based on 3-sigma range
    # y_min <- mean_val - 5 * std_dev
    # y_max <- mean_val + 5 * std_dev

    # # Create a data frame for the filled areas
    # fill_pos_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # negative values are replaced with the mean value
    # fill_pos_data$y[fill_data$y < mean_val] <- mean_val

    # # Create a data frame for the filled areas
    # fill_neg_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # positive values are replaced with the mean value
    # fill_neg_data$y[fill_data$y >= mean_val] <- mean_val


    # # Create the plot
    # p <- ggplot(d, aes(x = yt, y = xt)) +
    # geom_line() +
    # geom_polygon(data = fill_pos_data, aes(x = x, y = y), 
    #             fill = rgb(217/255, 83/255, 25/255, alpha = 0.9), 
    #             color = NA) +
    # geom_polygon(data = fill_neg_data, aes(x = x, y = y), 
    #             fill = rgb(173/255, 216/255, 230/255, alpha = 0.9), 
    #             color = NA) +
    # ylim(y_min, y_max) +
    # theme_minimal() +
    # labs(x = "Time (kyrs BP)", y = "", title = paste0("200ka Plot of Wentness Index of ", datasets[1]))

    # ggsave(filename = paste0("200ka_plot_",datasets[index],".png"), plot = p, path = "graphs/", width = 10, height = 6, dpi = 300)
}

```


  ```{r graph_ts, eval=FALSE}
  # this function is an artifact of the translation of mathplot functions and requires the read script to run beforehand
  graph_ts_200ka(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1)
  ```

  ToDo: It would be more  informative to have point data instead of polygons.


### spearman cross correlation analysis

The first step is to reproduce the cross correlation analysis from the paper (especially figure 3).





# Convert Matlab files and reproduce Paper graphs


## Read the files

The following reads data objects from matlab files.

```{r matlab conversion, eval=FALSE}
# Install the R.matlab package if it's not already installed
if (!require(R.matlab)) {
  install.packages("R.matlab")
}

# Load the R.matlab package
library(R.matlab)

# Use the readMat() function to read a .mat file
data <- readMat("data/Africa_NE_200/data_pentagramm_5_1200_025kyr_XCorr.mat")
```

# targets

Visualize the status of the targets pipeline:

```{r target vis, eval=TRUE}
tar_visnetwork()
```