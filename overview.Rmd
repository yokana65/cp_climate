---
title: "Compositional Data Analysis"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

# Introduction

This is where you can write an introduction to your document.

Install the required packages:

```{r eval=TRUE, include=FALSE}
if (!require(rmarkdown)) {
  install.packages("rmarkdown")
}

if (!require(targets)) {
  install.packages("targets")
}

if (!require(visNetwork)) {
  install.packages("visNetwork")
}

if (!require(ggplot2)) {
  install.packages("ggplot2")
}
if (!require(robCompositions)) {
  install.packages("robCompositions")
}

# call libraries
# TODO: make this part more lenient
library(ggplot2)
```


## Arguments

The paper uses two pairs as proxy for aridity: Ti/Al and Ti/Ca. The first pair is used for the last 200kyr and the second pair for the last 1200kyr. The data is from the KL15 core.
A proposed alternative is the use of K as a proxy for aridity.



# Data overview

### Questions:

- [] In the KL15_xrf data, what is the difference between the Area estimates and the DArea? (it seems Area is used trough the analysis)

## ODP

This is the data from the Ocean Drilling Program close to Cyprus. Data contains several Cores, an age model and XRF data. XRF data is a proxy for the chemical composition of the sediment. 

The analysis is done with Ti.AL as a proxy for wetness (?). 
  

## CHB 14

## KL 15

In the XRF data we have the following variables:

```{r, eval=TRUE}
data <- tar_read(data_kl15)
colnames(data)
```


TODO: adjust data object
```{r, eval=TRUE}
data_select <- data[, c("depth", "age", "Br_Area", "Rb_Area", "Sr_Area", "Zr_Area", "Ru_Area", "Mg_Area",
 "Al_Area", "Si_Area", "S_Area", "K_Area", "Ca_Area", "Ti_Area", "Fe_Area")]
variables <- c("Br_Area", "Rb_Area", "Sr_Area", "Zr_Area", "Ru_Area", "Mg_Area",
 "Al_Area", "Si_Area", "S_Area", "K_Area", "Ca_Area", "Ti_Area", "Fe_Area")
# for compositional data, we are interested in the sum of all components
data_select$aggregate <- rowSums(data_select[3:ncol(data_select)])
summary(data_select)
```


## Exploratory DA

Lets do some boxplots for our variables:

```{r, eval=TRUE}
par(mfrow = c(1,2))
boxplot(data[, variables], main="Boxplots of the Element counts", names = variables, las = 2)
boxplot(data[, variables[variables != "Ca_Area"]], main="Boxplots of the Element counts", names = variables[variables != "Ca_Area"], las = 2)
par(mfrow = c(1,1))


```

We do have dominant role of Ca. Mg and Rb don't show relevant variation (at first glance) and can probably be dropped in later steps of analysis (better argumentation is needed though).
Variance of Sr, Si and Fe is also more dominant than the other elements (is this reflected in the Simplex as well?). 

We want to analyse the Area minerals together with a time component, which gives us a Compositional Time Series (CTS).

The compositional data does not sum up to a constant value. In fact the spread of its sum is rather big.
We want to know how the sum of the compositional parts is behaving over time:

```{r, eval=TRUE}
ggplot(data_select, aes(x = age, y = aggregate)) +
  geom_line() +
  geom_smooth(method = "loess") +
  labs(x = "Compositional sum over time", y = "Sum", title = "Trend over Time") +
  theme_minimal()
```

It looks like its getting more compact with time until there is a turning point (which might make sense from a geological perspective?).
Anyway, the non constant makes sense since we probably don't measure all components of the sediment.

Due to the high variation of the sum, we could apply the concept of sparsely sampled data (Vgl. Greven et Steyer, 2023). The idea is that
we assume that we only pull a sample from the whole composition. 


Just for fun and orientation, let's have a look at the distribution of elements at a certain time point:
```{r, eval=TRUE}
row <- data_select[1, 3:(ncol(data_select)-1)]
# Convert the row to a data frame and reshape it to a long format
df <- as.data.frame(t(row))
df$Element <- rownames(df)
colnames(df)[1] <- "Value"
df$Value <- as.numeric(df$Value)

# Create the bar plot
ggplot(df, aes(x = Element, y = Value)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Element", y = "Value", title = "Values of Elements for 1 row") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

A first look of the data in KL_15_XRF_tuned:

TODO: remove all data calls with target loads

```{r, eval=TRUE}
# we want to read them in R
data_KL15 <- read.table("data/Africa_NE_200/data/data_KL15_XRF_tuned.txt", header = TRUE, sep = "\t")

# summarise all attributes of our data
summary(data_KL15)

```


For the **correlation** analysis the following datasets are used:

- script_read_data_KL15_XRF

```{r, eval=TRUE}
# we want to read them in R
data_KL15 <- read.table("data/Africa_NE_200/data/data_KL15_XRF.txt", header = TRUE, sep = "\t")

# summarise all attributes of our data
summary(data_KL15)

```



```{r, eval=TRUE}
data_kl15_agem <- read.table("data/Africa_NE_200/data/data_KL15-2_smooth_spline_ages.txt", header = TRUE, sep = "\t")

summary(data_kl15_agem)

```

This appears to be confidence intervals for the age data. "accrate" is the accumulation rate, which indicates the speed of sedimentation.

We also need the data with "quality flags"

```{r, eval=TRUE}
data_kl15_qf <- read.table("data/Africa_NE_200/data/data_KL15_qf.txt", header = TRUE, sep = "\t")

summary(data_kl15_qf)

```




- script_read_data_KL15_orbitalforcing
- script_read_data_KL15_odp967
- script_read_data_KL15_odp722
- script_read_data_KL15_odp721_722
- script_readco2_age
- script_read_data_KL15_CHB_K_RRM_MHT500



# Data Analysis

read the data:
TODO: write pipeline to avoid loading the data all the time

```{r, eval=TRUE, include=FALSE}
# run the read_data_200kyr_all.R script
source("scripts/read_data_200kyr_all.r")
```

## Compositional Data Analysis

As an entree point, we have to two R packages at hand for compositional data analysis: `compositions` and `robCompositions`. 
The first one is more general (and more popular)[https://www.rdocumentation.org/packages/compositions/versions/2.0-8] and the 
second one is more focused on robust statistics, including ("robust" PCA)[https://www.rdocumentation.org/packages/robCompositions/versions/2.4.1].

In the following we take a first look at centered log ratios (clr), which are the most recommended technique. Anyway, bafter some deliberation it seems that 
isometric log ratios (ilr) are more appropriate for our data and methods, that is for data centered around one dimension (Ca) ? and for PCA.

### Exploratory Analysis

### Centered log ratios

Without further ado, since this is the most robust transformation for compositional data.

```{r, eval=TRUE}
data_comp <- tar_read(data_kl15_comp)

# What happens with the relative information if we constrain each row to sum up to 1?
data_comp_1 <- constSum(data_comp, const = 1) 
# all(rowSums(data_comp_1) == 1)
# data_comp_1$aggregate <- rowSums(data_comp_1)
ratio_ti_al <- data_comp$Ti_Area / data_comp$Al_Area
head(ratio_ti_al)
ratio_ti_al_1 <- data_comp_1$Ti_Area / data_comp_1$Al_Area
head(ratio_ti_al_1)
# Bang!
```

That looks nice, but is the relative information (i.e. the ratio of one mineral to the others) more important than the absolute value (i.e. the amount of CA that is in the sediment)?

By the way, the transformation above proves that relative information is scale invariant while absolute information is not.

We can calculate the Aitchison distance between all elements for each point of time:
```{r, eval=TRUE}
aDist(data_comp[1:7, ])
```

Now we can calculate the clr transformation for the data:
```{r, eval=TRUE}
clr_coord <- cenLR(data_comp)$x.clr
ggplot(clr_coord, aes(x = Ti_Area, y = Al_Area)) +
  geom_point(size = 2) +
  xlab("Ti Area (clr)") +
  ylab("Al Area (clr)")
```

**Interpretation:** 
In very unprecise terms: The log of the ratio of equal components is zero, i.e. they are of equal importance.
The clr transformation takes the natural log of each value in a row and divides it by the square root of the products of the remaining parts.
In very unqualified language, that is the relevant information of one part in relation to all the others. If the
clr is positive, the part is more important than the average, if it is negative, it is less important.

So here Al and Ti clearly have a linear trend, i.e. if one is less important then so is the other. Also the spread is higher for more
more negative values, which makes sense since their importance goes into oblivion.

### Dendrogram

```{r, eval=TRUE}
# TODO: the dataset needs to be transformed to a pivot table?
res <- hclust(dist(data_comp_1), method = "single")
plot(res)
```

Thats not what we want since we calculate the distances between observations. But it is actually really interesting to see. What is wrong with **observation 17!**
And additionaly clusters of observations (that is times) are relevant as well. 

But now, how do we get the dendogram for the distances between the elements?

```{r, eval=TRUE}
res <- hclust(dist(t(data_comp_1)), method = "single")
plot(res)
```

So at first sight, that is disappointing. Even regarding relative information Ca clearly dominates the picture. 



Also we should try other clustering methods then single linkage.

```{r, eval=TRUE}
res <- hclust(dist(t(data_comp_1)), method = "ward.D2")
plot(res)
```

Does not change much.

There are ways out of that dilemma. We could use isometric transformations with Ca as pivot!

### PCA

Steps of analysis are build on: https://www.geo.fu-berlin.de/en/v/soga-r/Advances-statistics/Multivariate-approaches/Principal-Component-Analysis/PCA-the-basics/Data-preparation/index.html

Something to have in mind is that PCA is scale sensitive, while CoDa is scale invariant. That is an additional reason for the transformation in clr, alr or ilr. It also illustrates the fact that
any steps are unneccassary in CoDaPCA. 

With robCompositions: 

**Aim**: 
- to reduce our 14-D space without loosing too much of the relative information, that is the compositional aspects of the data.
- describe the relation between the observations (in time) and the compositional parts.


In the orthonormal space we calculated independent ilr coordinates (why not clr?).


#### methods

There is a bunch of methods to calculate PCA.

- SVD: Single Vector Decomposition
- Eigenvector decomposition

With `robCompositions` we can use the `pcaCoDa` function, which uses eigen-decomposition of the covariates. 
The composition is represented in orthonormal coordinates, which are the ilr coordinates, and then standard PCA is applied.


We can calculate the eigenvalues and eigenvectors for CoDa:

```{r, eval=TRUE}
data_co <- tar_read(data_kl15_comp)
pca_cov <- cov(data_co)
pca_eigen <- eigen(pca_cov)
rownames(pca_eigen$vectors) <- colnames(data_co)
pca_eigen$values
pca_eigen$vectors
```

#### Ternary Diagram

```{r, eval=TRUE}

data_3_comp <- data[, c("Ti_Area", "Al_Area", "Ca_Area")]
ternaryDiag(data_3_comp, line = "pca")
```

Yeah, we can see the problem.

```{r, eval=TRUE}
data <- tar_read(data_kl15_comp)
data_3_comp <- data[, c("Ti_Area", "Al_Area", "Fe_Area")]
ternaryDiag(data_3_comp, line = "pca")
```

It seems that a lot of the variation in our data is concentrated on specific components. (Which basically means that the are 
quite independent and we don't find much relative information at all?)


#### PCA calculation

```{r, eval=TRUE}
## compositional PCA, non-robust
p_comp <- pcaCoDa(data, method = "classical")
## compositional PCA, robust
set.seed(234) # to reproduce the result exactly
p_comp_rob <- pcaCoDa(data, method = "robust")
summary(p_comp)
summary(p_comp_rob)
plot(p_comp_rob, type = "l")
```

First of all, that looks better than expected.

Let's plot the first two components:

```{r, eval=TRUE}
par(mfrow=c(1,2), mar = c(4,4,2,2))
biplot(p_comp, xlabs = rownames(data))
biplot(p_comp_rob, xlabs = rownames(data))
```

That also looks like we can work with. While the first component seems to be associated with Ca (no surprise) and Sr, the second is a combination of Zr, Ti and Fe and maybe most interestingly Br seems to take
a special place and is probably dominant in the third component.

TODO: **Calculate scores for the components and plot them over time.**
TODO: Validate result and compare it with euclidean distances -> explain the difference!


### Theorie

A clr coordinate contains the relative information of one part related to all the others. I am not a geologist, but if that is what we are looking for, might depend on 
the process of sedimentation. And to be honest, I have no idea about that one.

## DA Functions

```{r, eval=TRUE, include=FALSE}
graph_ts_200ka <- function(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1) {
    # with targets we can use read_target to load the data
    d <- get(datasets[index])

    #*******************************200ka Plot of wentness index*************************************
    # Create the plot
    if (!display) {
      png(filename = paste0("graphs/200ka_polygonplot", datasets[index],".png"))
    }

    yt <- d[,2] # your time series y values
    xt <- d[,1] # your time series age values

    # Assuming d is a matrix with two columns
    mean_yt <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # Define ylim based on 3-sigma range
    # y_min <- mean_yt - 5 * std_dev
    # y_max <- mean_yt + 5 * std_dev
    
    #***************************Plotting the graph********************************
    plot(xt, yt, type = "n", xlab = "", ylab = "", xlim = range(xt), ylim = range(yt), main = paste0("200ka Plot of ", datasets[index]))

    # Create the filled areas
    # age matrix
    fill_x <- c(xt, rev(xt))
    # Terrigenous matrix
    fill_y <- c(yt, rep(mean_yt, length(yt)))
    fill_y[yt < mean_yt] <- mean_yt
    fill_y[1] <- mean_yt
    fill_y[length(fill_y)] <- mean_yt
    color_rgb <- col2rgb('#D95319') / 255
    polygon(fill_x, fill_y, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    fill_y2 <- c(yt, rep(mean_yt, length(yt)))
    fill_y2[yt > mean_yt] <- mean_yt
    fill_y2[1] <- mean_yt
    fill_y2[length(fill_y2)] <- mean_yt
    color_rgb <- col2rgb('#0072BD') / 255
    polygon(fill_x, fill_y2, col = rgb(color_rgb[1], color_rgb[2], color_rgb[3], alpha = 0.1), border = NA)

    if (!display) {
        # Close the graphics device
        dev.off()
        return()
    }
    
    # #*******************alternative ggplot graph********************************
    # library(ggplot2)

    # # Assuming d is a data frame with two columns
    # mean_val <- mean(d[!is.na(d[,2]), 2], na.rm = TRUE)
    # std_dev <- sd(d[!is.na(d[,2]), 2], na.rm = TRUE)

    # # Define ylim based on 3-sigma range
    # y_min <- mean_val - 5 * std_dev
    # y_max <- mean_val + 5 * std_dev

    # # Create a data frame for the filled areas
    # fill_pos_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # negative values are replaced with the mean value
    # fill_pos_data$y[fill_data$y < mean_val] <- mean_val

    # # Create a data frame for the filled areas
    # fill_neg_data <- data.frame(x = c(d[,1], rev(d[,1])), 
    #                         y = c(d[,2], rep(mean_val, nrow(d))))
    # # positive values are replaced with the mean value
    # fill_neg_data$y[fill_data$y >= mean_val] <- mean_val


    # # Create the plot
    # p <- ggplot(d, aes(x = yt, y = xt)) +
    # geom_line() +
    # geom_polygon(data = fill_pos_data, aes(x = x, y = y), 
    #             fill = rgb(217/255, 83/255, 25/255, alpha = 0.9), 
    #             color = NA) +
    # geom_polygon(data = fill_neg_data, aes(x = x, y = y), 
    #             fill = rgb(173/255, 216/255, 230/255, alpha = 0.9), 
    #             color = NA) +
    # ylim(y_min, y_max) +
    # theme_minimal() +
    # labs(x = "Time (kyrs BP)", y = "", title = paste0("200ka Plot of Wentness Index of ", datasets[1]))

    # ggsave(filename = paste0("200ka_plot_",datasets[index],".png"), plot = p, path = "graphs/", width = 10, height = 6, dpi = 300)
}

```


  ```{r, eval=TRUE}
  graph_ts_200ka(display = TRUE, datasets = c("data_odp_967_22", "data_odp721_722_terr", "data_odp_709", "data_icdp_chb", "data_kl09", "data_kl11", "data_kl15", "data_lake_tana"), index = 1)
  ```

  ToDo: It would be more  informative to have point data instead of polygons.


### spearman cross correlation analysis

The first step is to reproduce the cross correlation analysis from the paper (especially figure 3).





# Convert Matlab files and reproduce Paper graphs


## Read the files

The following reads data objects from matlab files.

```{r, eval=FALSE}
# Install the R.matlab package if it's not already installed
if (!require(R.matlab)) {
  install.packages("R.matlab")
}

# Load the R.matlab package
library(R.matlab)

# Use the readMat() function to read a .mat file
data <- readMat("data/Africa_NE_200/data_pentagramm_5_1200_025kyr_XCorr.mat")
```

# targets

Visualize the status of the targets pipeline:

```{r, eval=TRUE}
tar_visnetwork()
```