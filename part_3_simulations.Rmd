---
title: "Simulations"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

# Simulation Study

```{r libraries, eval=TRUE , echo=FALSE, include=FALSE}
library(targets)
library(compositions)
library(mvtnorm)
library(ggplot2)
library(future.apply)
library(robCompositions)
library(gridExtra)
library(ggnewscale)

source("scripts/helper_functions.R")
source("scripts/fit_pca_ilr.R")
source("scripts/fit_pca_ilr_2.R")
source("scripts/conditional_scores_function.R")
source("scripts/gradient.R")
```

This chapter evaluates the performance of the MCEM algorithm introduced in the chapter [...]. The evaluation follows a two-step simulation procedure. First, the latent 
compositions $\boldsymbol{\pi_i}$ are simulated for 100 observation. The compositions are the backtransformations of the principal component 
decomposition of the clr-coordinates $\boldsymbol{\rho_i}  \stackrel{i.i.d.}{\sim} \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{K})$. Secondly, realistic count observations are drawn 
from the associated multinomial distribution $\boldsymbol{X}_{i} {\sim} \text{Multinom}(m, \boldsymbol{\pi}_{i})$ where $m$ denotes a fixed sample size for all observations 
$i=1,\ldots,100$.  

The simulated dataset of count compositions is then used to evaluate the performance of the MCEM algorithm. That is done by evaluating the MCEM algorithm's capability to identify the 
true mean vector $\boldsymbol{\mu}$ and covariance structure $\boldsymbol{K}$ of the latent compositions. For this purpose, the empirical mean vector is compared to the sample mean and the empirical 
covariance structure derived from the principal component analysis is compared with the result of both a standardal and a robust PCA.  
It is important to note that the MCEM algorithm operates directly on the simplex, while both standardal and robust PCA are performed on the transformed coordinates 
of the observations.  
The standardal PCA is conducted with the `prcomp` base function in R using a singular value decomposition of the clr-transformed coordinates. This is equivalent to an 
eigendecomposition on the ilr-coordinates (Filzmoser et al. 2019, p.135). The robust principal component analysis is based on the minimum covariance determinant (MCD) 
estimator (Rousseeuw 1985; Rousseeuw and Van Driessen 1999) and calculated with the function `princomp` from the `robCompositions` package. The MCD estimator uses the 
subset of observations that has the smallest determinant for its covariance matrix among all subsets of the same size. Since the determinant measures the overall spread 
of the data, the MCD estimator identifies the most compact subset of observations by minimizing the spread and consequently provides a valid method to control against 
outliers (Filzmoser et al. 2019, p. 92).  

Since the sample for the latent compositions $\boldsymbol{\pi_i}$ are obtained as back transformations from the simulated clr-coordinates, the compositions can be 
simulated by the principal component decomposition: $\boldsymbol{\pi}_{i}=\operatorname{clr}^{-1}\left({\sum_{k=1}^N z_{i k} \boldsymbol{\varphi}_{k}}\right)$ with N being the 
number of principal components.  

For a basic comparison a setting with five compositional parts and only two principal components and no no interaction effects (TODO: define?) is considered. Therefore the set of latent compositions 
can be specified as $\boldsymbol{\pi}_i =  \operatorname{clr}^{-1}\left(\boldsymbol{\mu}+z_{i 1} \boldsymbol{\varphi}_{1}+z_{i 2} \boldsymbol{\varphi}_{2}\right)$ where 
$z_{i 1} \stackrel{i . i . d .}{\sim} Z_{1}$ and $z_{i 2} \stackrel{i . i . d .}{\sim} Z_{2}$. The scores associated with the two principal components are normally distributed 
as $Z_{1} \stackrel{i . i . d .}{\sim} N(0,0.6)$ and $Z_{2} \stackrel{i . i . d .}{\sim} N(0,0.3)$.  
The mean vector is set to $\boldsymbol{\mu} = (0, 1, 0.5, -1, -0.5)$, which simulates a relative strong variation in dominance between the compositional parts. The principal components 
consist of two contrasts, the first between the second and third compositional part $\boldsymbol{\varphi}_{1} = \left(0, \sqrt{\frac{1}{2}}, -\sqrt{\frac{1}{2}}, 0, 0\right)$ amd the second between the first and the 
fourth compositional part $\boldsymbol{\varphi}_{2} = \left(\sqrt{\frac{1}{2}}, 0, 0, -\sqrt{\frac{1}{2}}, 0\right)$.  
To further evaluate the algorithm's performance, the sample size $m_i$ is changed in three steps. Starting with a very small sample size of 20, the size is doubled in every step. 
This approach allows to assess the effectiveness of the MCEM algorithm for count compositions with small sample sizes. The conducted simulation study contains 50 random runs for every 
sample size setting.
The distances between the oracle estimates for mean and covariance and each method can be computed with the L2-norm and the Frobenius norm.   

```{r latent parameters front, eval=TRUE}
set_1 <- c("#e0ecf4", "#9ebcda", "#8856a7")

D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues <- c(0.6, 0.3)
mean_2 <- c(0, 1, 0.5, -1, -0.5) # those are clr-coordinates

eigenvalues_2 <- c(0.6, 0.3, 0, 0)
V_2 <- cbind(c(0, sqrt(1/2), -sqrt(1/2), 0, 0), c(-sqrt(1/2), 0, 0, sqrt(1/2), 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_2 <- V_2 %*% diag(eigenvalues_2) %*% t(V_2) 

true_v_2 <- mean_2
```



```{r evaluation mean vector front, eval=TRUE}
summary_stats <- function(x_data) {
  data.frame(
    mean = colMeans(x_data),
    sd = apply(x_data, 2, sd),
    min = apply(x_data, 2, min),
    max = apply(x_data, 2, max),
    n_zeros = apply(x_data, 2, function(x) sum(x == 0))
  )
}

simulation_data_list_2 <- list(
  tar_read(sim_comp_2_smi_nSim_20),
  tar_read(sim_comp_2_smi_nSim_40),
  tar_read(sim_comp_2_smi_nSim_80),
  tar_read(sim_comp_2_smi_nSim_160)
)

n_simulations <- length(simulation_data_list_2[[1]])

pca_clr_rob_2 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_2 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_2[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_2[[j]][[i]] <- prcomp(clr_transform(x_data, replace_zeros = "neutral"))
    pca_clr_rob_2[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_2 <- list(
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_20),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_40),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_80),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_160)
)

diff_mean_2 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_2 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_2 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_2[[j]][[i]] <- sqrt(sum((true_v_2 - t(basis_matrix) %*% simulation_results_list_2[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_2[[j]][[i]] <- sqrt(sum((true_v_2 - pca_clr_std_2[[j]][[i]]$center)^2))
    diff_mean_rob_clr_2[[j]][[i]] <- sqrt(sum((true_v_2 - pca_clr_rob_2[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_2
diff_mean_std_clr = diff_mean_std_clr_2
diff_mean_rob_clr = diff_mean_rob_clr_2

scenario_stats <- list()

for(j in 1:4) {
  simulation_data <- simulation_data_list_2[[j]]
  # Sammle Statistiken aller Simulationen für Szenario j
  all_stats <- lapply(simulation_data, function(sim) {
    x_data <- do.call(rbind, sim$x_data)
    summary_stats(x_data)
  })
  
  # Berechne Durchschnitt über alle Simulationen
  avg_stats <- Reduce('+', all_stats) / length(all_stats)
  
  # Speichere Ergebnis
  scenario_stats[[j]] <- avg_stats
  
  # Ausgabe
  print(paste("Average Statistics for Scenario", j))
  print(avg_stats)
}
```

```{r boxplot simulation front, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_2 <- data.frame(
  differences = c(
    unlist(diff_mean_2[[1]]), unlist(diff_mean_2[[2]]), 
    unlist(diff_mean_2[[3]]), unlist(diff_mean_2[[4]]),
    unlist(diff_mean_std_clr_2[[1]]), unlist(diff_mean_std_clr_2[[2]]),
    unlist(diff_mean_std_clr_2[[3]]), unlist(diff_mean_std_clr_2[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_2$method <- factor(diff_df_2$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_2$size <- factor(diff_df_2$size, 
                        levels = c("20", "40", "80", "160"))

```


```{r evaluation covariance front, eval=TRUE, fig.width=12, fig.height=5}
sigma_distances_2 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_2 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_2 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_2[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_2[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_2[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_rob, type = "F")
  }
}

sigma_diff_df_2 <- data.frame(
  differences = c(
    unlist(sigma_distances_2[[1]]), unlist(sigma_distances_2[[2]]), 
    unlist(sigma_distances_2[[3]]), unlist(sigma_distances_2[[4]]),
    unlist(sigma_distances_std_clr_2[[1]]), unlist(sigma_distances_std_clr_2[[2]]),
    unlist(sigma_distances_std_clr_2[[3]]), unlist(sigma_distances_std_clr_2[[4]]),
    unlist(sigma_distances_rob_clr_2[[1]]), unlist(sigma_distances_rob_clr_2[[2]]),
    unlist(sigma_distances_rob_clr_2[[3]]), unlist(sigma_distances_rob_clr_2[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_2$method <- factor(sigma_diff_df_2$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_2$size <- factor(sigma_diff_df_2$size, 
                        levels = c("20", "40", "80", "160"))

plot1 <- ggplot(diff_df_2, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))


plot2 <- ggplot(sigma_diff_df_2, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))

grid.arrange(plot1, plot2, ncol = 2)

```



<!-- Zusammenfassen werden zunächst einmal die Ergebnisse für das erste Simulationsbeispiel zusammengetragen. Dafür wurden jeweils **50** Simulationen durchgeführt, 
bei denen 100 Zähl Kompositionen mit den oben beschriebenen Parametern generiert wurden. Als Mittelwertvektor $\boldsymbol{\mu}$ der clr-transformierten Koordinaten 
wurde der Vektor $\boldsymbol{\mu} = (0, 1.5, 0.5, -1.5, -0.5)$ gewählt, womit eine relativ starke Variation der relativen Kompositionsanteile abgebildet wird. D.h. 
die zweite Komponente (TODO: Interpretation clr Koodinaten überprüfen) ist relative zu allen anderen Komponenten dominant, während die vierte Komponente stark unterrepräsentiert 
ist. Dies führt für die vierte Komponente zu einer erheblichen Anzahl von Nullwerten in den beobachteten Zähl Kompositionen. Dies stellt ein erhebliches Problem für 
die klassische Anwendung multivariater Methoden dar (TODO: ausführen). Da der MCEM Algorithmus jedoch direkt die Daten auf dem Simplex evaluiert, wird das Problem 
vermieden (TODO: besser erklären oder raus).   -->


<!-- TODO: Vergleich von eigenvalues und totalVariance (Vgl. Filzmoser et al. 2019, p. 133) -->

The simple simulation study already provides several valuable insights. First, it is evident that the quality of results improves with increasing sample size. The MCEM 
method shows on average, a clear improvement compared to both standard and robust PCA. Furthermore it is much better in determining the true structure of the mean vectors 
for samples with small sample sizes. In addition, looking at the distances of the covariance matrices, the difference between the methods is particularly pronounced in 
the settings with small sample sizes, but still holds for moderate samples sizes. This supports the suitability of the MCEM method for datasets with small sample sizes.  
Interestingly, the opposite effect is observed when comparing standard PCA with its robust variant. The robust computation of PCA offers some advantages at moderate sample 
sizes, while for very small samole sizes, high variance and worse mean estimates are observed. This aligns with the expectation that robust estimation of the covariance 
structure requires a certain number of samples to effectively identify outliers.

Following, the first two principal components are display for every method. 

```{r plot_pca_results, eval=TRUE}
plot_pca_rotation_sim <- function(rotation_list1, rotation_list2, rotation_list3, scale = 1, main = "PCA Comparison") {
    # Create empty dataframe for all rotations
    df_all <- data.frame()
    
    df_oracle <- data.frame(
      x = c(0, sqrt(1/2), -sqrt(1/2), 0, 0),
      y = c(-sqrt(1/2), 0, 0, sqrt(1/2), 0),
      method = "oracle PC",
      simulation = 1
    )
    # Process each simulation
    for(i in seq_along(rotation_list1)) {
        # Create data frames for each rotation
        df1 <- data.frame(
            x = rotation_list1[[i]][,1] * scale, 
            y = rotation_list1[[i]][,2] * scale, 
            method = "MCEM",
            simulation = i
        )
        
        df2 <- data.frame(
            x = rotation_list2[[i]][,1] * scale, 
            y = rotation_list2[[i]][,2] * scale, 
            method = "standard PCA",
            simulation = i
        )
        
        df3 <- data.frame(
            x = rotation_list3[[i]][,1] * scale, 
            y = rotation_list3[[i]][,2] * scale, 
            method = "robust PCA",
            simulation = i
        )
  
        df_all <- rbind(df_all, df1, df2, df3)
    }
    
    df_all <- rbind(df_all, df_oracle)
    
    # Add labels
    labels <- if (!is.null(rownames(rotation_list1[[1]]))) {
        rownames(rotation_list1[[1]])
    } else {
        seq_len(nrow(rotation_list1[[1]]))
    }
    
    df_all$method <- factor(df_all$method, 
                           levels = c("MCEM", "standard PCA", "robust PCA", "oracle PC"))
    
    # Calculate mean directions for 5 components
    mean_directions <- do.call(rbind, lapply(unique(df_all$method[1:3]), function(m) {
        # Get all simulations for this method
        method_data <- df_all[df_all$method == m, ]

        # Reshape to get components together (every 5 rows belong to one simulation)
        comp_x <- matrix(method_data$x, nrow = 5)
        comp_y <- matrix(method_data$y, nrow = 5)

        # Calculate mean for each component
        mean_x <- rowMeans(comp_x)
        mean_y <- rowMeans(comp_y)

        data.frame(
            x = mean_x,
            y = mean_y,
            method = m
        )
    }))

    ggplot() +
        geom_segment(data = df_all,
                    aes(x = 0, y = 0, xend = x, yend = y, 
                        color = method, group = simulation),
                    arrow = arrow(length = unit(0.2, "cm")),
                    alpha = 0.3, linewidth = 0.7) +
        # Add mean directions
        geom_segment(data = mean_directions,
                    aes(x = 0, y = 0, xend = x, yend = y, color = method),
                    arrow = arrow(length = unit(0.2, "cm")),
                    linewidth = 0.7) +
        geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
        geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
        scale_color_manual(values = c("#8d450e", "#9ebcda", "#8856a7", "black"),
                          guide = "none") +
        new_scale_color() +
        scale_color_manual(values = c("#4a0000", "#00008B", "#4B0082")) +
        labs(title = main,
             x = "PC1",
             y = "PC2") +
        theme_grey() +
        theme(legend.position = "top")
}

mcem_rotation_list <- list(n_simulations)
standard_rotation_list <- list(n_simulations)
robust_rotation_list <- list(n_simulations)

mcem_rotation_list <- lapply(seq_along(simulation_results_list_2[[2]]), function(j) {
  t(basis_matrix) %*% simulation_results_list_2[[2]][[j]]$pca$rotation
})

standard_rotation_list <- lapply(seq_along(pca_clr_std_2[[2]]), function(j) {
    pca_clr_std_2[[2]][[j]]$rotation
})

robust_rotation_list <- lapply(seq_along(pca_clr_rob_2[[2]]), function(j) {
    pca_clr_rob_2[[2]][[j]]$loadings
})

plot_pca_rotation_sim(mcem_rotation_list,
                      standard_rotation_list, 
                      robust_rotation_list, 
                      scale = 1, 
                      main =  expression(paste("PCA comparison for ", m[i], " = 40")))

```

The thicker lines represent the means of the principal components over all simulations. Interestingly, it can be seen that the main advantage of the 
MCEM method is not the direction of the components but a better capability to catch the correct length of the respective vector. In fact it seems that 
the standardal Method underestimates the variation in the second principal components and the robust method overestimates it.

```{r mean directions, eval=TRUE}
rotation_list1 <- mcem_rotation_list
rotation_list2 <- standard_rotation_list
rotation_list3 <- robust_rotation_list

scale <- 1

    df_all <- data.frame()
    
    # Process each simulation
    for(i in seq_along(rotation_list1)) {
        # Create data frames for each rotation
        df1 <- data.frame(
            x = rotation_list1[[i]][,1] * scale, 
            y = rotation_list1[[i]][,2] * scale, 
            method = "MCEM",
            simulation = i
        )
        
        df2 <- data.frame(
            x = rotation_list2[[i]][,1] * scale, 
            y = rotation_list2[[i]][,2] * scale, 
            method = "standard PCA",
            simulation = i
        )
        
        df3 <- data.frame(
            x = rotation_list3[[i]][,1] * scale, 
            y = rotation_list3[[i]][,2] * scale, 
            method = "robust PCA",
            simulation = i
        )
        
        df_all <- rbind(df_all, df1, df2, df3)
    }
    
    # Add labels
    labels <- if (!is.null(rownames(rotation_list1[[1]]))) {
        rownames(rotation_list1[[1]])
    } else {
        seq_len(nrow(rotation_list1[[1]]))
    }
    
    df_all$method <- factor(df_all$method, 
                           levels = c("MCEM", "standard PCA", "robust PCA"))
    
    # Calculate mean directions for 5 components
    mean_directions <- do.call(rbind, lapply(unique(df_all$method), function(m) {
        # Get all simulations for this method
        method_data <- df_all[df_all$method == m, ]

        # Reshape to get components together (every 5 rows belong to one simulation)
        comp_x <- matrix(method_data$x, nrow = 5)
        comp_y <- matrix(method_data$y, nrow = 5)

        # Calculate mean for each component
        mean_x <- rowMeans(comp_x)
        mean_y <- rowMeans(comp_y)

        data.frame(
            pc_1 = mean_x,
            pc_2 = mean_y,
            method = m
        )
    }))

mean_directions
```

To further investigate 
that, the screeplots for the Simulations can be plotted. 

```{r summary screeplot, eval=TRUE}
normalize <- function(v) {
    v / sqrt(sum(v^2))
}
# Create dataframe with eigenvalues from all simulations
scree_df <- data.frame()
for(i in seq_along(simulation_results_list_2[[1]])) {
  # MCEM eigenvalues
  mcem_eig <- simulation_results_list_2[[1]][[i]]$pca$sdev^2
  mcem_eig <- normalize(mcem_eig)
  
  # standard PCA eigenvalues
  standard_eig <-  pca_clr_std_2[[1]][[i]]$sdev^2
  standard_eig <- normalize(standard_eig)
  
  # Robust PCA eigenvalues
  robust_eig <- pca_clr_rob_2[[1]][[i]]$eigenvalues
  robust_eig <- normalize(robust_eig)
  
  scree_df <- rbind(scree_df, 
                    data.frame(eigenvalue = c(mcem_eig, standard_eig[1:4], robust_eig),
                             component = rep(1:length(mcem_eig), 3),
                             method = rep(c("MCEM", "standard PCA", "robust PCA"), 
                                       each = length(mcem_eig)),
                             simulation = i))
}

scree_df$method <- factor(scree_df$method, 
                           levels = c("MCEM", "standard PCA", "robust PCA"))
# Create screeplot
ggplot(scree_df, aes(x = component, y = eigenvalue, color = method, group = interaction(simulation, method))) +
    geom_line(alpha = 0.7) +
    geom_point(alpha = 0.7) +
    # stat_summary(aes(group = method), fun = mean, geom = "line", size = 1.2) +
    scale_color_manual(values = c("#8d450e", "#9ebcda", "#8856a7")) +
    labs(x = "Principal Component",
         y = "Eigenvalue",
         title = "Screeplot Comparison") +
    theme_grey() +
    theme(legend.position = "top")
```

And the covariance structure for the oracle, the MCEM estimate and the standard and robust PCA estimate.
TODO: plus graphic with the calculated mean structure?

```{r comparison covariance structure, eval=TRUE}
true_sigma_2

```

Following a more complex setting with 13 compositional parts and 4 components is shown for each method. 

**The main deviance to the other settings is the sample size $m_i$**:
Is is sampled from a density distribution with:
```{r density distribution front, eval=TRUE}
data <- tar_read(data_kl15)
data <- data * 0.01
density_estimate <- density(data$aggregate)
plot(density_estimate)
```

alternatively it would be possible to construct a normal density with the kl15 parameters:


```{r density distribution normal, eval=TRUE}
data <- tar_read(data_kl15)
data <- data * 0.01
mu <- mean(data$aggregate)
sigma <- sd(data$aggregate)

# Create normal density instead of kernel density
x <- seq(min(data$aggregate), max(data$aggregate), length.out = 512)
density_estimate_N <- list(
    x = x,
    y = dnorm(x, mean = mu, sd = sigma)
)
plot(density_estimate_N)
```

The clr mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters front 2, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_8 = c(0.5, 0.2, 0.12, 0.08)
mean_8 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_8 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_8 <- V_8 %*% diag(eigenvalues_8) %*% t(V_8) 

true_sigma_8

(true_v_8 <- mean_8)
```

```{r evaluation mean vector front 2, eval=TRUE}

simulation_data_list_8 <- list(
  tar_read(sim_6_01_n100),
  tar_read(sim_6_01_n200),
  tar_read(sim_6_01_n400),
  tar_read(sim_6_01_n800)
)

pca_clr_rob_8 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_8 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_8[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_8[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_8[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_8 <- list(
  tar_read(pca_6_01_n100),
  tar_read(pca_6_01_n200),
  tar_read(pca_6_01_n400),
  tar_read(pca_6_01_n800)
)

diff_mean_8 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_8 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_8 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_8[[j]][[i]] <- sqrt(sum((true_v_8 - t(basis_matrix) %*% simulation_results_list_8[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_8[[j]][[i]] <- sqrt(sum((true_v_8 - pca_clr_std_8[[j]][[i]]$center)^2))
    # diff_mean_rob_clr_8[[j]][[i]] <- sqrt(sum((true_v_8 - pca_clr_rob_8[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_8
diff_mean_std_clr = diff_mean_std_clr_8
# diff_mean_rob_clr = diff_mean_rob_clr_8

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation front 2, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_8 <- data.frame(
  differences = c(
    unlist(diff_mean_8[[1]]), unlist(diff_mean_8[[2]]), 
    unlist(diff_mean_8[[3]]), unlist(diff_mean_8[[4]]),
    unlist(diff_mean_std_clr_8[[1]]), unlist(diff_mean_std_clr_8[[2]]),
    unlist(diff_mean_std_clr_8[[3]]), unlist(diff_mean_std_clr_8[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("100", "200", "400", "800"), 
                         each = n_simulations), 2))
)

diff_df_8$method <- factor(diff_df_8$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_8$size <- factor(diff_df_8$size, 
                        levels = c("100", "200", "400", "800"))

ggplot(diff_df_8, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Observations",
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 6") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```


```{r evaluation covariance front 2, eval=TRUE}

sigma_distances_8 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_8 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_8 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_8[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_mcem, type = "F")
    
    sigma_hat_clr <- with(pca_clr_std_8[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_8[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances front 2, eval=TRUE}
sigma_diff_df_8 <- data.frame(
  differences = c(
    unlist(sigma_distances_8[[1]]), unlist(sigma_distances_8[[2]]), 
    unlist(sigma_distances_8[[3]]), unlist(sigma_distances_8[[4]]),
    unlist(sigma_distances_std_clr_8[[1]]), unlist(sigma_distances_std_clr_8[[2]]),
    unlist(sigma_distances_std_clr_8[[3]]), unlist(sigma_distances_std_clr_8[[4]]),
    unlist(sigma_distances_rob_clr_8[[1]]), unlist(sigma_distances_rob_clr_8[[2]]),
    unlist(sigma_distances_rob_clr_8[[3]]), unlist(sigma_distances_rob_clr_8[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("100", "200", "400", "800"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_8$method <- factor(sigma_diff_df_8$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_8$size <- factor(sigma_diff_df_8$size, 
                        levels = c("100", "200", "400", "800"))

ggplot(sigma_diff_df_8, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Observations",
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```


# Implementierung




## Objective Function

```{r objective function, eval=FALSE}
conditional_scores_log_ilr_db_2 <- function(scores,
                                          x_data_i,
                                          pca,
                                          basis_matrix,
                                          sc_factor) {
  scaling_factor <- sc_factor
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  norm_constant <- sum(exp(clr_comp))

  # Compute scaled log likelihood
  log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i) * log(norm_constant)
  # apply scaling when sc_factor ungleich 1
  if (scaling_factor != 1) {
    log_likelihood <- log_likelihood - scaling_factor
  }

  # Prior remains unchanged as it's already well-scaled
  log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))

  return(log_likelihood + log_prior)
}
```


## Gradient

```{r gradient, eval=FALSE}

gradient_cslc_ilr_db_2 <- function(scores,
                                 x_data_i,
                                 pca,
                                 basis_matrix,
                                 sc_factor) {
  scaling_factor <- sc_factor
  m_i <- sum(x_data_i)
  ilr_comp <- as.vector(pca$center + pca$rotation %*% scores)
  clr_comp <- ilr2clr(ilr_comp)
  composition <- clrInv_long(clr_comp)


  grad_vecs <- sapply(seq_along(scores), function(k) {
    e_k <- basis_matrix[k, ] 
    v_k <- pca$rotation[, k]
    term1 <- sum(x_data_i * e_k)
    term2 <- m_i * sum(composition * e_k)

    grad_k <- scaling_factor * v_k * (term1 - term2) 

    return(grad_k)
  })
  grad <- rowSums(grad_vecs)

  grad <- grad - scores / (pca$sdev^2)

  return(grad)
}

```

## Parallel processing


Implement the MCEM-algorithm with parallel processing:

```{r parallel processing, eval=FALSE}
# use parallel processing 
library(future.apply)

# Parallel E-Step implementation
parallel_estep <- function(x_data, pca, basis_matrix, r, k, lambda, sc_factor) {
  future_lapply(seq_along(x_data), function(i) {
    optim_result <- optim(rep(0, length = length(pca$sdev)),
                          conditional_scores_log_ilr_db_2,
                          gr = gradient_cslc_ilr_db_2,
                          x_data_i = x_data[[i]],
                          pca = pca,
                          basis_matrix = basis_matrix,
                          sc_factor = sc_factor,
                          control = list(fnscale = -1),
                          method = "BFGS")

    scores_median <- as.vector(optim_result$par)
    proposal_scores <- sapply(1:(r * k), function(t) {
      matrix(rnorm(length(scores_median),
                   mean = scores_median,
                   sd = lambda * pca$sdev))
    })

    log_weights <- apply(proposal_scores, 2, function(scores) {
      conditional_scores_log_ilr_db_2(scores,
                                      x_data[[i]],
                                      pca,
                                      basis_matrix,
                                      sc_factor) -
        sum(dnorm(scores,
                  mean = scores_median,
                  sd = lambda * pca$sdev,
                  log = TRUE))
    })

    weights <- stabilize_weights(log_weights)

    list(proposal_scores = proposal_scores,
         weights = weights)
  }, future.seed = TRUE)
}


fit_compositional_pca_ilr_db_par <- function(x_data,
                                         max_iter = 50,
                                         r = 10,
                                         lambda = 1,
                                         eps = 0.01,
                                         sc_factor = 0.001,
                                         sum_exp = TRUE,
                                         workers = 4) {
  start_time <- Sys.time()
  if (!is.list(x_data) && !is.matrix(x_data)) {
    stop("Input x_data must be a list or a matrix")
  }

  if (is.data.frame(x_data) || is.matrix(x_data)) {
    x_data <- apply(x_data, 1, function(x) x, simplify = FALSE)
  }

  lengths <- unique(sapply(x_data, length))
  if (length(lengths) != 1) {
    stop("All observations must have the same number of components")
  }
  D <- lengths

  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)

  # initial estimates
  nu <- rep(0, D - 1)
  Sigma <- diag(D - 1)
  pca <- prcomp(Sigma, center = FALSE)
  pca$center <- nu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  scores_median_list <- list(length(x_data))

  # Set up parallel workers
  plan(multisession, workers = workers)

  if (max_iter > 0) {
    for (k in 1:max_iter) {
      cat("Iteration:", k, "\n")

      # Parallel E-Step
      estep_results <- parallel_estep(x_data, pca, basis_matrix, r, k, lambda, sc_factor)

      # Extract results
      proposal_scores <- lapply(estep_results, `[[`, "proposal_scores")
      weights <- lapply(estep_results, `[[`, "weights")

      monitor_global_ess(weights, k)

      # M-Step ###################
      scores_matrix <- sapply(seq_along(weights), function(i){
          proposal_scores[[i]] %*% weights[[i]]
      })
      na_count <- sum(is.na(scores_matrix))  
      mu_scores <- rowMeans(scores_matrix, na.rm = TRUE)  
      cat(sprintf("Removed %d NA values when calculating mu_scores\n", na_count))      
      # update parameters
      pca_old <- pca
      pca$center <- pca$center + pca$rotation %*% mu_scores
      cat("center:", pca$center, "\n")
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i) {
          Reduce("+", lapply(1:(r * k), function(t) {
          C_it <- weights[[i]][t] * (proposal_scores[[i]][, t] - mu_scores) %*%
              t((proposal_scores[[i]][, t] - mu_scores))
          }))
      })) / length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
          cat("error eigen() in iteration", k, "for observation", i, "\n")
          cat("error message:", e$message, "\n")
          print("pca$sdev:")
          print(pca$sdev)
      })
      negative_eigenvalues <- eigen_decomp$values < 0
      if (any(negative_eigenvalues)) {
          warning(sprintf("Warning: %d eigenvalues are negative.\n
          They have been set to zero.",
                          sum(negative_eigenvalues)))
      }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      pca$rotation <- pca$rotation %*% eigen_decomp$vectors
      clr_rotation <- t(basis_matrix) %*% pca$rotation %*% basis_matrix
      cat("PCA1:", clr_rotation[ , 1], "\n")
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value center_diff:", critical_value_1, "\n")
      Sigma_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k) {
          pca_old$rotation[, k] %*% t(pca_old$rotation[, k]) * (pca_old$sdev[k]^2)
      }))
      Sigma_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k) {
        pca$rotation[, k] %*% t(pca$rotation[, k]) * (pca$sdev[k]^2)
      }))
      Sigma_diff <- Sigma_old - Sigma_new
      critical_value_2 <- norm(Sigma_diff, type = "F")
      cat("critical value Sigma_diff:", critical_value_2, "\n")  
      if (max(critical_value_1, critical_value_2) < eps) {
        constant <- apply(pca$rotation, 2, function(g) {
          sqrt(sum(g^2))
        })
        pca$rotation <- t(t(pca$rotation) / constant)
        pca$sdev <- pca$sdev * constant  
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "minutes"))
        return(list("iteration" = k,
                    "pca" = pca,
                    "x_data" = x_data,
                    "list_center" = center_list,
                    "list_sdev" = sdev_list,
                    "time" = elapsed_time))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) {
    sqrt(sum(g^2))
  })
  pca$rotation <- t(t(pca$rotation) / constant)
  pca$sdev <- pca$sdev * constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter,
              "pca" = pca,
              "x_data" = x_data,
              "list_center" = center_list,
              "list_sdev" = sdev_list,
              "time" = elapsed_time))
}

```

# Simulation

In general, we want to construct several sets of different context for the MCEM-algorithm:

- Different number of observations N
- Different number of compositional components D
- Different number of samples m_i
- equal share szenarios vs. unequal share szenarios
- Different number of principal components

We canstruct a matrix of Dx(D-1) principal component vectors and 
set the (D-1) vector of eigenvalues.


## Simulation Setting manual

```{r simulation_setting_1 function, eval=FALSE}
build_setting_1 <- function(n_samples,
                            eigenvalues = c(0.6, 0.3, 0.05, 0.05),
                            mean = c(0, 2, 0.5, -2, -0.5),
                            n_counts = 500) {
  set.seed(123)
  v1 <- c(0, 1/sqrt(2), -1/sqrt(2), 0, 0)  # Contrast between parts 2 and 3
  v2 <- c(-1/3, -1/3, -1/3, 1, 0)  # Focus on part 4
  v3 <- c(1/sqrt(2), 0, 0, -1/sqrt(2), 0)  # Additional contrast
  v4 <- c(1/2, 1/2, 0, 0, -1)  # Additional contrast

  V <- cbind(v1, v2, v3, v4)
  Sigma <- V %*% diag(eigenvalues) %*% t(V)
  
  clr_coords <- rmvnorm(n_samples, mean = mean, sigma = Sigma)
  ilr_coords <- clr2ilr(clr_coords)

  compositions <- clrInv(clr_coords)
  composition_list <- apply(compositions, 1, function(x) x, simplify = FALSE)

  x_data <- lapply(1:n_samples, function(i) {
    probs <- composition_list[[i]]
    rmultinom(1, n_counts, probs)[, 1]
  })
  x_data_matrix <- do.call(rbind, x_data)

  return(list("x_data" = x_data, "Sigma" = Sigma,"x_data_matrix" = x_data_matrix))
}

setting_1 <- build_setting_1(n_samples = 2000, n_counts = 300)
x_data_test <- setting_1$x_data
(sigma <- setting_1$Sigma)
```

Insights:

- the smaller m_i the more difficult it is to estimate the center, but the ESS value appears to be better

### Computation and pca_results

Targets are used for computation to avoid an unneccessary amount of computation.

```{r standard pca, eval=FALSE}
x_data_matrix <- setting_1$x_data_matrix
pca_ilr <- prcomp(ilr(x_data_matrix)) 
pca_clr <- prcomp(clr(x_data_matrix)) 
pca_ilr$center
plot_pca_rotation(pca_ilr$rotation)
plot_pca_rotation(pca_clr$rotation)
pca_ilr$rotation
pca_clr$rotation

# reconstruction Sigma
sigma_hat <- pca_ilr$rotation %*% pca_ilr$sdev^2 %*% t(pca_ilr$rotation) 

```
<!-- > pca_ilr$center
[1]  1.4395217 -0.4235406 -2.2104438 -0.6523904

> pca_ilr$rotation
            PC1         PC2         PC3         PC4
[1,]  0.1926516  0.43686024 -0.20391570 -0.85466769
[2,] -0.3175480 -0.81793217 -0.05469878 -0.47661126
[3,]  0.8993526 -0.37227342 -0.21991887  0.06490851
[4,] -0.2306807  0.03940283 -0.95239809  0.19537587

> pca_clr$rotation
             PC1         PC2         PC3         PC4        PC5
[1,]  0.21462584 -0.12366791 -0.44296865 -0.73649221 -0.4472136
[2,] -0.05782473 -0.74148159 -0.15458831  0.47219043 -0.4472136
[3,]  0.46731579  0.56918348 -0.23178643  0.45157631 -0.4472136
[4,] -0.83044395  0.33120898 -0.02250736 -0.01252504 -0.4472136
[5,]  0.20632705 -0.03524296  0.85185075 -0.17474949 -0.4472136 -->

```{r computation, eval=FALSE}
pca_results_1 <- fit_compositional_pca_ilr_db_par(x_data, eps = 0.02, workers = 10)

```

What we expect:

```{r expected results, eval=FALSE}


```

What we get:

```{r actual results, eval=FALSE}

```

## evaluation

True parameters:

```{r true parameters, eval=TRUE}
sim1 <- tar_read(sim_comp_1_smi)
sigma <- sim1$Sigma
x_data <- sim1$x_data
# change list into data matrix
x_matrix <- do.call(rbind, x_data)

D <- ncol(x_matrix)
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)


pca_sim1 <- prcomp(ilr(x_matrix))
pca_sim1$center
clr_pca <- prcomp(clr(x_matrix))
clr_rotation <- t(basis_matrix) %*% pca_sim1$rotation %*% basis_matrix
sigma_sampled <- pca_sim1$rotation %*% diag(pca_sim1$sdev^2) %*% t(pca_sim1$rotation)

rotation_true <- eigen(sigma)
rotation_true$vectors
```

> pca_sim1$center
[1]  1.3578022 -0.3892208 -0.5917104 -0.7016805

**that is not the true center, but the sampled center!**

> mean <- c(0, 1.5, 0.5, -1.5, -0.5) %*% t(basis_matrix)
> mean
         [,1]       [,2]      [,3]      [,4]
[1,] 1.414214 -0.4082483 -2.453739 -0.559017

        [,1]       [,2]      [,3]      [,4]
[1,] 1.06066 -0.2041241 -1.876388 -0.559017

> rotation_true$vectors
            [,1]        [,2]        [,3]         [,4]      [,5]
[1,] -0.02532766 -0.34909098 -0.37546886  0.732473295 0.4472136
[2,] -0.72116611 -0.24505649 -0.08896847 -0.460381772 0.4472136
[3,]  0.69114282 -0.29732982 -0.14039828 -0.462822753 0.4472136
[4,]  0.02185786  0.85336096 -0.26700427 -0.002454104 0.4472136
[5,]  0.03349309  0.03811633  0.87183989  0.193185333 0.4472136

> clr_rotation
            [,1]        [,2]        [,3]         [,4]        [,5]
[1,]  0.08571733  0.04002381  0.67511484 -0.315030596 -0.48582538
[2,] -0.38985998  0.21730316 -0.01295807 -0.447341034  0.63285592
[3,]  0.56223038 -0.63291841 -0.16132148 -0.007227651  0.23923716
[4,] -0.51065499 -0.18640826  0.05991235  0.704524488 -0.06737359
[5,]  0.25256725  0.56199971 -0.56074765  0.065074793 -0.31889411

> clr_pca$rotation
            PC1         PC2         PC3         PC4        PC5
[1,] -0.0323102 -0.49989536  0.50401171  0.54316929 -0.4472136
[2,]  0.4293292 -0.05986581  0.33385516 -0.70755443 -0.4472136
[3,] -0.8450978  0.10286017 -0.06071605 -0.26747528 -0.4472136
[4,]  0.2292771 -0.33349311 -0.79406573  0.07532597 -0.4472136
[5,]  0.2188018  0.79039411  0.01691490  0.35653445 -0.4472136

**Why are they so different?**


For a simulation with `n_simulations` runs, we can campare the parameter of interest with their true value.



## Simulation Setting 1

```{r simulation setting color, eval=TRUE}
set_1 <- c("#e0ecf4", "#9ebcda", "#8856a7")

```


This setting calculates a complex (known) laten covariance matrix 

1. 20 Simulationen mit 
  - mean vector: c(0, 1.5, 0.5, -1.5, -0.5)
  - eigenvalues: c(0.6, 0.3, 0.05, 0.05)
  - n_observations: c(100, 100, 100, 300)
  - n_components: 5
  - m_i: c(30, 60, 90, 90)

Sigma can be constructed as follows:

```{r latent parameters 1, eval=TRUE}
D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

v1 <- c(0, 1/sqrt(2), -1/sqrt(2), 0, 0)  # Contrast between parts 2 and 3
v2 <- c(-1/3, -1/3, -1/3, 1, 0)  # Focus on part 4
v3 <- c(1/sqrt(2), 0, 0, -1/sqrt(2), 0)  # Additional contrast
v4 <- c(1/2, 1/2, 0, 0, -1)  # Additional contrast
eigenvalues_1 <- c(0.6, 0.3, 0.05, 0.05)
mean_1 <- c(0, 1.5, 0.5, -1.5, -0.5)

V_1 <- cbind(v1, v2, v3, v4)
true_sigma_1 <- V_1 %*% diag(eigenvalues_1) %*% t(V_1)

(true_v_1 <- mean_1)
```

Sigma contains strong(er) variations for part 2,3 and 4 with negative correlation between 2 and 3 and 4 against 1,2,3. 

The mean vector contains relativly strong variation in the dominance of parts.

```{r evaluation mean vector, eval=TRUE}
n_simulations <- 20

simulation_data_list_1 <- list(
  tar_read(sim_comp_1_smi_nSim),
  tar_read(sim_comp_1_smi_nSim_60),
  tar_read(sim_comp_1_smi_nSim_90),
  tar_read(sim_comp_1_smi_nSim_90_300)
)

# Sanity check
length(simulation_data_list_1[[1]][[1]]$x_data) == 100
length(simulation_data_list_1[[1]][[1]]$x_data[[1]]) ==  5

pca_clr_rob_1 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_1 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_1[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_1[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_1[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_1 <- list(
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_1),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_1_60),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_1_90),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_1_90_300)
)

diff_mean_1 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_1 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_1 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_1[[j]][[i]] <- sqrt(sum((true_v_1 - t(basis_matrix) %*% simulation_results_list_1[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_1[[j]][[i]] <- sqrt(sum((true_v_1 - pca_clr_std_1[[j]][[i]]$center)^2))
    diff_mean_rob_clr_1[[j]][[i]] <- sqrt(sum((true_v_1 - pca_clr_rob_1[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_1
diff_mean_std_clr = diff_mean_std_clr_1
diff_mean_rob_clr = diff_mean_rob_clr_1
```

So, we get much better results. Let's visualize the results with boxplots for each case.

```{r boxplot simulation, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_1 <- data.frame(
  differences = c(
    unlist(diff_mean_1[[1]]), unlist(diff_mean_1[[2]]), 
    unlist(diff_mean_1[[3]]), unlist(diff_mean_1[[4]]),
    unlist(diff_mean_std_clr_1[[1]]), unlist(diff_mean_std_clr_1[[2]]),
    unlist(diff_mean_std_clr_1[[3]]), unlist(diff_mean_std_clr_1[[4]])
    # unlist(diff_mean_rob_clr[[1]]), unlist(diff_mean_rob_clr[[2]]),
    # unlist(diff_mean_rob_clr[[3]]), unlist(diff_mean_rob_clr[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "90", "300 observations"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_1$method <- factor(diff_df_1$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_1$size <- factor(diff_df_1$size, 
                        levels = c("30", "60", "90", "300 observations"))

ggplot(diff_df_1, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1[1:2]) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of mean estimate to true mean: Setting 1") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance, eval=TRUE}
true_sigma_1 <- simulation_data_list_1[[1]][[1]]$Sigma

sigma_distances_1 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_1 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_1 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_1[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_1[[j]][[i]] <- norm(true_sigma_1 - sigma_hat_mcem, type = "F")
    
    sigma_hat_clr <- with(pca_clr_std_1[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_1[[j]][[i]] <- norm(true_sigma_1 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_1[[j]][[i]],
      loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_1[[j]][[i]] <- norm(true_sigma_1 - sigma_hat_rob, type = "F")
  }
}

sigma_diff_df_1 <- data.frame(
  differences = c(
    unlist(sigma_distances_1[[1]]), unlist(sigma_distances_1[[2]]), 
    unlist(sigma_distances_1[[3]]), unlist(sigma_distances_1[[4]]),
    unlist(sigma_distances_std_clr_1[[1]]), unlist(sigma_distances_std_clr_1[[2]]),
    unlist(sigma_distances_std_clr_1[[3]]), unlist(sigma_distances_std_clr_1[[4]]),
    unlist(sigma_distances_rob_clr_1[[1]]), unlist(sigma_distances_rob_clr_1[[2]]),
    unlist(sigma_distances_rob_clr_1[[3]]), unlist(sigma_distances_rob_clr_1[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "90", "300 observations"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_1$method <- factor(sigma_diff_df_1$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_1$size <- factor(sigma_diff_df_1$size, 
                        levels = c("30", "60", "90", "300 observations"))



ggplot(sigma_diff_df_1, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of covariance estimate to true covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

### Visualisation Principal components

```{r pca biplot, eval=TRUE}
plot_pca_rotation_gg <- function(rotation1, rotation2, rotation3, scale = 1, main = "PCA Comparison") {
    # Create data frames for each rotation
    df1 <- data.frame(x = rotation1[,1] * scale, y = rotation1[,2] * scale, method = "MCEM")
    df2 <- data.frame(x = rotation2[,1] * scale, y = rotation2[,2] * scale, method = "standard PCA")
    df3 <- data.frame(x = rotation3[,1] * scale, y = rotation3[,2] * scale, method = "robust PCA")
    
    # Combine data
    df <- rbind(df1, df2, df3)
    
    # Add labels for rotation1
    labels <- if (!is.null(rownames(rotation1))) rownames(rotation1)
              else seq_len(nrow(rotation1))
    
    df$method <- factor(df$method, levels = c("MCEM", "standard PCA", "robust PCA"))


    # Create plot
    ggplot() +
        geom_segment(data = df, 
                    aes(x = 0, y = 0, xend = x, yend = y, color = method),
                    arrow = arrow(length = unit(0.2, "cm"))) +
        geom_text(data = df1, 
                 aes(x = x, y = y, label = labels),
                 hjust = -0.2) +
        geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
        geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
        scale_color_manual(values = c("#8d450e", "#9ebcda", "#8856a7")) +
        labs(title = main,
             x = "PC1",
             y = "PC2") +
        theme_grey() +
        theme(legend.position = "top")
}

plot_pca_rotation_gg(t(basis_matrix) %*% simulation_results_list_1[[1]][[1]]$pca$rotation, pca_clr_std_1[[1]][[1]]$rotation, pca_clr_rob_1[[1]][[1]]$loadings, main = "PCA Comparison")

```

As an ideal result, we would see the contrast of component 2 to 3 in PC1 and 4 against 1,2,3 in PC2.

## Simulation Setting 2

This setting calculates a simpler version, where the clr-coordinates are linear combinations of just two scores and 
two principal components:

  pc_1 <- c(0, 1, -1, 0, 0)  # Contrast between parts 2 and 3
  pc_2 <- c(-1, 0, 0, 1, 0)  # Contrast between parts 1 and 4

  lambda_1 <- eigenvalues[1]
  lambda_2 <- eigenvalues[2]

  
  clr_coords <- lapply(1:n_observations, function(i){
    mean + rnorm(1, 0, lambda_1)*pc_1 + rnorm(1, 0, lambda_2)*pc_2
  })

```{r latent parameters 2, eval=TRUE}
D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues <- c(0.6, 0.3)
mean_2 <- c(0, 1, 0.5, -1, -0.5) # those are clr-coordinates

# the structure of the covariance matrix is a bit degenerated because we work with only two components
eigenvalues_2 <- c(0.6, 0.3, 0, 0)
V_2 <- cbind(c(0, 1, -1, 0, 0), c(-1, 0, 0, 1, 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_2 <- V_2 %*% diag(eigenvalues_2) %*% t(V_2) 

true_sigma_2

(true_v_2 <- mean_2)
```



1. 20 Simulationen mit 
  - mean vector: c(0, 1, 0.5, -1, -0.5)
  - eigenvalues: c(0.6, 0.3)
  - n_observations: c(100, 100, 100, 300)
  - n_components: 5
  - m_i: c(30, 60, 90, 90)




```{r evaluation mean vector 2, eval=TRUE}

simulation_data_list_2 <- list(
  tar_read(sim_comp_2_smi_nSim),
  tar_read(sim_comp_2_smi_nSim_60),
  tar_read(sim_comp_2_smi_nSim_120),
  tar_read(sim_comp_2_smi_nSim_120_300)
)

pca_clr_rob_2 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_2 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_2[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_2[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_2[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_2 <- list(
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_60),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_120),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_120_300)
)

diff_mean_2 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_2 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_2 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_2[[j]][[i]] <- sqrt(sum((true_v_2 - t(basis_matrix) %*% simulation_results_list_2[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_2[[j]][[i]] <- sqrt(sum((true_v_2 - pca_clr_std_2[[j]][[i]]$center)^2))
    diff_mean_rob_clr_2[[j]][[i]] <- sqrt(sum((true_v_2 - pca_clr_rob_2[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_2
diff_mean_std_clr = diff_mean_std_clr_2
diff_mean_rob_clr = diff_mean_rob_clr_2
```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 2, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_2 <- data.frame(
  differences = c(
    unlist(diff_mean_2[[1]]), unlist(diff_mean_2[[2]]), 
    unlist(diff_mean_2[[3]]), unlist(diff_mean_2[[4]]),
    unlist(diff_mean_std_clr_2[[1]]), unlist(diff_mean_std_clr_2[[2]]),
    unlist(diff_mean_std_clr_2[[3]]), unlist(diff_mean_std_clr_2[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "300 observations"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_2$method <- factor(diff_df_2$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_2$size <- factor(diff_df_2$size, 
                        levels = c("30", "60", "120", "300 observations"))

ggplot(diff_df_2, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 2, eval=TRUE}
sigma_distances_2 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_2 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_2 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_2[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_2[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_2[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_2[[j]][[i]] <- norm(true_sigma_2 - sigma_hat_rob, type = "F")
  }
}

sigma_diff_df_2 <- data.frame(
  differences = c(
    unlist(sigma_distances_2[[1]]), unlist(sigma_distances_2[[2]]), 
    unlist(sigma_distances_2[[3]]), unlist(sigma_distances_2[[4]]),
    unlist(sigma_distances_std_clr_2[[1]]), unlist(sigma_distances_std_clr_2[[2]]),
    unlist(sigma_distances_std_clr_2[[3]]), unlist(sigma_distances_std_clr_2[[4]]),
    unlist(sigma_distances_rob_clr_2[[1]]), unlist(sigma_distances_rob_clr_2[[2]]),
    unlist(sigma_distances_rob_clr_2[[3]]), unlist(sigma_distances_rob_clr_2[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "300 observations"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_2$method <- factor(sigma_diff_df_2$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_2$size <- factor(sigma_diff_df_2$size, 
                        levels = c("30", "60", "120", "300 observations"))

ggplot(sigma_diff_df_2, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

Surprisingly, the standard PCA performs best in this setting. Let's have a look at the calculated Kovarianzes and the MCEM parameters:

```{r evaluation covariance sanity 2, eval=TRUE} 
true_sigma_2
with(simulation_results_list_2[[1]][[1]]$pca,t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
with(pca_clr_std_2[[1]][[1]], rotation %*% diag(sdev^2) %*% t(rotation))

simulation_results_list_2[[1]][[1]]$iteration
simulation_results_list_2[[1]][[1]]$time
simulation_results_list_2[[1]][[1]]$pca$sdev
```

The main reason seems to be that the MCEM algorithm fails to identify the second component. A reason for this could be that the identification
of the first component is already enough to reach convergence.
The MCEM algrotithm **fails** to identify relevant variations. What happens if we increase lambda? Or try with a smaller convergence criterium?



Plot the resulting pairs of results for the biplots of the first two components:

1. $m_i$ = 30 for the first simulation:

```{r biplot simulation 1, eval=TRUE}
plot_pca_rotation_gg(t(basis_matrix) %*% simulation_results_list_2[[1]][[1]]$pca$rotation, pca_clr_std_2[[1]][[1]]$rotation, pca_clr_rob_2[[1]][[1]]$loadings, main = "PCA Comparison")
```

## Simulation Setting 3

```{r latent parameters 3, eval=TRUE}
eigenvalues_3 = c(0.5, 0.3, 0.1, 0.1)
mean_3 = c(0, 1.5, 0.5, -1.5, -0.5)

# the structure of the covariance matrix is a bit degenerated because we work with only two components
V_3 <- cbind(c(0, 1/sqrt(2), -1/sqrt(2), 0, 0) , c(-1/3, -1/3, -1/3, 1, 0), c(1/2, 1/2, 0, 0, -1), c(1/sqrt(2), 0, 0, -1/sqrt(2), 0))
true_sigma_3 <- V_3 %*% diag(eigenvalues_3) %*% t(V_3) 

true_sigma_3

(true_v_3 <- mean_3)
```



That is exactly the same as for setting 2 but the MCEM algorithm is evaluated with a higher convergence criterium: 0.03 (instead of 0.01)


```{r evaluation mean vector 3, eval=TRUE}
simulation_data_list_3 <- list(
  tar_read(sim_3_smi_nSim_30),
  tar_read(sim_3_smi_nSim_60),
  tar_read(sim_3_smi_nSim_120),
  tar_read(sim_3_smi_nSim_400)
)

pca_clr_rob_3 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_3 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_3[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_3[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_3[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_3 <- list(
  tar_read(pca_smi_nSim_3_30),
  tar_read(pca_smi_nSim_3_60),
  tar_read(pca_smi_nSim_3_120),
  tar_read(pca_smi_nSim_3_400)
)

diff_mean_3 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_3 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_3 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_3[[j]][[i]] <- sqrt(sum((true_v_3 - t(basis_matrix) %*% simulation_results_list_3[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_3[[j]][[i]] <- sqrt(sum((true_v_3 - pca_clr_std_3[[j]][[i]]$center)^2))
    diff_mean_rob_clr_3[[j]][[i]] <- sqrt(sum((true_v_3 - pca_clr_rob_3[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_3
diff_mean_std_clr = diff_mean_std_clr_3
diff_mean_rob_clr = diff_mean_rob_clr_3
```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 3, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_3 <- data.frame(
  differences = c(
    unlist(diff_mean_3[[1]]), unlist(diff_mean_3[[2]]), 
    unlist(diff_mean_3[[3]]), unlist(diff_mean_3[[4]]),
    unlist(diff_mean_std_clr_3[[1]]), unlist(diff_mean_std_clr_3[[2]]),
    unlist(diff_mean_std_clr_3[[3]]), unlist(diff_mean_std_clr_3[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "400"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_3$method <- factor(diff_df_3$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_3$size <- factor(diff_df_3$size, 
                        levels = c("30", "60", "120", "400"))

ggplot(diff_df_3, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 3") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 3, eval=TRUE}
sigma_distances_3 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_3 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_3 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_3[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_3[[j]][[i]] <- norm(true_sigma_3 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_3[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_3[[j]][[i]] <- norm(true_sigma_3 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_3[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_3[[j]][[i]] <- norm(true_sigma_3 - sigma_hat_rob, type = "F")
  }
}

sigma_diff_df_3 <- data.frame(
  differences = c(
    unlist(sigma_distances_3[[1]]), unlist(sigma_distances_3[[2]]), 
    unlist(sigma_distances_3[[3]]), unlist(sigma_distances_3[[4]]),
    unlist(sigma_distances_std_clr_3[[1]]), unlist(sigma_distances_std_clr_3[[2]]),
    unlist(sigma_distances_std_clr_3[[3]]), unlist(sigma_distances_std_clr_3[[4]]),
    unlist(sigma_distances_rob_clr_3[[1]]), unlist(sigma_distances_rob_clr_3[[2]]),
    unlist(sigma_distances_rob_clr_3[[3]]), unlist(sigma_distances_rob_clr_3[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "400"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_3$method <- factor(sigma_diff_df_3$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_3$size <- factor(sigma_diff_df_3$size, 
                        levels = c("30", "60", "120", "400"))

ggplot(sigma_diff_df_3, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance: Setting 3") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

In this case a higher convergence criterium does not change the quality of the results **but it improves the speed of the algorithm significantly**.


## Simulation Setting 4

```{r latent parameters 4, eval=TRUE}
eigenvalues_4 = c(0.5, 0.3, 0.15, 0.05, 0)
mean_4 = c(0, 0.9, 0.3, -0.8, -0.2)

# the structure of the covariance matrix is a bit degenerated because we work with only two components
V_4 <- cbind(c(0, 1/sqrt(2), -1/sqrt(2), 0, 0) , c(0.5 * sqrt(2/3), 0.5 * sqrt(2/3), 0.5 * sqrt(2/3), -1, 0) , c(1/sqrt(2), 0, 0, -1/sqrt(2), 0) , c(1/4 * sqrt(4/5), 1/4 * sqrt(4/5), 1/4 * sqrt(4/5), 1/4 * sqrt(4/5), -1), c(0, 0, 0, 0, 0))
true_sigma_4 <- V_4 %*% diag(eigenvalues_4) %*% t(V_4) 

true_sigma_4

(true_v_4 <- mean_4)
```



**The mean** is more stable with `(0, 0.9, 0.3, -0.8, -0.2)`

This is similar to setting one, but with more complexity:

There are four relevant components:

1. Contrast between parts 2 and 3

2. Contrast between parts 1,2,3 and 4

3. Contrast between parts 1,2 and 5

4. Contrast between part 1 and 4

All MCEM results are evaluated with an epsilon of 0.03

```{r evaluation mean vector 4, eval=TRUE}

simulation_data_list_4 <- list(
  tar_read(sim_4_smi_nSim_30),
  tar_read(sim_4_smi_nSim_60),
  tar_read(sim_4_smi_nSim_120),
  tar_read(sim_4_smi_nSim_400)
)

pca_clr_rob_4 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_4 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_4[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_4[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_4[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_4 <- list(
  tar_read(pca_smi_nSim_4_30),
  tar_read(pca_smi_nSim_4_60),
  tar_read(pca_smi_nSim_4_120),
  tar_read(pca_smi_nSim_4_400)
)

diff_mean_4 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_4 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_4 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_4[[j]][[i]] <- sqrt(sum((true_v_4 - t(basis_matrix) %*% simulation_results_list_4[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_4[[j]][[i]] <- sqrt(sum((true_v_4 - pca_clr_std_4[[j]][[i]]$center)^2))
    diff_mean_rob_clr_4[[j]][[i]] <- sqrt(sum((true_v_4 - pca_clr_rob_4[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_4
diff_mean_std_clr = diff_mean_std_clr_4
diff_mean_rob_clr = diff_mean_rob_clr_4

# # Sanity check
# true_v
# t(basis_matrix) %*% simulation_results_list[[1]][[1]]$pca$center
# pca_clr_std_all[[1]][[1]]$center
# t(basis_matrix) %*% simulation_results_list[[2]][[1]]$pca$center
# pca_clr_std_all[[2]][[1]]$center
# t(basis_matrix) %*% simulation_results_list[[3]][[1]]$pca$center
# pca_clr_std_all[[3]][[1]]$center
# t(basis_matrix) %*% simulation_results_list[[4]][[1]]$pca$center
# pca_clr_std_all[[4]][[1]]$center

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 4, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_4 <- data.frame(
  differences = c(
    unlist(diff_mean_4[[1]]), unlist(diff_mean_4[[2]]), 
    unlist(diff_mean_4[[3]]), unlist(diff_mean_4[[4]]),
    unlist(diff_mean_std_clr_4[[1]]), unlist(diff_mean_std_clr_4[[2]]),
    unlist(diff_mean_std_clr_4[[3]]), unlist(diff_mean_std_clr_4[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "400"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_4$method <- factor(diff_df_4$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_4$size <- factor(diff_df_4$size, 
                        levels = c("30", "60", "120", "400"))

ggplot(diff_df_4, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 4") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 4, eval=TRUE}

sigma_distances_4 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_4 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_4 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_4[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_4[[j]][[i]] <- norm(true_sigma_4 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_4[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_4[[j]][[i]] <- norm(true_sigma_4 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_4[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_4[[j]][[i]] <- norm(true_sigma_4 - sigma_hat_rob, type = "F")
  }
}

# # sanity check
# true_sigma 
# with(simulation_results_list[[1]][[1]]$pca, t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
# with(pca_clr_std_all[[1]][[1]], rotation %*% diag(sdev^2) %*% t(rotation))
# with(pca_clr_rob_all[[1]][[1]], loadings %*% diag(eigenvalues) %*% t(loadings))
```

```{r visualize covariances 4, eval=TRUE}
sigma_diff_df_4 <- data.frame(
  differences = c(
    unlist(sigma_distances_4[[1]]), unlist(sigma_distances_4[[2]]), 
    unlist(sigma_distances_4[[3]]), unlist(sigma_distances_4[[4]]),
    unlist(sigma_distances_std_clr_4[[1]]), unlist(sigma_distances_std_clr_4[[2]]),
    unlist(sigma_distances_std_clr_4[[3]]), unlist(sigma_distances_std_clr_4[[4]]),
    unlist(sigma_distances_rob_clr_4[[1]]), unlist(sigma_distances_rob_clr_4[[2]]),
    unlist(sigma_distances_rob_clr_4[[3]]), unlist(sigma_distances_rob_clr_4[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("30", "60", "120", "400"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_4$method <- factor(sigma_diff_df_4$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_4$size <- factor(sigma_diff_df_4$size, 
                        levels = c("30", "60", "120", "400"))

ggplot(sigma_diff_df_4, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```



## Simulation Setting 5

This is the same as Setting 1 but with the sample sizes from Steyer and Greven.

Therefore, we have only two components:

1. Contrast between component 2 and 3 with a eigenvalue of 0.6
2. Contrast between component 1 and 1 with a eigenvalue of 0.3

The mean vector is set to `c(0, 1, 0.5, -1, -0.5)`

```{r latent parameters 5, eval=TRUE}
D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues <- c(0.6, 0.3)
mean_5 <- c(0, 1, 0.5, -1, -0.5) # those are clr-coordinates

# the structure of the covariance matrix is a bit degenerated because we work with only two components
eigenvalues_5 <- c(0.6, 0.3, 0, 0)
V_5 <- cbind(c(0, 1, -1, 0, 0), c(-1, 0, 0, 1, 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_5 <- V_5 %*% diag(eigenvalues_5) %*% t(V_5) 

true_sigma_5

(true_v_5 <- mean_5)
```

```{r evaluation mean vector 5, eval=TRUE}

simulation_data_list_5 <- list(
  tar_read(sim_comp_2_smi_nSim_20),
  tar_read(sim_comp_2_smi_nSim_40),
  tar_read(sim_comp_2_smi_nSim_80),
  tar_read(sim_comp_2_smi_nSim_160)
)

pca_clr_rob_5 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_5 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_5[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_5[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_5[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_5 <- list(
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_20),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_40),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_80),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_160)
)

diff_mean_5 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_5 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_5 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_5[[j]][[i]] <- sqrt(sum((true_v_5 - t(basis_matrix) %*% simulation_results_list_5[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_5[[j]][[i]] <- sqrt(sum((true_v_5 - pca_clr_std_5[[j]][[i]]$center)^2))
    diff_mean_rob_clr_5[[j]][[i]] <- sqrt(sum((true_v_5 - pca_clr_rob_5[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_5
diff_mean_std_clr = diff_mean_std_clr_5
diff_mean_rob_clr = diff_mean_rob_clr_5

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 5, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_5 <- data.frame(
  differences = c(
    unlist(diff_mean_5[[1]]), unlist(diff_mean_5[[2]]), 
    unlist(diff_mean_5[[3]]), unlist(diff_mean_5[[4]]),
    unlist(diff_mean_std_clr_5[[1]]), unlist(diff_mean_std_clr_5[[2]]),
    unlist(diff_mean_std_clr_5[[3]]), unlist(diff_mean_std_clr_5[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_5$method <- factor(diff_df_5$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_5$size <- factor(diff_df_5$size, 
                        levels = c("20", "40", "80", "160"))

ggplot(diff_df_5, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 4") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 5, eval=TRUE}

sigma_distances_5 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_5 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_5 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_5[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_5[[j]][[i]] <- norm(true_sigma_5 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_5[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_5[[j]][[i]] <- norm(true_sigma_5 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_5[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_5[[j]][[i]] <- norm(true_sigma_5 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 5, eval=TRUE}
sigma_diff_df_5 <- data.frame(
  differences = c(
    unlist(sigma_distances_5[[1]]), unlist(sigma_distances_5[[2]]), 
    unlist(sigma_distances_5[[3]]), unlist(sigma_distances_5[[4]]),
    unlist(sigma_distances_std_clr_5[[1]]), unlist(sigma_distances_std_clr_5[[2]]),
    unlist(sigma_distances_std_clr_5[[3]]), unlist(sigma_distances_std_clr_5[[4]]),
    unlist(sigma_distances_rob_clr_5[[1]]), unlist(sigma_distances_rob_clr_5[[2]]),
    unlist(sigma_distances_rob_clr_5[[3]]), unlist(sigma_distances_rob_clr_5[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_5$method <- factor(sigma_diff_df_5$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_5$size <- factor(sigma_diff_df_5$size, 
                        levels = c("20", "40", "80", "160"))

ggplot(sigma_diff_df_5, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```



## Simulation Setting 6

This is the same as Setting 3 but with the sample sizes from Steyer and Greven.

Therefore, we have four components:

1. Contrast between component 2 and 3 with a eigenvalue of 0.5
2. Contrast between component 4 and 1,2,3 with a eigenvalue of 0.3
3. Contrast betwenn component 5 and 1 and 2 with eigenvalue 0.1
4. Contrast between component 1 and 4 with eigenvalue 0.1

The mean vector is set to `c(0, 1.5, 0.5, -1.5, -0.5)`

```{r latent parameters 6, eval=TRUE}
n_simulations <- 60

D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_6 = c(0.5, 0.3, 0.1, 0.1)
mean_6 = c(0, 1.5, 0.5, -1.5, -0.5)

# the structure of the covariance matrix is a bit degenerated because we work with only two components
V_6 <- cbind(c(0, 1/sqrt(2), -1/sqrt(2), 0, 0) , c(-1/3, -1/3, -1/3, 1, 0), c(1/2, 1/2, 0, 0, -1), c(1/sqrt(2), 0, 0, -1/sqrt(2), 0))
true_sigma_6 <- V_6 %*% diag(eigenvalues_6) %*% t(V_6) 

true_sigma_6

(true_v_6 <- mean_6)
```

```{r evaluation mean vector 6, eval=TRUE}

simulation_data_list_6 <- list(
  tar_read(sim_3_smi_nSim_20),
  tar_read(sim_3_smi_nSim_40),
  tar_read(sim_3_smi_nSim_80),
  tar_read(sim_3_smi_nSim_160)
)

pca_clr_rob_6 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_6 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_6[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_6[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_6[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_6 <- list(
  tar_read(pca_smi_nSim_3_20),
  tar_read(pca_smi_nSim_3_40),
  tar_read(pca_smi_nSim_3_80),
  tar_read(pca_smi_nSim_3_160)
)

diff_mean_6 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_6 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_6 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_6[[j]][[i]] <- sqrt(sum((true_v_6 - t(basis_matrix) %*% simulation_results_list_6[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_6[[j]][[i]] <- sqrt(sum((true_v_6 - pca_clr_std_6[[j]][[i]]$center)^2))
    diff_mean_rob_clr_6[[j]][[i]] <- sqrt(sum((true_v_6 - pca_clr_rob_6[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_6
diff_mean_std_clr = diff_mean_std_clr_6
diff_mean_rob_clr = diff_mean_rob_clr_6

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 6, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_6 <- data.frame(
  differences = c(
    unlist(diff_mean_6[[1]]), unlist(diff_mean_6[[2]]), 
    unlist(diff_mean_6[[3]]), unlist(diff_mean_6[[4]]),
    unlist(diff_mean_std_clr_6[[1]]), unlist(diff_mean_std_clr_6[[2]]),
    unlist(diff_mean_std_clr_6[[3]]), unlist(diff_mean_std_clr_6[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_6$method <- factor(diff_df_6$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_6$size <- factor(diff_df_6$size, 
                        levels = c("20", "40", "80", "160"))

ggplot(diff_df_6, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 4") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 6, eval=TRUE}

sigma_distances_6 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_6 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_6 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_6[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_6[[j]][[i]] <- norm(true_sigma_6 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_6[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_6[[j]][[i]] <- norm(true_sigma_6 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_6[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_6[[j]][[i]] <- norm(true_sigma_6 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 6, eval=TRUE}
sigma_diff_df_6 <- data.frame(
  differences = c(
    unlist(sigma_distances_6[[1]]), unlist(sigma_distances_6[[2]]), 
    unlist(sigma_distances_6[[3]]), unlist(sigma_distances_6[[4]]),
    unlist(sigma_distances_std_clr_6[[1]]), unlist(sigma_distances_std_clr_6[[2]]),
    unlist(sigma_distances_std_clr_6[[3]]), unlist(sigma_distances_std_clr_6[[4]]),
    unlist(sigma_distances_rob_clr_6[[1]]), unlist(sigma_distances_rob_clr_6[[2]]),
    unlist(sigma_distances_rob_clr_6[[3]]), unlist(sigma_distances_rob_clr_6[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_6$method <- factor(sigma_diff_df_6$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_6$size <- factor(sigma_diff_df_6$size, 
                        levels = c("20", "40", "80", "160"))

ggplot(sigma_diff_df_6, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```




## Simulation Setting 7

This is the same  the more complex setting which builds on the structure of the kl15 data.



The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 7, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_7 = c(0.5, 0.2, 0.12, 0.08)
mean_7 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)

V_7 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_7 <- V_7 %*% diag(eigenvalues_7) %*% t(V_7) 

true_sigma_7

(true_v_7 <- mean_7)
```

```{r evaluation mean vector 7, eval=TRUE}

simulation_data_list_7 <- list(
  tar_read(sim_5_0001),
  tar_read(sim_5_001),
  tar_read(sim_5_01),
  tar_read(sim_5_05)
)

pca_clr_rob_7 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_7 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_7[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_7[[j]][[i]] <- prcomp(clr(x_data))
    # pca_clr_rob_7[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_7 <- list(
  tar_read(pca_5_0001),
  tar_read(pca_5_001),
  tar_read(pca_5_01),
  tar_read(pca_5_05)
)

diff_mean_7 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_7 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_7 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_7[[j]][[i]] <- sqrt(sum((true_v_7 - t(basis_matrix) %*% simulation_results_list_7[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_7[[j]][[i]] <- sqrt(sum((true_v_7 - pca_clr_std_7[[j]][[i]]$center)^2))
    # diff_mean_rob_clr_7[[j]][[i]] <- sqrt(sum((true_v_7 - pca_clr_rob_7[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_7
diff_mean_std_clr = diff_mean_std_clr_7
# diff_mean_rob_clr = diff_mean_rob_clr_7

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 7, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_7 <- data.frame(
  differences = c(
    unlist(diff_mean_7[[1]]), unlist(diff_mean_7[[2]]), 
    unlist(diff_mean_7[[3]]), unlist(diff_mean_7[[4]]),
    unlist(diff_mean_std_clr_7[[1]]), unlist(diff_mean_std_clr_7[[2]]),
    unlist(diff_mean_std_clr_7[[3]]), unlist(diff_mean_std_clr_7[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("0.0001", "0.001", "0.01", "0.05"), 
                         each = n_simulations), 2))
)

# define order of sequence
# diff_df_7$method <- factor(diff_df_7$method, 
#                         levels = c("MCEM", "sample mean"))

# diff_df_7$size <- factor(diff_df_7$size, 
#                         levels = c("0.001", "0.001", "0.01", "0.05"))

ggplot(diff_df_7, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 4") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 7, eval=TRUE}

sigma_distances_7 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_7 <- lapply(1:4, function(x) list(n_simulations))
# sigma_distances_rob_clr_7 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_7[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_7[[j]][[i]] <- norm(true_sigma_7 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_7[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_7[[j]][[i]] <- norm(true_sigma_7 - sigma_hat_clr, type = "F")
    
    # ILR method
    # sigma_hat_rob <- with(pca_clr_rob_7[[j]][[i]],
    # loadings %*% diag(eigenvalues) %*% t(loadings))
    # sigma_distances_rob_clr_7[[j]][[i]] <- norm(true_sigma_7 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 7, eval=TRUE}
sigma_diff_df_7 <- data.frame(
  differences = c(
    unlist(sigma_distances_7[[1]]), unlist(sigma_distances_7[[2]]), 
    unlist(sigma_distances_7[[3]]), unlist(sigma_distances_7[[4]]),
    unlist(sigma_distances_std_clr_7[[1]]), unlist(sigma_distances_std_clr_7[[2]]),
    unlist(sigma_distances_std_clr_7[[3]]), unlist(sigma_distances_std_clr_7[[4]])
    # unlist(sigma_distances_rob_clr_7[[1]]), unlist(sigma_distances_rob_clr_7[[2]]),
    # unlist(sigma_distances_rob_clr_7[[3]]), unlist(sigma_distances_rob_clr_7[[4]])
  ),
  method = factor(rep(c("MCEM", "standard"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("0.0001", "0.001", "0.01", "0.05"), 
                         each = n_simulations), 4))
)

# define order of sequence
sigma_diff_df_7$method <- factor(sigma_diff_df_7$method, 
                        levels = c("MCEM", "standard"))

sigma_diff_df_7$size <- factor(sigma_diff_df_7$size, 
                        levels = c("0.0001", "0.001", "0.01", "0.05"))

ggplot(sigma_diff_df_7, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```


## Simulation Setting 8

This is the same setting as number 7 but with a diverging number of observations (versus 2119) 
the more complex setting which builds on the structure of the kl15 data.

**The main deviance to the other settings is the sample size $m_i$**:
Is is sampled from a density distribution with:
```{r density distribution, eval=TRUE}
data <- tar_read(data_kl15)
data <- data * 0.01
density_estimate <- density(data$aggregate)
plot(density_estimate)
```

The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 8, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_8 = c(0.5, 0.2, 0.12, 0.08)
mean_8 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_8 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_8 <- V_8 %*% diag(eigenvalues_8) %*% t(V_8) 

true_sigma_8

(true_v_8 <- mean_8)
```

```{r evaluation mean vector 8, eval=TRUE}

simulation_data_list_8 <- list(
  tar_read(sim_6_01_n100),
  tar_read(sim_6_01_n200),
  tar_read(sim_6_01_n400),
  tar_read(sim_6_01_n800)
)

pca_clr_rob_8 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_8 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_8[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_8[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_8[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_8 <- list(
  tar_read(pca_6_01_n100),
  tar_read(pca_6_01_n200),
  tar_read(pca_6_01_n400),
  tar_read(pca_6_01_n800)
)

diff_mean_8 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_8 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_8 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_8[[j]][[i]] <- sqrt(sum((true_v_8 - t(basis_matrix) %*% simulation_results_list_8[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_8[[j]][[i]] <- sqrt(sum((true_v_8 - pca_clr_std_8[[j]][[i]]$center)^2))
    # diff_mean_rob_clr_8[[j]][[i]] <- sqrt(sum((true_v_8 - pca_clr_rob_8[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_8
diff_mean_std_clr = diff_mean_std_clr_8
# diff_mean_rob_clr = diff_mean_rob_clr_8

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 8, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_8 <- data.frame(
  differences = c(
    unlist(diff_mean_8[[1]]), unlist(diff_mean_8[[2]]), 
    unlist(diff_mean_8[[3]]), unlist(diff_mean_8[[4]]),
    unlist(diff_mean_std_clr_8[[1]]), unlist(diff_mean_std_clr_8[[2]]),
    unlist(diff_mean_std_clr_8[[3]]), unlist(diff_mean_std_clr_8[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("100", "200", "400", "800"), 
                         each = n_simulations), 2))
)

diff_df_8$method <- factor(diff_df_8$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_8$size <- factor(diff_df_8$size, 
                        levels = c("100", "200", "400", "800"))

ggplot(diff_df_8, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Observations",
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 6") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 8, eval=TRUE}

sigma_distances_8 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_8 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_8 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_8[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_mcem, type = "F")
    
    sigma_hat_clr <- with(pca_clr_std_8[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_8[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_8[[j]][[i]] <- norm(true_sigma_8 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 8, eval=TRUE}
sigma_diff_df_8 <- data.frame(
  differences = c(
    unlist(sigma_distances_8[[1]]), unlist(sigma_distances_8[[2]]), 
    unlist(sigma_distances_8[[3]]), unlist(sigma_distances_8[[4]]),
    # unlist(sigma_distances_std_clr_8[[1]]), unlist(sigma_distances_std_clr_8[[2]]),
    # unlist(sigma_distances_std_clr_8[[3]]), unlist(sigma_distances_std_clr_8[[4]]),
    unlist(sigma_distances_rob_clr_8[[1]]), unlist(sigma_distances_rob_clr_8[[2]]),
    unlist(sigma_distances_rob_clr_8[[3]]), unlist(sigma_distances_rob_clr_8[[4]])
  ),
  method = factor(rep(c("MCEM", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("100", "200", "400", "800"), 
                         each = n_simulations), 2))
)

# define order of sequence
sigma_diff_df_8$method <- factor(sigma_diff_df_8$method, 
                        levels = c("MCEM", "robust"))

sigma_diff_df_8$size <- factor(sigma_diff_df_8$size, 
                        levels = c("100", "200", "400", "800"))

ggplot(sigma_diff_df_8, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Observations",
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```


## Simulation Setting 9

Again, this is the same setting as in Setting 2 but the variation is in the tuning parameter **lambda**:

  pc_1 <- c(0, 1, -1, 0, 0)  # Contrast between parts 2 and 3
  pc_2 <- c(-1, 0, 0, 1, 0)  # Contrast between parts 1 and 4

  lambda_1 <- eigenvalues[1]
  lambda_2 <- eigenvalues[2]

  
  clr_coords <- lapply(1:n_observations, function(i){
    mean + rnorm(1, 0, lambda_1)*pc_1 + rnorm(1, 0, lambda_2)*pc_2
  })

```{r latent parameters 9, eval=TRUE}
D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues <- c(0.6, 0.3)
mean_9 <- c(0, 1, 0.5, -1, -0.5) # those are clr-coordinates

# the structure of the covariance matrix is a bit degenerated because we work with only two components
eigenvalues_9 <- c(0.6, 0.3, 0, 0)
V_9 <- cbind(c(0, 1, -1, 0, 0), c(-1, 0, 0, 1, 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_9 <- V_9 %*% diag(eigenvalues_9) %*% t(V_9) 

true_sigma_9

(true_v_9 <- mean_9)
```



1. 20 Simulationen mit 
  - mean vector: c(0, 1, 0.5, -1, -0.5)
  - eigenvalues: c(0.6, 0.3)
  - n_observations: c(100, 100, 100, 300)
  - n_components: 5
  - m_i: c(30, 60, 90, 90)




```{r evaluation mean vector 9, eval=TRUE}

simulation_data_list_9 <- list(
  tar_read(sim_comp_2_smi_nSim_80)
)

simulation_results_list_9 <- list(
  tar_read(pca_sim1_2_80_l06),
  tar_read(pca_sim1_2_80_l08),
  tar_read(pca_sim1_ilr_StdPara_smi_nSim_2_80),
  tar_read(pca_sim1_2_80_l12),
  tar_read(pca_sim1_2_80_l14)
)

diff_mean_9 <- replicate(5, list(n_simulations), simplify = FALSE)

for(j in 1:5) {
  for(i in 1:n_simulations) {
    diff_mean_9[[j]][[i]] <- sqrt(sum((true_v_9 - t(basis_matrix) %*% simulation_results_list_9[[j]][[i]]$pca$center)^2))
  }
}

diff_mean = diff_mean_9
```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 9, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_9 <- data.frame(
  differences = c(
    unlist(diff_mean_9[[1]]), unlist(diff_mean_9[[2]]), 
    unlist(diff_mean_9[[3]]), unlist(diff_mean_9[[4]]),
    unlist(diff_mean_9[[5]])
    # unlist(diff_mean_std_clr_2[[1]]), unlist(diff_mean_std_clr_2[[2]]),
    # unlist(diff_mean_std_clr_2[[3]]), unlist(diff_mean_std_clr_2[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM"), each = 5 * n_simulations)),
  size = factor(rep(rep(c("0.6", "0.8", "base", "1.2", "1.4"), 
                         each = n_simulations), 1))
)

diff_df_9$size <- factor(diff_df_9$size, 
                        levels = c("0.6", "0.8", "base", "1.2", "1.4"))

ggplot(diff_df_9, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 9, eval=TRUE}
sigma_distances_9 <- replicate(5, list(n_simulations), simplify = FALSE)

for(j in 1:5) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_9[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_9[[j]][[i]] <- norm(true_sigma_9 - sigma_hat_mcem, type = "F")
  }
}

sigma_diff_df_9 <- data.frame(
  differences = c(
    unlist(sigma_distances_9[[1]]), unlist(sigma_distances_9[[2]]), 
    unlist(sigma_distances_9[[3]]), unlist(sigma_distances_9[[4]]),
    unlist(sigma_distances_9[[5]])
  ),
  method = factor(rep(c("MCEM"), each = 5 * n_simulations)),
  size = factor(rep(rep(c("0.6", "0.8", "base", "1.2", "1.4"), 
                         each = n_simulations), 1))
)

sigma_diff_df_9$size <- factor(sigma_diff_df_9$size, 
                        levels = c("0.6", "0.8", "base", "1.2", "1.4"))

ggplot(sigma_diff_df_9, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```


## Simulation Setting 10

This is the same setting as number 8 but using the original scale of the kl15 dataset.

**The main deviance to the other settings is the sample size $m_i$**:
Is is sampled from a density distribution with:
```{r density distribution 10, eval=TRUE}
data <- tar_read(data_kl15)
density_estimate <- density(data$aggregate)
plot(density_estimate)
```

The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 10, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_10 = c(0.5, 0.2, 0.12, 0.08)
mean_10 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_10 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_10 <- V_10 %*% diag(eigenvalues_10) %*% t(V_10) 

true_sigma_10

(true_v_10 <- mean_10)
```

```{r evaluation mean vector 10, eval=TRUE}

simulation_data_list_10 <- list(
  tar_read(sim_6_sc1_n200),
  tar_read(sim_6_sc1_n400),
  tar_read(sim_6_sc1_n800),
  tar_read(sim_6_sc1_n1600)
)

pca_clr_rob_10 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_10 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_10[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_10[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_10[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_10 <- list(
  tar_read(pca_6_sc1_n200),
  tar_read(pca_6_sc1_n400),
  tar_read(pca_6_sc1_n800),
  tar_read(pca_6_sc1_n1600)
)

diff_mean_10 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_10 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_10 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_10[[j]][[i]] <- sqrt(sum((true_v_10 - t(basis_matrix) %*% simulation_results_list_10[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_10[[j]][[i]] <- sqrt(sum((true_v_10 - pca_clr_std_10[[j]][[i]]$center)^2))
    # diff_mean_rob_clr_10[[j]][[i]] <- sqrt(sum((true_v_10 - pca_clr_rob_10[[j]][[i]]$center)^2))
  }
}
diff_mean = diff_mean_10
diff_mean_std_clr = diff_mean_std_clr_10
# diff_mean_rob_clr = diff_mean_rob_clr_10

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 10, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_10 <- data.frame(
  differences = c(
    unlist(diff_mean_10[[1]]), unlist(diff_mean_10[[2]]), 
    unlist(diff_mean_10[[3]]), unlist(diff_mean_10[[4]]),
    unlist(diff_mean_std_clr_10[[1]]), unlist(diff_mean_std_clr_10[[2]]),
    unlist(diff_mean_std_clr_10[[3]]), unlist(diff_mean_std_clr_10[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("200", "400", "800", "1600"), 
                         each = n_simulations), 2))
)

diff_df_10$method <- factor(diff_df_10$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_10$size <- factor(diff_df_10$size, 
                        levels = c("200", "400", "800", "1600"))

ggplot(diff_df_10, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Observations",
       fill = "method",
       title = "Distance of true mean to estimated mean: Setting 6") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 10, eval=TRUE}

sigma_distances_10 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_10 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_10 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_10[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_10[[j]][[i]] <- norm(true_sigma_10 - sigma_hat_mcem, type = "F")
    
    sigma_hat_clr <- with(pca_clr_std_10[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_10[[j]][[i]] <- norm(true_sigma_10 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_10[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_10[[j]][[i]] <- norm(true_sigma_10 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 10, eval=TRUE}
sigma_diff_df_10 <- data.frame(
  differences = c(
    unlist(sigma_distances_10[[1]]), unlist(sigma_distances_10[[2]]), 
    unlist(sigma_distances_10[[3]]), unlist(sigma_distances_10[[4]]),
    unlist(sigma_distances_std_clr_10[[1]]), unlist(sigma_distances_std_clr_10[[2]]),
    unlist(sigma_distances_std_clr_10[[3]]), unlist(sigma_distances_std_clr_10[[4]]),
    unlist(sigma_distances_rob_clr_10[[1]]), unlist(sigma_distances_rob_clr_10[[2]]),
    unlist(sigma_distances_rob_clr_10[[3]]), unlist(sigma_distances_rob_clr_10[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("200", "400", "800", "1600"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_10$method <- factor(sigma_diff_df_10$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_10$size <- factor(sigma_diff_df_10$size, 
                        levels = c("200", "400", "800", "1600"))

ggplot(sigma_diff_df_10, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Observations",
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

## Simulation Setting 11

This is a comparison of different levels of scaled data for the complex setting.

That means the main difference is the scaling of the data, with the following levels:

- 1
- 0.1
- 0.01
- 0.001

The convergence criterium is set to 0.5

The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 11, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_11 = c(0.5, 0.2, 0.12, 0.08)
mean_11 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_11 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_11 <- V_11 %*% diag(eigenvalues_11) %*% t(V_11) 

true_sigma_11

(true_v_11 <- mean_11)
```

```{r evaluation mean vector 11, eval=TRUE}

simulation_data_list_11 <- list(
  tar_read(sim_6_sc1_n2000)
  )

pca_clr_rob_11 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
simulation_data <- simulation_data_list_11[[1]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    x_data_01 <- x_data * 0.1
    x_data_001 <- x_data * 0.01
    x_data_0001 <- x_data * 0.001

    pca_clr_rob_11[[1]][[i]] <- pcaCoDa(x_data, method = "robust")
    pca_clr_rob_11[[2]][[i]] <- pcaCoDa(x_data_01, method = "robust")
    pca_clr_rob_11[[3]][[i]] <- pcaCoDa(x_data_001, method = "robust")
    pca_clr_rob_11[[4]][[i]] <- pcaCoDa(x_data_0001, method = "robust")


  }


simulation_results_list_11 <- list(
  tar_read(pca_6_sc1_n2000),
  tar_read(pca_6_sc01_n2000),
  tar_read(pca_6_sc001_n2000),
  tar_read(pca_6_sc0001_n2000)
)

diff_mean_11 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_11 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_11[[j]][[i]] <- sqrt(sum((true_v_11 - t(basis_matrix) %*% simulation_results_list_11[[j]][[i]]$pca$center)^2))
    diff_mean_rob_clr_11[[j]][[i]] <- sqrt(sum((true_v_11 - pca_clr_rob_11[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_11
diff_mean_rob_clr = diff_mean_rob_clr_11

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 11, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_11 <- data.frame(
  differences = c(
    unlist(diff_mean_11[[1]]), unlist(diff_mean_11[[2]]), 
    unlist(diff_mean_11[[3]]), unlist(diff_mean_11[[4]]),
    # unlist(diff_mean_std_clr_10[[1]]), unlist(diff_mean_std_clr_10[[2]]),
    # unlist(diff_mean_std_clr_10[[3]]), unlist(diff_mean_std_clr_10[[4]])
    unlist(diff_mean_rob_clr_11[[1]]), unlist(diff_mean_rob_clr_11[[2]]),
    unlist(diff_mean_rob_clr_11[[3]]), unlist(diff_mean_rob_clr_11[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01", "0.001"), 
                         each = n_simulations), 2))
)

diff_df_11$method <- factor(diff_df_11$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_11$size <- factor(diff_df_11$size, 
                        levels = c("original", "0.1", "0.01", "0.001"))

ggplot(diff_df_11, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Scale",
       fill = "method",
       title = "Distance of true mean to estimated mean (n = 2000)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 11, eval=TRUE}

sigma_distances_11 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_11 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_11[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_11[[j]][[i]] <- norm(true_sigma_11 - sigma_hat_mcem, type = "F")
    
    # sigma_hat_clr <- with(pca_clr_std_10[[j]][[i]],
    #   rotation %*% diag(sdev^2) %*% t(rotation))
    # sigma_distances_std_clr_10[[j]][[i]] <- norm(true_sigma_10 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_11[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_11[[j]][[i]] <- norm(true_sigma_11 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 11, eval=TRUE}
sigma_diff_df_11 <- data.frame(
  differences = c(
    unlist(sigma_distances_11[[1]]), unlist(sigma_distances_11[[2]]), 
    unlist(sigma_distances_11[[3]]), unlist(sigma_distances_11[[4]]),
    # unlist(sigma_distances_std_clr_11[[1]]), unlist(sigma_distances_std_clr_11[[2]]),
    # unlist(sigma_distances_std_clr_11[[3]]), unlist(sigma_distances_std_clr_11[[4]]),
    unlist(sigma_distances_rob_clr_11[[1]]), unlist(sigma_distances_rob_clr_11[[2]]),
    unlist(sigma_distances_rob_clr_11[[3]]), unlist(sigma_distances_rob_clr_11[[4]])
  ),
  method = factor(rep(c("MCEM", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01", "0.001"), 
                         each = n_simulations), 2))
)

# define order of sequence
sigma_diff_df_11$method <- factor(sigma_diff_df_11$method, 
                        levels = c("MCEM", "robust"))

sigma_diff_df_11$size <- factor(sigma_diff_df_11$size, 
                        levels = c("original", "0.1", "0.01", "0.001"))

ggplot(sigma_diff_df_11, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Scale",
       fill = "method",
       title = "Distance of true covariance to estimated covariance (n = 2000)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

## Simulation Setting 12

This is a comparison of different levels of scaled data for the complex setting with low observation numbers (n=100).

That means the main difference is the scaling of the data, with the following levels:

- 1
- 0.1
- 0.01
- 0.001

The convergence criterium is set to 0.5

The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 12, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_12 = c(0.5, 0.2, 0.12, 0.08)
mean_12 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_12 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_12 <- V_12 %*% diag(eigenvalues_12) %*% t(V_12) 

true_sigma_12

(true_v_12 <- mean_12)
```

```{r evaluation mean vector 12, eval=TRUE}

simulation_data_list_12 <- list(
  tar_read(sim_6_sc1_n100)
  )

pca_clr_std_12 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_rob_12 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
simulation_data <- simulation_data_list_12[[1]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    x_data_01 <- x_data * 0.1
    x_data_001 <- x_data * 0.01
    x_data_0001 <- x_data * 0.001

    pca_clr_rob_12[[1]][[i]] <- pcaCoDa(x_data, method = "robust")
    pca_clr_rob_12[[2]][[i]] <- pcaCoDa(x_data_01, method = "robust")
    pca_clr_rob_12[[3]][[i]] <- pcaCoDa(x_data_001, method = "robust")
    pca_clr_rob_12[[4]][[i]] <- pcaCoDa(x_data_0001, method = "robust")    
    pca_clr_std_12[[1]][[i]] <- princomp(clr(x_data))
    pca_clr_std_12[[2]][[i]] <- princomp(clr(x_data_01))
    pca_clr_std_12[[3]][[i]] <- princomp(clr(x_data_001))
    pca_clr_std_12[[4]][[i]] <- princomp(clr(x_data_0001))


  }


simulation_results_list_12 <- list(
  tar_read(pca_6_sc1_n100),
  tar_read(pca_6_sc01_n100),
  tar_read(pca_6_sc001_n100),
  tar_read(pca_6_sc0001_n100)
)

diff_mean_12 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_12 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_12[[j]][[i]] <- sqrt(sum((true_v_12 - t(basis_matrix) %*% simulation_results_list_12[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_12[[j]][[i]] <- sqrt(sum((true_v_12 - pca_clr_std_12[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_12
diff_mean_std_clr = diff_mean_std_clr_12

for (j in 1:4) {
    for (i in 1:n_simulations) {
        print(paste("j =", j, "i =", i))
        print(paste("Time:", simulation_results_list_12[[j]][[i]]$time))
        print(paste("Iteration:", simulation_results_list_12[[j]][[i]]$iteration))
        print("-------------------")
    }
}

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 12, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_12 <- data.frame(
  differences = c(
    unlist(diff_mean_12[[1]]), unlist(diff_mean_12[[2]]), 
    unlist(diff_mean_12[[3]]), unlist(diff_mean_12[[4]]),
    unlist(diff_mean_std_clr_12[[1]]), unlist(diff_mean_std_clr_12[[2]]),
    unlist(diff_mean_std_clr_12[[3]]), unlist(diff_mean_std_clr_12[[4]])
    # unlist(diff_mean_rob_clr_12[[1]]), unlist(diff_mean_rob_clr_12[[2]]),
    # unlist(diff_mean_rob_clr_12[[3]]), unlist(diff_mean_rob_clr_12[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01", "0.001"), 
                         each = n_simulations), 2))
)

diff_df_12$method <- factor(diff_df_12$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_12$size <- factor(diff_df_12$size, 
                        levels = c("original", "0.1", "0.01", "0.001"))

ggplot(diff_df_12, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Scale",
       fill = "method",
       title = "Distance of true mean to estimated mean (n = 100)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

And for the Kovarianzmatrix:

```{r evaluation covariance 12, eval=TRUE}

sigma_distances_12 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_12 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_12[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_12[[j]][[i]] <- norm(true_sigma_12 - sigma_hat_mcem, type = "F")
    
    # sigma_hat_clr <- with(pca_clr_std_10[[j]][[i]],
    #   rotation %*% diag(sdev^2) %*% t(rotation))
    # sigma_distances_std_clr_10[[j]][[i]] <- norm(true_sigma_10 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_12[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_12[[j]][[i]] <- norm(true_sigma_12 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 12, eval=TRUE}
sigma_diff_df_12 <- data.frame(
  differences = c(
    unlist(sigma_distances_12[[1]]), unlist(sigma_distances_12[[2]]), 
    unlist(sigma_distances_12[[3]]), unlist(sigma_distances_12[[4]]),
    # unlist(sigma_distances_std_clr_12[[1]]), unlist(sigma_distances_std_clr_12[[2]]),
    # unlist(sigma_distances_std_clr_12[[3]]), unlist(sigma_distances_std_clr_12[[4]]),
    unlist(sigma_distances_rob_clr_12[[1]]), unlist(sigma_distances_rob_clr_12[[2]]),
    unlist(sigma_distances_rob_clr_12[[3]]), unlist(sigma_distances_rob_clr_12[[4]])
  ),
  method = factor(rep(c("MCEM", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01", "0.001"), 
                         each = n_simulations), 2))
)

# define order of sequence
sigma_diff_df_12$method <- factor(sigma_diff_df_12$method, 
                        levels = c("MCEM", "robust"))

sigma_diff_df_12$size <- factor(sigma_diff_df_12$size, 
                        levels = c("original", "0.1", "0.01", "0.001"))

ggplot(sigma_diff_df_12, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Scale",
       fill = "method",
       title = "Distance of true covariance to estimated covariance (n = 100)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

## Simulation Setting 13

Same as Setting 12 but the scaling is done in the simulation resulting in **true count compositions** trough rescaling of $m_i$ instead of artifical rescaling.
This is a comparison of different levels of scaled data for the complex setting with low observation numbers (n=100).

That means the main difference is the scaling of the data, with the following levels:

- 1
- 0.1
- 0.01
- 0.001

The convergence criterium is set to 0.5

The mean vector is set to `c(-1.257, -1.741, 1.713, -0.6, 1.07, -3.624, -1.254, 1.05, -0.9, 0.505, 4.0, -0.49)` 
and there are 4 components. 

```{r latent parameters 13, eval=TRUE}
n_simulations <- 20

D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

eigenvalues_13 = c(0.5, 0.2, 0.12, 0.08)
mean_13 = c(-1.257, -1.741, 1.713, -0.6,
             1.07, -3.624, -1.254, 1.05, -0.9,
             0.505, 4.0, -0.49, 1.532)

pc_1 <- c(0.7, 0, 0, -0.2, 0, 0, -0.2, -0.2, 0.4, -0.1, 0, -0.2, -0.2) / sqrt(0.7^2 + 3*0.2^2 + 0.4^2 + 0.1^2)
pc_2 <- c(0.5, 0, -0.4, 0.4, 0, 0, 0, 0, 0, 0, -0.5, 0, 0) / sqrt(0.5^2 + 0.4^2 + 0.4^2 + 0.5^2)
pc_3 <- c(0, 0, 0.3, 0, 0.4, 0, 0, 0, -0.7, 0, 0, 0, 0) / sqrt(0.3^2 + 0.4^2 + 0.7^2)
pc_4 <- c(0.3, -0.2, 0, -0.3, 0.6, 0.3, 0, 0, -0.4, 0, 0, 0, -0.3)  / sqrt(0.3^2 + 0.2^2 + 0.3^2 + 0.6^2 + 0.3^2 + 0.4^2 + 0.3^2)

V_13 <- cbind(pc_1, pc_2, pc_3, pc_4)
true_sigma_13 <- V_13 %*% diag(eigenvalues_13) %*% t(V_13) 

true_sigma_13

(true_v_13 <- mean_13)
```

```{r evaluation mean vector 13, eval=TRUE}

simulation_data_list_13 <- list(
  tar_read(sim_6_sc1_n100),
  tar_read(sim_6_sc01_n100),
  tar_read(sim_6_sc001_n100),
  tar_read(sim_6_01_n100)  
  )

# calculate standard PCA for each simulation
pca_clr_rob_13 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_13 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_13[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_13[[j]][[i]] <- prcomp(clr(x_data))
    pca_clr_rob_13[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_13 <- list(
  tar_read(pca_6_sc1_n100),
  tar_read(pca_6_sc01_n100_vs2,),
  tar_read(pca_6_sc001_n100_vs2),
  tar_read(pca_6_sc001_n100_vs2)
)

diff_mean_13 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_13 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_13[[j]][[i]] <- sqrt(sum((true_v_13 - t(basis_matrix) %*% simulation_results_list_13[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_13[[j]][[i]] <- sqrt(sum((true_v_13 - pca_clr_std_13[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_13
diff_mean_rob_clr = diff_mean_std_clr_13

```

Let's visualize the results with boxplots for each case.

```{r boxplot simulation 13, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_13 <- data.frame(
  differences = c(
    unlist(diff_mean_13[[1]]), unlist(diff_mean_13[[2]]), 
    unlist(diff_mean_13[[3]]),
    unlist(diff_mean_std_clr_13[[1]]), unlist(diff_mean_std_clr_13[[2]]),
    unlist(diff_mean_std_clr_13[[3]])
    # unlist(diff_mean_rob_clr_13[[1]]), unlist(diff_mean_rob_clr_13[[2]]),
    # unlist(diff_mean_rob_clr_13[[3]]), unlist(diff_mean_rob_clr_13[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 3 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01"), 
                         each = n_simulations), 2))
)

diff_df_13$method <- factor(diff_df_13$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_13$size <- factor(diff_df_13$size, 
                        levels = c("original", "0.1", "0.01"))

ggplot(diff_df_13, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = "Scale",
       fill = "method",
       title = "Distance of true mean to estimated mean (n = 100)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

TODO: What does a squared difference of 1.0 mean for a 13 dimensional vector?

And for the Kovarianzmatrix:

```{r evaluation covariance 13, eval=TRUE}
sigma_distances_13 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_13 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_13 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    sigma_hat_mcem <- with(simulation_results_list_13[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_13[[j]][[i]] <- norm(true_sigma_13 - sigma_hat_mcem, type = "F")
    
    sigma_hat_clr <- with(pca_clr_std_13[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_13[[j]][[i]] <- norm(true_sigma_13 - sigma_hat_clr, type = "F")
    
    sigma_hat_rob <- with(pca_clr_rob_13[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_13[[j]][[i]] <- norm(true_sigma_13 - sigma_hat_rob, type = "F")
  }
}

```

```{r visualize covariances 13, eval=TRUE}
sigma_diff_df_13 <- data.frame(
  differences = c(
    unlist(sigma_distances_13[[1]]), unlist(sigma_distances_13[[2]]), 
    unlist(sigma_distances_13[[3]]),
    # unlist(sigma_distances_std_clr_13[[1]]), unlist(sigma_distances_std_clr_13[[2]]),
    # unlist(sigma_distances_std_clr_13[[3]]), unlist(sigma_distances_std_clr_13[[4]]),
    unlist(sigma_distances_rob_clr_13[[1]]), unlist(sigma_distances_rob_clr_13[[2]]),
    unlist(sigma_distances_rob_clr_13[[3]])
  ),
  method = factor(rep(c("MCEM", "robust"), each = 3 * n_simulations)),
  size = factor(rep(rep(c("original", "0.1", "0.01"), 
                         each = n_simulations), 2))
)

# define order of sequence
sigma_diff_df_13$method <- factor(sigma_diff_df_13$method, 
                        levels = c("MCEM", "robust"))

sigma_diff_df_13$size <- factor(sigma_diff_df_13$size, 
                        levels = c("original", "0.1", "0.01"))

ggplot(sigma_diff_df_13, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = "Scale",
       fill = "method",
       title = "Distance of true covariance to estimated covariance (n = 100)") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))
```

## Simulation Setting 14

This setting is the same as the base setting with 5 dimensions and 2 components. It uses a **balanced**
setting with a clr-mean around zero for every component. This should result in less zeros in the samples.


```{r latent parameters 14, eval=TRUE}

D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

mean_14 <- c(0, 0.05, 0.1, -0.05, -0.1)

eigenvalues_14 <- c(0.6, 0.3, 0, 0)
V_14 <- cbind(c(0, sqrt(1/2), -sqrt(1/2), 0, 0), c(-sqrt(1/2), 0, 0, sqrt(1/2), 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_14 <- V_14 %*% diag(eigenvalues_14) %*% t(V_14) 

true_v_14 <- mean_14
```



```{r evaluation mean vector 14, eval=TRUE}

simulation_data_list_14 <- list(
  tar_read(sim_comp_2_smi_nSim_eb_20),
  tar_read(sim_comp_2_smi_nSim_eb_40),
  tar_read(sim_comp_2_smi_nSim_eb_80),
  tar_read(sim_comp_2_smi_nSim_eb_160)
)

n_simulations <- length(simulation_data_list_14[[1]])

simulation_data_list_14_pi <- list(
  tar_read(sim_comp_2_smi_nSim_eb_20_pi),
  tar_read(sim_comp_2_smi_nSim_eb_40_pi),
  tar_read(sim_comp_2_smi_nSim_eb_80_pi),
  tar_read(sim_comp_2_smi_nSim_eb_160_pi)
)

oracle_mean <- lapply(1:4, function(x) list(n_simulations))
oracle_covar <- lapply(1:4, function(x) list(n_simulations))

# calculate oracle estimates of the mean and the covariance given the simulated "true" compositions
for(j in 1:4) {
  simulation_data <- simulation_data_list_14_pi[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    comp_list <- sim$composition_list
    comp_data <- do.call(rbind, comp_list)
    
    oracle_mean[[j]][[i]] <- colMeans(comp_data)
    oracle_covar[[j]][[i]] <- cov(comp_data)
  }
}



pca_clr_rob_14 <- lapply(1:4, function(x) list(n_simulations))
pca_clr_std_14 <- lapply(1:4, function(x) list(n_simulations))

# calculate standard PCA for each simulation
for(j in 1:4) {
  simulation_data <- simulation_data_list_14[[j]]
  for(i in 1:length(simulation_data)) {
    sim <- simulation_data[[i]]
    x_data_list <- sim$x_data
    x_data <- do.call(rbind, x_data_list)
    
    pca_clr_std_14[[j]][[i]] <- prcomp(clr_transform(x_data, replace_zeros = "neutral"))
    pca_clr_rob_14[[j]][[i]] <- pcaCoDa(x_data, method = "robust")
  }
}

simulation_results_list_14 <- list(
  tar_read(pca_sim1_2_20_eb),
  tar_read(pca_sim1_2_40_eb),
  tar_read(pca_sim1_2_80_eb),
  tar_read(pca_sim1_2_160_eb)
)

diff_mean_14 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_std_clr_14 <- lapply(1:4, function(x) list(n_simulations))
diff_mean_rob_clr_14 <- lapply(1:4, function(x) list(n_simulations))

diff_oracle_mean_14 <- lapply(1:4, function(x) list(n_simulations))
diff_oracle_mean_std_clr_14 <- lapply(1:4, function(x) list(n_simulations))
diff_oracle_mean_rob_clr_14 <- lapply(1:4, function(x) list(n_simulations))

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_mean_14[[j]][[i]] <- sqrt(sum((true_v_14 - t(basis_matrix) %*% simulation_results_list_14[[j]][[i]]$pca$center)^2))
    diff_mean_std_clr_14[[j]][[i]] <- sqrt(sum((true_v_14 - pca_clr_std_14[[j]][[i]]$center)^2))
    diff_mean_rob_clr_14[[j]][[i]] <- sqrt(sum((true_v_14 - pca_clr_rob_14[[j]][[i]]$center)^2))
  }
}

for(j in 1:4) {
  for(i in 1:n_simulations) {
    diff_oracle_mean_14[[j]][[i]] <- sqrt(sum((true_v_14 - t(basis_matrix) %*% simulation_results_list_14[[j]][[i]]$pca$center)^2))
    diff_oracle_mean_std_clr_14[[j]][[i]] <- sqrt(sum((true_v_14 - pca_clr_std_14[[j]][[i]]$center)^2))
    diff_oracle_mean_rob_clr_14[[j]][[i]] <- sqrt(sum((true_v_14 - pca_clr_rob_14[[j]][[i]]$center)^2))
  }
}

diff_mean = diff_mean_14
diff_mean_std_clr = diff_mean_std_clr_14
diff_mean_rob_clr = diff_mean_rob_clr_14

scenario_stats <- list()

for(j in 1:4) {
  simulation_data <- simulation_data_list_2[[j]]
  # Sammle Statistiken aller Simulationen für Szenario j
  all_stats <- lapply(simulation_data, function(sim) {
    x_data <- do.call(rbind, sim$x_data)
    summary_stats(x_data)
  })
  
  # Berechne Durchschnitt über alle Simulationen
  avg_stats <- Reduce('+', all_stats) / length(all_stats)
  
  # Speichere Ergebnis
  scenario_stats[[j]] <- avg_stats
  
  # Ausgabe
  print(paste("Average Statistics for Scenario", j))
  print(avg_stats)
}
```

```{r boxplot simulation 14, eval=TRUE}
# Daten in einen DataFrame zusammenführen
diff_df_14 <- data.frame(
  differences = c(
    unlist(diff_mean_14[[1]]), unlist(diff_mean_14[[2]]), 
    unlist(diff_mean_14[[3]]), unlist(diff_mean_14[[4]]),
    unlist(diff_mean_std_clr_14[[1]]), unlist(diff_mean_std_clr_14[[2]]),
    unlist(diff_mean_std_clr_14[[3]]), unlist(diff_mean_std_clr_14[[4]])
    # unlist(diff_mean_rob_clr_all[[1]]), unlist(diff_mean_rob_clr_all[[2]]),
    # unlist(diff_mean_rob_clr_all[[3]]), unlist(diff_mean_rob_clr_all[[4]])
  ),
  method = factor(rep(c("MCEM", "sample mean"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 2))
)

# define order of sequence
diff_df_14$method <- factor(diff_df_14$method, 
                        levels = c("MCEM", "sample mean"))

diff_df_14$size <- factor(diff_df_14$size, 
                        levels = c("20", "40", "80", "160"))

```


```{r evaluation covariance 14, eval=TRUE, fig.width=12, fig.height=5}
sigma_distances_14 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_std_clr_14 <- lapply(1:4, function(x) list(n_simulations))
sigma_distances_rob_clr_14 <- lapply(1:4, function(x) list(n_simulations))


for(j in 1:4) {
  for(i in 1:n_simulations) {
    # MCEM method
    sigma_hat_mcem <- with(simulation_results_list_14[[j]][[i]]$pca,
      t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
    sigma_distances_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_mcem, type = "F")
    
    # CLR method
    sigma_hat_clr <- with(pca_clr_std_14[[j]][[i]],
      rotation %*% diag(sdev^2) %*% t(rotation))
    sigma_distances_std_clr_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_clr, type = "F")
    
    # ILR method
    sigma_hat_rob <- with(pca_clr_rob_14[[j]][[i]],
    loadings %*% diag(eigenvalues) %*% t(loadings))
    sigma_distances_rob_clr_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_rob, type = "F")
  }
}

sigma_diff_df_14 <- data.frame(
  differences = c(
    unlist(sigma_distances_14[[1]]), unlist(sigma_distances_14[[2]]), 
    unlist(sigma_distances_14[[3]]), unlist(sigma_distances_14[[4]]),
    unlist(sigma_distances_std_clr_14[[1]]), unlist(sigma_distances_std_clr_14[[2]]),
    unlist(sigma_distances_std_clr_14[[3]]), unlist(sigma_distances_std_clr_14[[4]]),
    unlist(sigma_distances_rob_clr_14[[1]]), unlist(sigma_distances_rob_clr_14[[2]]),
    unlist(sigma_distances_rob_clr_14[[3]]), unlist(sigma_distances_rob_clr_14[[4]])
  ),
  method = factor(rep(c("MCEM", "standard", "robust"), each = 4 * n_simulations)),
  size = factor(rep(rep(c("20", "40", "80", "160"), 
                         each = n_simulations), 3))
)

# define order of sequence
sigma_diff_df_14$method <- factor(sigma_diff_df_14$method, 
                        levels = c("MCEM", "standard", "robust"))

sigma_diff_df_14$size <- factor(sigma_diff_df_14$size, 
                        levels = c("20", "40", "80", "160"))

plot1 <- ggplot(diff_df_14, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist mean",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true mean to estimated mean") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))


plot2 <- ggplot(sigma_diff_df_14, aes(x = size, y = differences, fill = method)) +
  geom_boxplot() +
  theme_grey() +
  scale_fill_manual(values = set_1) + 
  labs(y = "dist covariance",
       x = expression(m[j]),
       fill = "method",
       title = "Distance of true covariance to estimated covariance") +
  theme(legend.position = "top",
        plot.title = element_text(hjust = 0.5, size = 10))

grid.arrange(plot1, plot2, ncol = 2)

```



# base example



```{r example, eval=TRUE}
cts_data <- tar_read(data_kl15_comp)
pca_cts <- pcaCoDa(cts_data)
plot_pca_rotations(pca_cts$loadings, fixed = FALSE)
```


# test targets

## test base simulations

```{r test balanced simulation setting, eval=TRUE}
V <- get_helmert(5)

D <- 5
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

mean_14 <- c(0, 0.2, 0.1, -0.2, -0.1)

eigenvalues_14 <- c(0.6, 0.3, 0, 0)
V_14 <- cbind(c(0, sqrt(1/2), -sqrt(1/2), 0, 0), c(-sqrt(1/2), 0, 0, sqrt(1/2), 0), c(0, 0, 0, 0, 0), c(0, 0, 0, 0, 0))
true_sigma_14 <- V_14 %*% diag(eigenvalues_14) %*% t(V_14) 

true_v_14 <- mean_14

sim <- tar_read(sim_comp_2_smi_nSim_eb_160)
sim_data <- sim[[1]]$x_data_matrix
summary(sim_data)
rotation_matrix <- V_14[1:4 , 1:2] 
rotation_df <- as.data.frame(rotation_matrix) 
rownames(rotation_df) <- paste0("Component ", 1:nrow(rotation_df))
plot_pca_rotations(rotation_df, main = "PCA Rotation - Components 1 & 2", scale = 1, fixed = FALSE)
```

We have a base simulation with no zeros and easy covariance structure. 

standard estimation:

```{r test balanced simulation setting standard estimation 1, eval=TRUE}
standard_estimation <- prcomp(clr(sim_data))
plot_pca_rotations(standard_estimation$rotation, main = "PCA standard", scale = 1, fixed = FALSE)
```


Robust estimation:

```{r test balanced simulation setting standard estimation 2, eval=TRUE}
rob_estimation <- pcaCoDa(sim_data, method = "robust")
plot_pca_rotations(rob_estimation$loadings, main = "PCA robust", scale = 1, fixed = FALSE)
```

MCEM

```{r test balanced simulation setting standard estimation 3, eval=FALSE}
set.seed(12)
result <- fit_pca_ilr_vs_4(
          sim_data, 
          sc_factor = 1,
          lambda = 1,
          max_iter = 80,
          eps = 0.02
      )
rotation_clr <- V %*% result$pca$rotation 
plot_pca_rotations(rotation_clr, main = "PCA MCEM vs4", scale = 1, fixed = FALSE)

# Compare new version fit_pca_clr_vs_4
set.seed(12)
result_2 <- fit_pca_clr_vs_4(
          sim_data, 
          sc_factor = 1,
          lambda = 1,
          max_iter = 80,
          eps = 0.02
      )
rotation_clr <- result_2$pca$rotation 
plot_pca_rotations(rotation_clr, main = "PCA MCEM vs4", scale = 1, fixed = FALSE)

# Check version with scores
set.seed(12)
result_3 <- fit_pca_ilr_vs_8(
          sim_data, 
          sc_factor = 1,
          lambda = 1,
          max_iter = 80,
          eps = 0.02,
          scores = TRUE,
      )
rotation_clr <- result_3$pca$rotation 
plot_pca_rotations(rotation_clr, main = "PCA MCEM vs4", scale = 1, fixed = FALSE)

```

```{r test balanced simulation setting standard estimation 4, eval=FALSE}
result <- fit_pca_ilr_vs_5(
          sim_data, 
          sc_factor = 1,
          lambda = 1,
          max_iter = 80,
          eps = 0.02
      )
# rotation_clr <- V %*% result$pca$rotation 
plot_pca_rotations(result$pca$rotation , main = "PCA MCEM vs5", scale = 1, fixed = FALSE)
```

```{r test balanced simulation setting standard estimation 5, eval=TRUE}
results_mcem <- tar_read(pca_sim1_2_160_eb)
sim_test <- tar_read(pca_sim1_2_160_eb)
rotation_1 <-results_mcem[[1]]$pca$rotation

```


The algorithm has an ESS of around 10% and needs 26 seconds.

Numerical comparison:

```{r numerical comparison, eval=FALSE}
sigma_hat_mcem <- with(simulation_results_list_14[[j]][[i]]$pca,
  t(basis_matrix) %*% rotation %*% diag(sdev^2) %*% t(rotation) %*% basis_matrix)
sigma_distances_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_mcem, type = "F")

# CLR method
sigma_hat_clr <- with(pca_clr_std_14[[j]][[i]],
  rotation %*% diag(sdev^2) %*% t(rotation))
sigma_distances_std_clr_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_clr, type = "F")

# ILR method
sigma_hat_rob <- with(pca_clr_rob_14[[j]][[i]],
loadings %*% diag(eigenvalues) %*% t(loadings))
sigma_distances_rob_clr_14[[j]][[i]] <- norm(true_sigma_14 - sigma_hat_rob, type = "F")

```

## mcem versions performance

1. 

```{r algorithm performence, eval = TRUE}
D <- 13
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)

test_1 <- tar_read(pca_count_ilr_vs1_1perc)

print(paste("Performence version 1 with unrounded counts per milisecond, eps = 0.01"))
print(paste("Time:", test_1$time))
print(paste("Iteration:", test_1$iteration))
print(paste("1st PC:", test_1$pca$rotation[,1] %*% basis_matrix))
rotation_clr <- test_1$pca$rotation %*% basis_matrix
sum(rotation_clr[,1])
plot_pca_rotations(rotation_clr, fixed = TRUE)
```

2. 

```{r algorithm performence 2, eval = TRUE}
test_2 <- tar_read(pca_count_ilr_vs1_01)

print(paste("Performence version 1 with unrounded counts per milisecond, eps = 0.02"))
print(paste("Time:", test_2$time))
print(paste("Iteration:", test_2$iteration))
print(paste("1st PC:", test_2$pca$rotation[,1] %*% basis_matrix))
```

3. 

```{r algorithm performence 3, eval = TRUE}
test_3 <- tar_read(pca_count_ilr_vs1_0001)

print(paste("Performence version 1 with unrounded counts per 0.001, eps = 0.02"))
print(paste("Time:", test_3$time))
print(paste("Iteration:", test_3$iteration))
print(paste("1st PC:", test_3$pca$rotation[,1] %*% basis_matrix))
```


4. 

```{r algorithm performence 4, eval = TRUE}
test_4 <- tar_read(pca_count_ilr_vs2_sc)

print(paste("Performence version 1 original scale, eps = 0.035, lambda = 0.8"))
print(paste("Time:", test_4$time))
print(paste("Iteration:", test_4$iteration))
print(paste("1st PC:", test_4$pca$rotation[,1] %*% basis_matrix))
```

5. 

```{r algorithm performence 5, eval = TRUE}
test_5 <- tar_read(pca_count_ilr_vs5_sc01)

print(paste("Performence version 5 unrounded counts in milisecs, eps = 0.03"))
print(paste("Time:", test_5$time))
print(paste("Iteration:", test_5$iteration))
print(paste("1st PC:", test_5$pca$rotation[,1]))
```


6. 

```{r algorithm performence 6, eval = TRUE}
test_6 <- tar_read(pca_count_ilr_vs5)

print(paste("Performence version 5 original scaler, eps = 0.03"))
print(paste("Time:", test_6$time))
print(paste("Iteration:", test_6$iteration))
print(paste("1st PC:", test_6$pca$rotation[,1]))
```

7. 

```{r algorithm performence 7, eval = TRUE}
test_7 <- tar_read(pca_count_ilr_vs5_e06)

print(paste("Performence version 5 original scale, eps = 0.06"))
print(paste("Time:", test_7$time))
print(paste("Iteration:", test_7$iteration))
print(paste("ESS:", test_7$ESS))
print(paste("1st PC:", test_7$pca$rotation[,1]))
```

8. 

```{r algorithm performence 8, eval = TRUE}
test_8 <- tar_read(pca_count_ilr_vs5_sc01_e06)

print(paste("Performence version 5 milisecs, eps = 0.06"))
print(paste("Time:", test_8$time))
print(paste("Iteration:", test_8$iteration))
print(paste("ESS:", test_8$ESS))
print(paste("1st PC:", test_8$pca$rotation[,1]))
```

9. 

```{r algorithm performence 9, eval = TRUE}
test_9 <- tar_read(pca_count_ilr_vs5_sc01_noSign)

print(paste("Performence version 5 milisecs, eps = 0.03, noSign"))
print(paste("Time:", test_9$time))
print(paste("Iteration:", test_9$iteration))
print(paste("ESS:", test_9$ESS))
print(paste("1st PC:", test_9$pca$rotation[,1]))
```

10. 

```{r algorithm performence 10, eval = TRUE}
test_10 <- tar_read(pca_count_ilr_vs5_sc01_r30)

print(paste("Performence version 5 milisecs, eps = 0.03, r=30"))
print(paste("Time:", test_10$time))
print(paste("Iteration:", test_10$iteration))
print(paste("ESS:", test_10$ESS))
print(paste("1st PC:", test_10$pca$rotation[,1]))
```

11. 

```{r algorithm performence 11, eval = TRUE}
test_11 <- tar_read(pca_count_ilr_vs6_sc01)

print(paste("Performence version 6 milisecs, eps = 0.03"))
print(paste("Time:", test_11$time))
print(paste("Iteration:", test_11$iteration))
print(paste("ESS:", test_11$ESS))
print(paste("1st PC:", test_11$pca$rotation[,1]))
```

12. 

```{r algorithm performence 12, eval = TRUE}
test_12 <- tar_read(pca_count_ilr_vs6_acomp)

print(paste("Performence version 6 compistions, eps = 0.03"))
print(paste("Time:", test_12$time))
print(paste("Iteration:", test_12$iteration))
print(paste("ESS:", test_12$ESS))
print(paste("1st PC:", test_12$pca$rotation[,1]))
```


## time evaluation

1. 100 observations, 5 Dimensions and m_i = 30
 => erster Testlauf mit 60 min
 => zweiter Testlauf ohne Konvergenz
2. mit algorithm aus Debug => geht schon mal deutlich schneller! => aber konvergiert nicht
Warum ist er schneller?
3. Version 4 ohne Parallelism (Problem ist, dass die Reihenfolge der Schritte von Bedeutung ist)
 => deutlich schneller, aber konvergiert erst nach 47 iterationen: 4 Minuten
3. anderer Gradient? 

```{r test targets 1, eval=FALSE}
simulations <- tar_read(sim_comp_1_smi_nSim)
sim_1 <- simulations[[1]]
x_data <- sim_1$x_data

n_observations <- 100
eigenvalues <- c(0.6, 0.3, 0.05, 0.05)
mean <- c(0, 1.5, 0.5, -1.5, -0.5) 
n_counts <- 30
    
set.seed(11)
sim_composition_1_result <-
        build_setting_2comp_5parts(n_observations,
                                 eigenvalues,
                                 mean,
                                 n_counts)

sim_composition_2_result <-
        build_setting_2comp_5parts(n_observations = 500,
                                 eigenvalues,
                                 mean,
                                 n_counts = 120)

sim_composition_3_result <-
        build_setting_2comp_5parts(n_observations = 100,
                                 eigenvalues,
                                 mean,
                                 n_counts = 120)

      eps <- 0.01
      sc_factor <- 1
      sum_exp <- TRUE
      workers <- 10

sim_list <- sim_composition_1_result$x_data

# run mit 100 obsv und 30 mi
result <- fit_pca_ilr_vs_4(
          sim_list,
          max_iter = 70,
          sc_factor = 1,
          lambda = 1.7
      )

# run mit numerischen gradienten
result <- fit_compositional_pca_vs1_0(
          sim_list,
          max_iter = 70,
      )

result <- fit_pca_ilr_vs_5(
          sim_list,
          max_iter = 70,
          sc_factor = 1,
      )

result <- fit_compositional_pca_vs1_0(
          sim_list,
          max_iter = 70,
      )

result <- fit_compositional_pca_ilr_sc(
          sim_list,
          max_iter = 70,
      )

```

1. run mit 100 obsv und 30 mi: ESS around 40%, immer noch relativ viele Iterationen
Iteration 58: Mean ESS = 199.42
center: 1.092601 -0.2898866 -2.0432 -0.5537829 
critical value center_diff: 0.004487818 
critical value Sigma_diff: 0.00556544 
[1] "The algorithm converged after: 5.64449197451274 minutes"
1.1. run mit lambda = 1.7
Iteration 49: Mean ESS = 48.45
center: 1.094805 -0.2868017 -2.097223 -0.5397752 
critical value center_diff: 0.008850101 
critical value Sigma_diff: 0.008255015 
[1] "The algorithm converged after: 4.31488004525503 minutes"

2. same aber mit numerischem gradienten
=> konvergiert auch nicht schneller. Konvergiert überhaupt nicht:
Iteration: 70 
Iteration 70: Mean ESS = 307.57
center: 1.114362 -0.2883287 -2.232748 -0.5384367 
critical value center_diff: 0.004038523 
critical value Sigma_diff: 0.1014627 
clr-coordinates PCA1: 0.4674783 0.4402556 -0.2143162 -0.5717572 -0.1216605 
square root Eigenvalues: 1 0.7669431 0.4819049 0.3091026 

3. Erhöhe die Anzahl der Beobachtungen und der Stichprobe (führt zu deutlich geringerer ESS)
=> vor allem erhöht sich die Berechnungsdauer (Aufgrund der Beobachtungen)
Iteration 60: Mean ESS = 54.80
center: 1.056612 -0.2091303 -1.86429 -0.5650443 
critical value center_diff: 0.001666562 
critical value Sigma_diff: 0.00942241 
[1] "The algorithm converged after: 31.6623538096746 minutes"

4. Was ist eigentlich, wenn wir den alten Algorithmus verwenden?
=> Problem mit infiniten Values

5. Andere "alte" Variante: => Gradient ist falsch und es gibt zwar ein tolles Konvergenzverhalten, aber center is komplett neben der Spur
Iteration 8: Mean ESS = 79.92
Conditional score mean value -2.21:
Mean scores: 0.002497081 -0.0086868 0.003129301 0.01488901 
center: 0.01216824 -0.003429703 -0.01138274 -0.004876012 
Eigenvalues: 0.5783652 0.5675998 0.5524144 0.5482507 
critical value center_diff: 0.003508776 
critical value Sigma_diff: 0.005202193 
[1] "The algorithm converged after: 1.49322626193364 minutes"

Das hat auch damit zu tun, dass Likelihood und Hilfsverteilung besser passen, wenn der Mittelwertvektor der Hilfsverteilung off ist -> kleiner Werte

Das Problem im Konvergenzverhalten liegt an der sehr spitzen Verteilung der likelihood Funktion, d.h. die meisten Werte sind sehr klein und einige wenige
nah bei Null. Lässt sich ein ähnlicher Effekt mit lambda erzeugen? -> ja, teilweise

```{r test targets 3, eval=FALSE}  
sim_list <- tar_read(sim_comp_1_smi_nSim)

sim <- sim_list[[1]]
x_data <- sim$x_data
x_data[[1]]
result <- fit_pca_ilr_vs_4(
          x_data, 
          sc_factor = 1,
          max_iter = 80,
      )
```

Iteration 47: Mean ESS = 197.29
center: 1.095132 -0.2887021 -1.974885 -0.5978521 
critical value center_diff: 0.002372343 
critical value Sigma_diff: 0.009951388 
[1] "The algorithm converged after: 4.03395715554555 minutes"

Warum konvergiert der Algorithmus hier, aber nicht in targets? Wegen Standardisierung sdev?

## simple example

with a similar setting as in the paper

## count data

```{r count data, eval=FALSE}
x_data <- tar_read(data_kl15_comp)

x_data <- x_data * 0.01
set.seed(12)
pca_results_ilr_std_vs4 <-
      fit_pca_ilr_vs_4(x_data,
                       max_iter = 70,
                       r = 10,
                       lambda = 1,
                       eps = 0.01,
                       sc_factor = 1,
                       sum_exp = TRUE)

pca_results_ilr_std_vs1 <-
      fit_pca_ilr_vs_2(x_data,
                       max_iter = 70,
                       r = 10,
                       lambda = 1,
                       eps = 0.01,
                       sc_factor = 1,
                       sum_exp = TRUE)
```

Standard MCEM without sdev standardisierung braucht 43 Iterationen (zeitangabe ist: 1.29 Stunden)
ESS ist ca. bei 20 / 300 was auch ziemlich schlecht ist. 
center: -0.3251475 2.580131 -0.1727546 1.346242 -2.450683 -0.587455 1.618599 -0.3713966 0.9664395 4.205408 -0.4582662 1.517096

Version mit Parallelität zeigt so ziemlich das selbe Konvergenzverhalten. Ebenfalls 43 Iterationen, aber nur 36 Minuten
center: -0.314049 2.577283 -0.1631479 1.341978 -2.451277 -0.5902604 1.614901 -0.3866673 0.9619091 4.203238 -0.4613224 1.51431 
square root Eigenvalues: 0.3378715 0.2485879 0.1151774 0.07805561 0.0642686 0.05804606 0.04420633 0.03598033 0.03430581 0.02054505 0.01626922 0.01226602 
clr-coordinates PCA1: 0.6108956 -0.2075434 0.08272231 0.08179075 0.2401847 0.2375228 -0.2514116 -0.4179767 0.08466202 -0.1002224 0.09402394 -0.3043087 -0.1503393 

## Vergleich Greven Algorithmus

Eine Frage ist, wie sehr sich die print statements auswirken.

# Synthetic Principal Component Decomposition

For the synthetic principal component decomposition, we use the following settings:

The clr-covariance matrix K can be constructed by the **singular value decomposition**:

```{r svd K, eval=FALSE}
D <- 3
basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
basis_matrix <- do.call(rbind, basis_vectors)
E <- get_helmert(3)

mean_clr <- c(0, 1, 0.5) 
eigenvalues <- c(0.6, 0.3, 0)
eigenvalues_ilr <- c(0.6, 0.3)

egv_clr <- cbind(c(0, sqrt(1/2), -sqrt(1/2)), c(-sqrt(1/2), sqrt(1/2), 0), c(0, 0, 0))
K <- egv_clr %*% diag(eigenvalues) %*% t(egv_clr) 

V <- egv_clr %*% E
Vb <- egv_clr %*% t(basis_matrix) *(-1)
Vc <- clr2ilr(egv_clr)

# Sigma <- V %*% diag(eigenvalues_ilr) %*% t(V)
# I <- matrix(c(1,0,0,
#               0,1,0), nrow=3, ncol=2)

# Sigma_true <- Sigma[1:2,1:2]
# edc <- eigen(Sigma_true)
# edc$vectors

# egv_inv <- E %*% edc$vectors
# egv_clr_inv <- cbind(egv_inv, 0)
# K_inv <- egv_clr_inv %*% diag(eigenvalues) %*% t(egv_clr_inv) 

# try single transformation
V_1 <- egv_clr[,1] %*% E
V_2 <- egv_clr[,2] %*% E
V_b <- cbind(t(V_1),t(V_2))
Sigma_true <- V_b %*% diag(eigenvalues_ilr) %*% t(V_b)
edc_b <- eigen(Sigma_true)
egv_ilr <- edc_b$values
egv_ilr <- c(egv_ilr, 0)
# egv_inv_b_1 <- E %*% edc_b$vectors[,1]
# egv_inv_b_2 <- E %*% edc_b$vectors[,2]
# IS EQUIVALENT to
egv_inv_b <- E %*% edc_b$vectors


egv_clr_inv_b <- cbind(egv_inv_b, 0)
K_inv_b <- egv_clr_inv_b %*% diag(egv_ilr) %*% t(egv_clr_inv_b) 
# that is correct now

# Regain clr components is more complicated due to mathematical errors in eigen


# Are the clr-scores and the ilr-scores identical
# Can I directly switch between K and Sigma?
K_test <- E %*% Sigma_b %*% t(E)
Sigma_test <- t(E) %*% K %*% E

pca_clr <- princomp(covmat=K)
pca_ilr <- princomp(covmat=Sigma_true)
pca_clr$scores 
pca_ilr$scores
library(mvtnorm)
mean_ilr <- mean_clr %*% E
draws_clr <- rmvnorm(n = 100, mean = mean_clr, sigma = K)
draws_ilr <- draws_clr %*% E

scores_clr <- scale(draws_clr, center=TRUE, scale = FALSE) %*% pca_clr$loadings 
scores_ilr <- scale(draws_ilr, center=TRUE, scale = FALSE) %*% pca_ilr$loadings 

# is K singular?
det(K)
det(Sigma_true)

```



TODO: from `princomp`: fix_sign	parameter:
Should the signs of the loadings and scores be chosen so that the first element of each loading is non-negative?
