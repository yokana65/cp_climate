---
title: "Implementation MCEM algorithm"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---

```{r read library, eval=TRUE}
library(compositions)
library(targets)
```

# MCEM Theory

For a random variable $X_i$ with $D$ components, we observe $x_i$, which are the observed count compositions $x_{i} = (x_{i1}, x_{i2}, ..., x_{iD})^T$ of our random variable $X_{i} \sim \text{Multinom}(m_i, \boldsymbol{\pi_i})$
that is modelled through the following latent process: 

$$
\begin{equation*}
X_{i j} \stackrel{i . i . d .}{\sim} \operatorname{clr}^{-1}\left(G_{i}\right) \quad \text { with } \quad G_{i}=\sum_{k=1}^{D-1} \theta_{i k} e_{k} \text { and } \boldsymbol{\theta}_{i}=\left(\theta_{i 1}, \ldots, \theta_{i D-1}\right) \stackrel{i . i . d .}{\sim} \mathcal{N}(\boldsymbol{\nu}, \boldsymbol{\Sigma}) \tag{5}
\end{equation*}
$$


We use the eigendecomposition of $\Sigma^{(h)}$ to select an auxiliary distribution: 
$\boldsymbol{\theta}_{i} \sim \mathcal{N}\left(\boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)$ is equivalent to $\boldsymbol{z}_{i}=\boldsymbol{V}^{(h)}\left(\boldsymbol{\theta}_{i}-\boldsymbol{\nu}^{(h)}\right) \sim \mathcal{N}\left(\mathbf{0}, \operatorname{diag}\left(\sigma_{1}^{2}(h), \ldots, \sigma_{D-1}^{2}{ }^{(h)}\right)\right)$
to approximate the conditional distribution by a weighted sum of 
$$
\begin{equation*}
\mathbb{E}\left(\log \left(p\left(\boldsymbol{\theta}_{i} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right)\right) \approx \sum_{t=1}^{r} \frac{\omega_{i t}}{\sum_{t=1}^{r} \omega_{i t}} \log \left(p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right) \tag{8}
\end{equation*}
$$

with weights $\omega_{i t}, t=1, \ldots, r$ given as $\omega_{i t}=\frac{p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)}{p_{i}^{*}\left(\boldsymbol{\theta}_{i}^{(t)}\right)}$ for all $i=1, \ldots, n$ 

## Conditional Score Density

This gives us our objective function:

$$
\begin{align*}
& p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) \propto p\left(\mathbf{x}_{i} \mid \mathbf{z}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) p\left(\mathbf{z}_{i} \mid \boldsymbol{\Sigma}^{(h)}\right)=p\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{i}=\boldsymbol{V}^{(h) T} \mathbf{z}_{i}+\boldsymbol{\nu}^{(h)}\right) \prod_{k=1}^{D-1} p\left(z_{i k} \mid \sigma_{k}^{2}{ }^{(h)}\right) \\
& =\prod_{j=1}^{D} \operatorname{clr}^{-1}\left(\sum_{k=1}^{D-1} \nu_{k}^{(h)} e_{k}+\boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}\right)\left(x_{i j}\right) \prod_{k=1}^{D-1} p\left(z_{i k} \mid \sigma_{k}^{2(h)}\right) \\
& =\frac{\exp \left(\sum_{j=1}^{D} \, x_{ij} \, \left(\mu^{(h)}\left(A_{j}\right)+\sum_{k=1}^{D-1} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}\left(A_{j}\right)\right)\right)}{\left(\sum_{j=1}^{D}  \exp \left(\mu^{(h)}(A_{j})+\sum_{k=1}^{D-1} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}(A_j)\right)\right)^{m_{i}}} \prod_{k=1}^{D-1} p\left(z_{i k} \mid \sigma_{k}^{2^{(h)}}\right) \tag{1}
\end{align*}
$$


For the Expectation step of the MCEM algorithm, we apply importance sampling to generate samples of the conditional distribution 
$\theta_i \mid \boldsymbol{x}_i, \boldsymbol{\nu}, \boldsymbol{\Sigma}$ for $i=1, \ldots, n$.



```{r conditional scores density, eval=TRUE}
# objective function
conditional_scores_log_clr_composition <- function(scores, x_data_i, pca){
#   clr_density <- cbind(x_grid, pca$center + pca$rotation%*%scores)
clr_comp <- pca$center + pca$rotation%*%scores
norm_constant <- sum(exp(clr_comp))

log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant)
log_prior <- - sum(0.5*scores^2/(pca$sdev^2))
log_posterior <- log_likelihood + log_prior

return(log_posterior) 
}
```


## The gradient

The gradient from the continous case:

$$
\nabla \log \left(p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)\right)=\sum_{k=1}^{N} \boldsymbol{v}_{k}^{(h)}\left(\sum_{j=1}^{m_{i}} e_{k}\left(x_{i j}\right)-m_{i}\left\langle f_{\mathbf{z}_{i}}, e_{k}\right\rangle_{\mathbb{L}_{2}}\right)-\left(\frac{z_{i l}}{\sigma_{l}^{2^{(h)}}}\right)_{l=1, \ldots, N}
$$

where $f_{\mathbf{z}_{i}}=\operatorname{clr}^{-1}\left(\mu^{(h)}+\sum_{k=1}^{N} \underline{\boldsymbol{z}_{i}^{T}} \boldsymbol{v}_{k}^{(h)} e_{k}\right)$ for all $\mathbf{z}_{i}=\left(z_{i 1}, \ldots, z_{i N}\right)^{T} \in \mathbb{R}^{N}$.

Can be adapted to the compositional case as follows:
$$
\nabla \log \left(p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)\right)=\sum_{k=1}^{D-1} \boldsymbol{v}_{k}^{(h)}\left(\sum_{j=1}^{D} x_{i j} e_{k}\left(A_{j}\right)-m_{i}\left\langle \pi_{\mathbf{z}_{i}}, e_{k}\right\rangle\right)-\left(\frac{z_{i l}}{\sigma_{l}^{2^{(h)}}}\right)_{l=1, \ldots, D-1}
$$

where $\pi_{\mathbf{z}_{i}}=\operatorname{clr}^{-1}\left(\mu^{(h)}+\sum_{k=1}^{D-1} \underline{\boldsymbol{z}_{i}^{T}} \boldsymbol{v}_{k}^{(h)} e_{k}\right)$ for all $\mathbf{z}_{i}=\left(z_{i 1}, \ldots, z_{i D-1}\right)^{T} \in \mathbb{R}^{D-1}$.

```{r gradient, eval=TRUE}
gradient_cslc <- function(scores, x_data_i, pca){

  m_i <- sum(x_data_i)

  composition <- clrInv(pca$center + pca$rotation%*%scores)

  grad <- sapply(seq_along(scores), function(k) {
    # Zugriff auf den k-ten Basisvektor
    e_k <- pca$rotation[, k]

    # Term 1: Summe über die beobachteten Daten und den Basisvektor
    term1 <- sum(x_data_i * e_k)

    # Term 2: m_i * Skalarprodukt von pi_z_i und e_k
    term2 <- m_i * sum(composition * e_k)

    # Gradient für die k-te Komponente
    grad_k <- term1 - term2 - scores[k] / (pca$sdev[k]^2)

    return(grad_k)
})

return(grad)
#   sapply(seq_along(scores), function(k){
#     scalar_prod <- sum(density[,2]*pca$rotation[, k]*diff(mid_points))
#     sum(pca$rotation[idxs, k]) - length(idxs)*scalar_prod - scores[k]/(pca$sdev[k]^2)
#   })
}
```


### Update parameters and convergence criterium

Both can be easily adopted from the density case: 
1. update 
$$
\begin{aligned}
\boldsymbol{\nu}^{(h+1)} & =\frac{1}{\sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}} \sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t} \boldsymbol{\theta}_{i}^{(t)} \\
\boldsymbol{\Sigma}^{(h+1)} & =\frac{1}{\sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}} \sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}\left(\boldsymbol{\theta}_{i}^{(t)}-\boldsymbol{\nu}^{(h+1)}\right)\left(\boldsymbol{\theta}_{i}^{(t)}-\boldsymbol{\nu}^{(h+1)}\right)^{T}
\end{aligned}
$$
2. check convergence:
 $\left\|\boldsymbol{\nu}^{(h+1)}-\boldsymbol{\nu}^{(h)}\right\|<\epsilon$ and $\left\|\boldsymbol{\Sigma}^{(h+1)}-\boldsymbol{\Sigma}^{(h)}\right\|<\epsilon$ for a threshold $\epsilon>0$

## Inital values

The inital values are set to
$\boldsymbol{\nu}^{(0)}=\mathbf{0} \in \mathbb{R}^{D-1}$ and $\boldsymbol{\Sigma}^{(0)}=\mathbb{I}_{D} \in \mathbb{R}^{(D-1) \times(D-1)}$

## clr and ilr coordinates


Computationally, we can work with the clr-coordinates, since: 
$$
\begin{align*}
\operatorname{clr}(\mathbf{x}) \cdot \mathbf{V}^{t}= & \ln (\mathbf{x}) \cdot \mathbf{V}^{t}=: \operatorname{ilr}(\mathbf{x})=\xi  \tag{2.9}\\
& \operatorname{ilr}(\mathbf{x}) \cdot \mathbf{V}=\operatorname{clr}(\mathbf{x}) \longrightarrow \mathbf{x}=\mathscr{C}[\exp (\xi \cdot \mathbf{V})] \tag{2.10}
\end{align*}
$$

We just have to be aware of the problems that arise when computing the covariance matrix with the clr-coordinates. 

Given that the covariance matrix of $G_i$ is $K$ and $\Sigma$ is the covariance matrix of the basis coefficients $\boldsymbol{\theta}_i$, we have the following relationship between $K$ and $\Sigma$:
and $clr(x)=irl(x)E$, with $E$ being the matrix of orthonormal basis with each row being:
$$
e_{k}=\sqrt{\frac{k}{k+1}}(\overbrace{k^{-1}, \ldots, k^{-1}}^{k \text { times }},-1,0, \ldots, 0)^{T}
$$

The D-dimensional vector $\mu$ can be calculated with the following formula:
$$\mu=\sum{k=1}^{D-1} \nu_{k}^{(h)} e_{k}$$ 

To get a better understanding, we want to check the differences between clr- and ilr-PCA:

```{r clr vs ilr, eval=TRUE}
x <-matrix(c(0.2,0.5,0.3,0.1,0.4,0.5,0.4,0.3,0.3,0.3,0.5,0.2), nrow=4,ncol=3,byrow=TRUE)

# Compute the clr-coordinates
clr_coords <- clr(x)
ilr_coords <- ilr(x)

pca_clr <-prcomp(clr_coords)
pca_ilr <- prcomp(ilr_coords)

# Plot the first two principal components
# TODO: find out how to plot the principal components
scale <- 1
# TODO: translate that into a function
plot(pca_clr$rotation[, 1], pca_clr$rotation[, 2], xlab = "PC1", ylab = "PC2", main = "PCA - clr")
  # Add arrows for each variable
  arrows(0, 0, pca_clr$rotation[,1] * scale, pca_clr$rotation[,2] * scale, 
         length = 0.1, col = "blue")
  # Add origin lines
  abline(h = 0, v = 0, lty = 2, col = "gray")
biplot(pca_clr)
biplot(pca_ilr)
```

As we can see, the principal components are different (and have dimension D-1).

We should be able to reconstruct one from the other with the help of orthonormal bases:

```{r orthonormal basis, eval=TRUE}
generate_orthonormal_basis <- function(k, D) {
  # i: index of the basis vector (1-based index)
  # n: total number of elements in the vector
  
  # Calculate the scaling factor
  scaling_factor <- sqrt(k / (k + 1))
  
  # Create the vector with i elements of 1/i, followed by -1, and then zeros
  basis_vector <- c(rep(1 / k, k), -1, rep(0, D - k - 1))
  
  # Scale the vector
  basis_vector <- scaling_factor * basis_vector
  
  return(basis_vector)
}
D <- 3

basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)

(basis_matrix <- do.call(rbind, basis_vectors))
```

In addition, we want to check if the `center` is the sum of all eigenvectors and if we can transfrom them from ilr to clr

$\mu=\sum_{k=1}^{D-1} \nu_{k}^{(h)} e_{k}$ 

But the center is calculated as the column means of the respective coordinates:

center <- colMeans(X)

```{r check center, eval=TRUE}
# Summe als Multiplikation mit dem Einheitsvektor
mu_clr <- colMeans(clr_coords)
nu_ilr <- colMeans(ilr_coords)
# this is equivalent to pca$center

# Now, how can I calculate mu with pca_ilr$rotation?

# what is this?
# mu_pca <- pca_ilr$rotation %*% pca_ilr$center

```

Transformation ilr -> clr with (D-1)x(D) basis_matrix

Remember that one dimension of the eigenvectors of the clr coordinates is irrelevant, i.e. its eigenvalue is zero

```{r transform clr to ilr, eval=TRUE}
x_clr <- ilr_coords %*% basis_matrix*(-1)
# thats equal to clr_coords

# the question is does that hold for eigenvectors to?
clr_v <- t(pca_ilr$rotation %*% basis_matrix)
# Now each row of clr_v is an eigenvector of the clr coordinates!!


# joker <- colMeans(clr_v)
# clr_v[,1]
# pca_clr$rotation[,1]
# it does not
clr_v <- ilr2clr(pca_ilr$rotation)
clr_v[,1]
pca_clr$rotation[,1]
# that is exactly the same as before (theory vs. praxis?)
```

ACHTUNG! Die Koordinaten sind angeordnet! (Wieso??)

```{r equivalenz test, eval=TRUE}
angle <- acos(abs(sum(clr_v[,1] * pca_clr$rotation[,1])) / 
             (sqrt(sum(clr_v[,1]^2)) * sqrt(sum(pca_clr$rotation[,1]^2))))
(angle_degrees <- angle * 180/pi)

# Vergleich der erklärten Varianzen
var_explained_clr <- pca_clr$sdev^2 / sum(pca_clr$sdev^2)
var_explained_ilr <- pca_ilr$sdev^2 / sum(pca_ilr$sdev^2)
```

The elements are equivilent. We have a problem though, because the order of coordinates can change. 

Let's try to visualise that:

```{r transformed visualization, eval=TRUE}
plot(pca_clr$rotation[, 1], pca_clr$rotation[, 2], xlab = "PC1", ylab = "PC2", main = "PCA - clr")
  # Add arrows for each variable
  arrows(0, 0, pca_clr$rotation[,1] * scale, pca_clr$rotation[,2] * scale, 
         length = 0.1, col = "blue")
  text(pca_clr$rotation[,1] * scale * 1.1, pca_clr$rotation[,2] * scale * 1.1, 
       rownames(pca_clr$rotation), cex = 0.8)
  # Add origin lines
  abline(h = 0, v = 0, lty = 2, col = "gray")
```

```{r transformed visualization 2, eval=TRUE}
# Now we try to get the same from the ilr coordinates
clr_v <- t(pca_ilr$rotation %*% basis_matrix*(-1))
plot(clr_v[, 1], clr_v[, 2], xlab = "PC1", ylab = "PC2", main = "PCA - clr")
  # Add arrows for each variable
  arrows(0, 0, clr_v[,1] * scale, clr_v[,2] * scale, 
         length = 0.1, col = "blue")
  text(clr_v[,1] * scale * 1.1, clr_v[,2] * scale * 1.1, 
       rownames(clr_v), cex = 0.8)
  # Add origin lines
  abline(h = 0, v = 0, lty = 2, col = "gray")
```

Normal computation leads to same representation but with upside down orientation. Multiplikation witrh (-1) for the bases solves that problem.
Ok, it remains "Spiegelverkehrt" but that does not really matter for the results.

That means I can switch between W and V (and in general between the two spaces of clr and ilr coordinates).

Also the **argument** for working in ilr-coordinates is that I dont have to calculate coordinates for the redundant dimension and
therefore dont need to be bothered with zero eigenvalues. **Another** way would be to restrict Sigma to be (D-1)x(D-1), but then
I still have to deal with weird weights before (?).

## Methods

In general, it is possible to use the `compositions`package to get most relevant methods. But we want to implement our own Methods
to get a better understanding of the underlying functions.

### ilrBase

with:
$$
e_{k}=\sqrt{\frac{k}{k+1}}(\overbrace{k^{-1}, \ldots, k^{-1}}^{k \text { times }},-1,0, \ldots, 0)^{T}
$$

### clr

### ilr

# Algorithm

## Implementation

First computational implementation:


```{r fit pca function, eval=TRUE}
fit_compositional_pca <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){
start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  # initial estimates
  D <- length(x_data[[1]])
  nu <- rep(0, D)
  Sigma <- diag(D)
  # compute initial pca
  pca <- prcomp(Sigma)
  pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
  pca$center <- nu
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                              x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_clr_composition(scores, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
      }
      if (any(!is.finite(weights[[i]]))) {
         stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)

      eigen_decomp <- eigen(Sigma)
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data))
}
```



## simulation

We assume that `x_data` is a list of D-dimensional vectorsm representing the count composition of observation i. 

For this simulation approach, we use specific orthonormal bases as representatives for the principal components. With that
we can directly calculate the "true" densities $\pi_i$ as back transformations of $\rho_i$, which are linear combinations
of the principal components.

```{r simulation pcas, eval=TRUE}
# PCA 1 with lambda = 0.5
(c(rep(sqrt(5/(5+1)*5^(-1)), 5),-1, rep(0, 7)))
# PCA 2 with lambda = 0.2
(c(rep(sqrt(12/(12+1)*12^(-1)), 12),0))

```

The procedure to construct the simulated Data with two principal components that focus on the 5th and 13th part is as follows:

```{r test data, eval=TRUE}

# define number of components
n_components <- 13
x_grid <- seq(1, n_components)
# make the neutral element the center for a start
raw_data <- data.frame("x" = x_grid, "y" = rep(0, n_components))

center_function_comp <- function(comp_data){
  mean <- mean(comp_data[,2])
  comp_data[,2] <- comp_data[,2] - mean
  comp_data
}

clr_mean <- center_function_comp(raw_data)

# zwei Hauptkomponenten im clr-Raum
# PC1 is a balancing effect between the the first five elements and the sixths element
# alternative: use Egozcue et al. (2003) orthogonal basis
pc_1 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(5/(5+1)*5^(-1)), 5),-1, rep(0, 7)))
pc_1 <- center_function_comp(pc_1)
# centering resolves in the sum of all elements being zero
# Normalisierung
pc_1[,2] <- pc_1[,2]/norm(pc_1[,2],type="2")


pc_2 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(12/(12+1)*12^(-1)), 12),0))
pc_2 <- center_function_comp(pc_2)
pc_2[,2] <- pc_2[,2]/norm(pc_2[,2], type="2")

lambda_1 <- 0.5
lambda_2 <- 0.2

# Schritt 2: Simulieren Sie die Scores und konstruieren Sie die Dichten
n_data <- 30
true_observed_clr_comp <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})

# error checking: Sum to zero
check_columns_sum_to <- function(data, integer) {
  n_cols <- ncol(data)
  
  columns_sum_to_zero <- logical(n_cols)
  
  for (i in 1:n_cols) {
    column_sum <- sum(data[, i])
    columns_sum_to_zero[i] <- (column_sum < integer)
  }
  
  return(columns_sum_to_zero)
}
check_columns_sum_to(true_observed_clr_comp, 0.001)

inverse_clr_trafo <- function(clr_density){
  f_integral <- sum(exp(clr_density[,2]))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

true_observed_comp <- lapply(1:n_data, function(i){
  # TODO: remove x_grid
  clr_density <- data.frame(x_grid, true_observed_clr_comp[,i])
  inverse_clr_trafo(clr_density)
})

# Schritt 3: Datenpunkte aus den Dichten ziehen
# Define number of counts when dealing with multinomial distributions
n_counts <- 2000
n_samples <- 40
# change the structure to a list of compositions
# x_data <- lapply(1:n_data, function(i){
#   probs <- true_observed_densities[[i]][,2]
#   t(rmultinom(n_samples, n_counts, probs))
# })
x_data <- unlist(lapply(1:n_data, function(i) {
  probs <- true_observed_comp[[i]][,2]
  # TODO: needs to be redesigned since X is modelled as a latent process G_i and not as Multinomial
  samples <- t(rmultinom(n_samples, n_counts, probs))
  lapply(1:nrow(samples), function(j) samples[j,])
}), recursive = FALSE)

# turn x_data into a matrix
# TODO: Why do I need a list at all? 
x_data_matrix <- do.call(rbind, x_data)

```



```{r simulation decomposition pcas, eval=TRUE}
library(compositions)

x_acomp <- acomp(x_data_matrix)
x_clr <- clr(x_acomp)
x_ilr <- ilr(x_acomp)
pca <- prcomp(na.omit(x_clr))
pcx <- princomp(x_clr)
pcy <- princomp(x_acomp)
pcz <- princomp(x_ilr)
pcq <- prcomp(na.omit(x_ilr))
# pcx$loadings
# pca$rotation
# pcy$loadings
ilr_eigenvectors <- pcz$loadings
clr_eigenvectors <- ilr2clr(ilr_eigenvectors)
biplot(pca)
```

TODO! Explain the differences for PCA on ilr-coefficients and clr-coordinates.
<!-- 

### orthonormal bases


```{r orthonormal basis, eval=TRUE}
generate_orthonormal_basis <- function(k, D) {
  # i: index of the basis vector (1-based index)
  # n: total number of elements in the vector
  
  # Calculate the scaling factor
  scaling_factor <- sqrt(k / (k + 1))
  
  # Create the vector with i elements of 1/i, followed by -1, and then zeros
  basis_vector <- c(rep(1 / k, k), -1, rep(0, D - k - 1))
  
  # Scale the vector
  basis_vector <- scaling_factor * basis_vector
  
  return(basis_vector)
}
D <- 3

basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)

(basis_matrix <- do.call(rbind, basis_vectors))
```


We need the computation of the orthonormal bases $e_k$

```{r bases, eval=FALSE}
x <- matrix(c(0.2, 0.5, 0.3,
              0.1, 0.4, 0.5,
              0.4, 0.3, 0.3,
              0.3, 0.5, 0.2), nrow = 4, ncol = 3, byrow = TRUE)

# Zugriff auf die Funktion
optimal_basis <- compositions:::gsi.optimalilrBase(x)
build_base <- compositions:::gsi.buildilrBase(x)
# Ausgabe der optimalen ilr-Basis
build_base

``` 

-->

### Possible adjustments

## adjusted conditional scores density and gradient

Since our data has a very big sample size (~600.000 per observation), we need to adjust the likelihood function to avoid infinite or zero weight calculations.

```{r adjusted ilr conditional scores density, eval=TRUE}
# objective function
conditional_scores_log_ilr <- function(scores, x_data_i, pca, basis_matrix){
ilr_comp <- pca$center + pca$rotation%*%scores
clr_comp <- t(ilr_comp)%*%basis_matrix
norm_constant <- sum(exp(clr_comp))
# if (!is.finite(norm_constant) || norm_constant <= 0) {
#   print("norm constant for scores :")
#   print(scores)
#   print(norm_constant)
#   stop("norm_constant ist nicht finit oder nicht positiv.")
# }

log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant)

log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))
log_posterior <- log_likelihood + log_prior

return(log_posterior)
}
```

```{r adjusted clr conditional scores density, eval=TRUE}
# objective function
conditional_scores_log_clr_adjusted <- function(scores, x_data_i, pca){
clr_ilr <- pca$center + pca$rotation%*%scores
norm_constant <- sum(exp(ilr_comp))
# if (!is.finite(norm_constant) || norm_constant <= 0) {
#   print("norm constant for scores :")
#   print(scores)
#   print(norm_constant)
#   stop("norm_constant ist nicht finit oder nicht positiv.")
# }

log_likelihood <- sum(x_data_i * clr_comp_adj) - sum(x_data_i)*log(norm_constant)

log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))
log_posterior <- log_likelihood + log_prior

return(log_posterior)
}
```

TODO: when this objective function is used, we probably have to adjust the gradient as well!



TODO: build error check for correct structure of x_data_i
TODO: build numerical stability functions
TODO: check zentrierung der Daten -->

```{r gradient adjusted, eval=FALSE}
gradient_cslc_ilr <- function(scores, x_data_i, pca, basis_matrix){

  m_i <- sum(x_data_i)

  ilr_comp <- pca$center + pca$rotation%*%scores
  clr_comp <- t(ilr_comp)%*%basis_matrix
  composition <- clrInv(clr_comp)

  grad <- sapply(seq_along(scores), function(k) {
    # Zugriff auf den k-ten Basisvektor
    e_k <- basis_matrix[k, ]

    # Term 1: Summe über die beobachteten Daten und den Basisvektor
    term1 <- sum(x_data_i * e_k)

    # Term 2: m_i * Skalarprodukt von pi_z_i und e_k
    term2 <- m_i * sum(composition * e_k)

    # Gradient für die k-te Komponente
    grad_k <- term1 - term2 - scores[k] / (pca$sdev[k]^2)

    return(grad_k)
})

return(grad)
}
```

## helper functions

### stabilize weights

```{r stabilize weights, eval=TRUE}
stabilize_weights <- function(log_weights) {
    # Find maximum log weight
    max_log_weight <- max(log_weights)
    
    # Subtract maximum and exponentiate
    stable_weights <- exp(log_weights - max_log_weight)
    
    # Normalize
    stable_weights / sum(stable_weights)
}

```

### monitor weights

```{r monitor weights, eval=TRUE}
monitor_weights <- function(weights, iteration) {
  for (i in 1:length(weights)) {
    ess <- 1/sum(weights[[i]]^2)
    max_weight <- max(weights[[i]])
    cat(sprintf("Iteration %d:\n ESS: %.2f\n Max weight: %.2e\n Observation: %d\n", 
                iteration, ess, max_weight, i))
  }
}

monitor_global_ess <- function(all_weights, k) {
    mean_ess <- mean(sapply(all_weights, function(w) 1/sum(w^2)))
    cat(sprintf("Iteration %d: Mean ESS = %.2f\n", k, mean_ess))
}
```


### check optim inputs

```{r optim input, eval=TRUE}
check_optim_inputs <- function(pca, x_data_i, k) {
    cat("PCA dimensions:", dim(pca$rotation), "\n")
    cat("Data dimensions:", length(x_data_i), "\n")
    cat("Number of components:", length(pca$sdev), "\n")
    cat("Iteration:", k, "\n")
}

```

### Plot results

```{r plot rotation matrix, eval=TRUE}
plot_pca_rotation <- function(rotation, scale = 1, main = "PCA - clr") {
    # Create base plot
    plot(rotation[, 1], rotation[, 2], 
         xlab = "PC1", ylab = "PC2", 
         main = main)
    
    # Add arrows from origin to loadings
    arrows(0, 0, 
           rotation[,1] * scale, 
           rotation[,2] * scale, 
           length = 0.1, col = "blue")

    # Add labels using rownames if they exist, otherwise use indices
    labels <- if(!is.null(rownames(rotation))) rownames(rotation) else 1:nrow(rotation)
    
    text(rotation[,1], rotation[,2], 
         labels = labels, 
         pos = 4, 
         cex = 0.8)
    
    # Add reference lines
    abline(h = 0, v = 0, lty = 2, col = "gray")
}
```


## ilr implementation

```{r ilr implementation, eval=TRUE}

fit_compositional_pca_ilr <- function(x_data,
                                max_iter = 50,
                                r = 10,
                                lambda = 1,
                                eps = 0.01){
  start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  D <- length(x_data[[1]])  
  # create orthonormal basis
  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)
  
  # initial estimates
  nu <- rep(0, D-1)
  Sigma <- diag(D-1)
  # compute initial pca
  pca <- list()
  eigen_decomp <- eigen(Sigma)
  pca$rotation <- eigen_decomp$vectors
  pca$sdev <- eigen_decomp$values
  # center needs to be a column vector
  pca$center <- matrix(nu, ncol = 1)
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      cat("Iteration:", k, "\n")
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check 
        # check_optim_inputs(pca, x_data[[i]])
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_ilr, gr = gradient_cslc_ilr,
                              x_data_i = x_data[[i]], pca = pca, basis_matrix = basis_matrix,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            print("optim result:")
            print(optim_result)
            print("pca center:")
            print(pca$center)
        })
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })

        # check value of the conditional distribution
        conditional_scores_list[[i]] <- conditional_scores_log_ilr_sc(scores, x_data[[i]], pca, basis_matrix) 
        # cat(sprintf("Conditional score mean value %.2f:\n at observation %d\n", 
        #         conditional_scores_list[[i]], i))
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_ilr(scores, x_data[[i]], pca, basis_matrix) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        weights[[i]] <- stabilize_weights(log_weights)
        # if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
        #   print("non finite weights for observation: ", i)
        #   # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        # }
      }
      monitor_global_ess(weights, k)
      mean_conditional <- mean(unlist(conditional_scores_list), na.rm = TRUE)
          cat(sprintf("Conditional score mean value %.2f:\n", 
                mean_conditional))
      # if (any(!is.finite(weights[[i]]))) {
      #     print("non finite weights for observation: ", i)
      #     # print(weights[[i]])
      #    # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      # }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }), na.rm = TRUE)
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      # if (any(!is.finite(pca$sdev)) || any(pca$sdev <= 0)) {
      #   print("pca$sdev:")
      #   print(pca$sdev)
      #   stop(paste("Fehler in Iteration", k, ": pca$sdev enthält nicht-finite oder nicht-positive Werte."))
      # }
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      # pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value 1:", critical_value_1, "\n")
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")
      cat("critical value 2:", critical_value_2, "\n")            

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data, "time" = elapsed_time))
}

```

equivalent with clr


```{r clr implementation, eval=TRUE}

fit_compositional_pca_clr <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){
  start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  D <- length(x_data[[1]])  
  # create orthonormal basis
  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)
  
  # initial estimates
  mu <- rep(0, D)
  K <- diag(D)
  # compute initial pca
  pca <- prcomp(K)
  # restrict to D-1 dimensions
  pca$rotation <- pca$rotation[,1:(D - 1)]
  pca$sdev <- pca$sdev[1:(D - 1)]
  pca$center <- mu

  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                              x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_clr_composition(scores, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
        if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
          print("non finite weights for observation: ", i)
          stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        }
      }
      if (any(!is.finite(weights[[i]]))) {
          print("non finite weights for observation: ", i)
          print(weights[[i]])
         stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      print(mu_scores)
      print(weights)
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      print(Sigma)
      eigen_decomp <- eigen(Sigma)
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      # if (any(!is.finite(pca$sdev)) || any(pca$sdev <= 0)) {
      #   print("pca$sdev:")
      #   print(pca$sdev)
      #   stop(paste("Fehler in Iteration", k, ": pca$sdev enthält nicht-finite oder nicht-positive Werte."))
      # }
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      # pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data))
}

```




## Test with simulated data


1. get simulated data

```{r get simulated data, eval=FALSE}
sim_data <- tar_read(simulation_composition_1)
count_data <- sim_data$x_data
testrun <- fit_compositional_pca_ilr(count_data, max_iter = 50, r = 10, lambda = 1, eps = 0.01)
plot_pca_rotation(testrun$pca$rotation)
```

```{r test function, eval=TRUE}
fit_compositional_pca_db <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){
  start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  # initial estimates
  D <- length(x_data[[1]])
  nu <- rep(0, D)
  Sigma <- diag(D)
  # compute initial pca
  pca <- prcomp(Sigma)
  pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
  pca$center <- nu
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                              x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            # print("pca$sdev:")
            # print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_clr_composition(scores, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
        if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
          print("non finite weights for observation: ", i)
          stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        }
      }
      if (any(!is.finite(weights[[i]]))) {
          print("non finite weights for observation: ", i)
          print(weights[[i]])
         stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)

      eigen_decomp <- eigen(Sigma)
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      if (any(!is.finite(pca$sdev)) || any(pca$sdev <= 0)) {
        print("pca$sdev:")
        print(pca$sdev)
        stop(paste("Fehler in Iteration", k, ": pca$sdev enthält nicht-finite oder nicht-positive Werte."))
      }
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data))
}

```

```{r test visualisation, eval=TRUE}
sim_1_pca <- tar_read(composition_pca_sim_1)
biplot(sim_1_pca$pca)
```

The algorithm correctly identifies the principal components.

## Simple dataset

```{r simple dataset, eval=TRUE}
x <-matrix(c(20,50,30,10,40,50,40,30,30,30,50,20), nrow=4,ncol=3,byrow=TRUE)
x_list <- apply(x, 1, function(x) x, simplify = FALSE)
```

# Debugging

Here the comparison to standard PCA with our original data:

```{r comparison to standard PCA, eval=TRUE}
x <- tar_read(data_kl15_comp)
x_acomp <- acomp(x)
x_clr <- clr(x_acomp)
pca <- prcomp(na.omit(x_clr))
biplot(pca)
plot(pca)
```

Now lets run it with our adjusted algorithm in debbuging mode:

```{r MCEM with original data, eval=TRUE}
data_comp <- tar_read(data_kl15_comp)
colnames(data_comp)
data_comp <- unname(data_comp)
count_data <- apply(data_comp, 1, function(x) x, simplify = FALSE)
# count_data[[1]]
# count_data_small <- count_data[1:10]
testrun2 <- fit_compositional_pca_ilr(count_data, max_iter = 50, r = 15, lambda = 1, eps = 0.03)
plot_pca_rotation(testrun2$pca$rotation)
png("MCEM_rotation_plot.png", width = 800, height = 600)
plot_pca_rotation(testrun2$pca$rotation)
dev.off()
plot(testrun2$pca$sdev)

testrun_k50 <- fit_compositional_pca_ilr(count_data, max_iter = 50, r = 15, lambda = 1, eps = 0.03)
testrun_k10_r10_01_lambda <- fit_compositional_pca_ilr_bb(count_data, max_iter = 50, r = 10, lambda = 1, eps = 0.01)
testrun_k10_r10_01 <- fit_compositional_pca_ilr(count_data, max_iter = 30, r = 15, lambda = 0.6, eps = 0.03)
# run with scaling
testrun_scaling <- fit_compositional_pca_ilr_sc(count_data, max_iter = 30, r = 15, lambda = 0.8, eps = 0.03)

plot_pca_rotation(testrun_scaling$pca$rotation)
```

we want to compare that to the results of a classical pca with ilr-coefficients:

```{r classical pca with ilr, eval=TRUE}
data_comp <- tar_read(data_kl15_comp)

x_ilr <- ilr(data_comp)
x_clr <- clr(data_comp)
pca_ilr <- princomp(x_ilr)
pca_clr <- princomp(x_clr)
biplot(pca_ilr)
```

A problem is, that we can not really interprete ilr-coefficents. So lets try to compute the clr-coordinates:

```{r transformation into clr, eval=TRUE}
pca_results <- testrun_scaling$pca
rotation_ilr <- pca_results$rotation
rotation_clr <- t(rotation_ilr%*%basis_matrix)
rownames(rotation_clr) <- colnames(data_comp)
plot_pca_rotation(rotation_clr)
png("MCEM_rotation_clr_plot_scaling.png", width = 800, height = 600)
plot_pca_rotation(rotation_clr)
```

Compare that to the clr Biplot

```{r classical biplot, eval=TRUE}
plot_pca_rotation(pca_clr$loadings)
plot(pca_clr)
```


As a control function, we want to check if the adjusted algorithm reproduces the results of the simulated data:

```{r test simulation ilr, eval=FALSE}
sim_data <- tar_read(simulation_composition_1)
sim_data <- sim_data$x_data
# x_data_reduced <- lapply(sim_data, function(x) x[-1])
testrun3 <- fit_compositional_pca_ilr(sim_data, max_iter = 20, r = 30, lambda = 1, eps = 0.05)
plot_pca_rotation(testrun3$pca$rotation)
```

That gives us the result as we expected them for ilr-coordinates.

## Error message

1. in optim:
45: In x_data_i * clr_comp :
  longer object length is not a multiple of shorter object length


### prcomp

Therefore, we need the sourcecode of `prcomp`

```{r sourcecode, eval=TRUE}
# princomp is generische Funktion, i.e. depending on input class
methods("prcomp")
# its either acomp or default that we are interested in
getS3method("prcomp", "default")
# getS3method("princomp", "acomp")
```

Interesting parts:
```{r, eval=FALSE}
    edc <- eigen(cv, symmetric = TRUE)
    ev <- edc$values
    if (any(neg <- ev < 0)) {
        if (any(ev[neg] < -9 * .Machine$double.eps * ev[1L])) 
            stop("covariance matrix is not non-negative definite")
        else ev[neg] <- 0
    }
```

That is certainly something to integrate (for a short shorter implementation -> we could also provide the covariance matrix to `princomp`):

Be aware `princomp` != `prcomp`
and the `loadings` are not the same as the `rotation` values (even though they should contain the eigenvectors)
To be consistend with the old implementation, we use `prcomp` for now

## Debugging functions

### unit tests

we test general units of the algorithm

```{r test, eval=TRUE}
# x_data is set
D <- 13
# scores can be the clr-coordinates of a random sample
scores <- clr(x_data[[1]])
# check restriction
sum(scores)<0.0001
# calculate the pca on the clr-coordinates
x_clr <- clr(x_data_matrix)
pca <- prcomp(na.omit(x_clr))
# test the calculation of clr-coordinates (\rho) with scores and clr-eigenvectors
clr_comp <- pca$center + pca$rotation%*%scores
# test clrInv
composition <- clrInv(pca$center + pca$rotation%*%scores)
# finally run with inital values
nu <- numeric
nu <- rep(0, D)
Sigma <- diag(D)
# compute the pca
pca <- prcomp(Sigma)
# do I need a transpose? -> every column is supposed to be one eigenvector: yes, check ?eigen
# pca$rotation <- eigen_decomp$vectors
# # center the eigenvectors to achieve the restriction of sum zero
# pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
# pca$center <- apply(pca$rotation, 2, function(g) mean(g))
```

What changes when we compute the D-1 dimensional pca?

```{r test d-1, eval=FALSE}
nu <- rep(0, D-1)
Sigma <- diag(D-1)
# compute the pca
pca <- prcomp(Sigma)
# the data looses one dimension: the question if that is done with just removing the pivot?
x_data_i <- count_data[[1]][2:13]

# center the eigenvectors to achieve the restriction of sum zero
# pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
# pca$center <- apply(pca$rotation, 2, function(g) mean(g))
scores <- rep(0, length = length(pca$sdev))
# construct clr values given the proposed scores and Sigma
ilr_comp <- pca$center + pca$rotation%*%scores
# TODO: they should be centered. i.e. sum up to zero (not necessary of we would use ilr coefficients)
norm_constant <- sum(exp(clr_comp))
# clr_comp_adj <- clr_comp - max(clr_comp)

log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant)

log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))
(log_posterior <- log_likelihood + log_prior)


```

### optim

```{r check scores, eval=FALSE}
scores_list <- list()
x_data_i <- count_data[[1]]
i <- 1
# error check to analyse `vmmin` is not finite 
optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_ilr, gr = gradient_cslc_ilr,
                              x_data_i = x_data_i, pca = pca, basis_matrix = basis_matrix,
                              control = list(fnscale = -1), method = "BFGS")
scores_median <- as.vector(optim_result$par)
```

### conditional scores

What happens inside optim: The objective function is optimized given the gradient.

```{r optimisation, eval=FALSE}
scores <- rep(0, length = length(pca$sdev))
# construct clr values given the proposed scores and Sigma
clr_comp <- pca$center + pca$rotation%*%scores
# TODO: they should be centered. i.e. sum up to zero (not necessary of we would use ilr coefficients)
norm_constant <- sum(exp(clr_comp))
# clr_comp_adj <- clr_comp - max(clr_comp)

log_likelihood <- sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant)

log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))
(log_posterior <- log_likelihood + log_prior)
```

The problem lies in the fact that: 
$$p\left(\mathbf{x}_{i} \mid \mathbf{z}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) $$
is a value around -556132 for the first iteration. That is mainly due to the extreme value in 
$\left(\sum_{j=1}^{D}  \exp \left(\mu^{(h)}(A_{j})+\sum_{k=1}^{D-1} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}(A_j)\right)\right)^{m_{i}}$ which is alone around 10^6.

By the way, the interpretation is that given the initial values the prior has almost no influence and the data dominates the likelihood.

#### max likelihood 

The conditional distribution is maximised when the scores are the clr-coordinates of x_i and the pca results close to the true parameters.

```{r max likelihood, eval=FALSE}
x_data_1 <- count_data[[1]]
clr_data <- clr(x_data_1)
scores <- clr_data
clr_counts <- clr(data_comp)
pca <- prcomp(na.omit(clr_counts))

clr_comp <- pca$center + pca$rotation%*%scores
norm_constant <- sum(exp(clr_comp))

log_likelihood <- sum(x_data_1 * clr_comp) - sum(x_data_1)*log(norm_constant)

log_prior <- - sum(0.5 * scores^2 / (pca$sdev^2))
log_posterior <- log_likelihood + log_prior

plot(pca$sdev)
```

The problem lies in the **singular** sigma.


For the optim function, an integer 0 indicates successful completion.
Also $counts gives the number of calls to the objective function and the gradient. 1 means that the iteration limit has been reached -> **increase max_iter**



Lets have a look at the auxialiary distribution and the conditional distribution of scores:

For our first observation, we

```{r check objective function 5, eval=FALSE}
data_comp <- tar_read(data_kl15_comp)
data_comp <- unname(data_comp)
count_data <- apply(data_comp, 1, function(x) x, simplify = FALSE)
count_data_small <- count_data[1:10]

lambda <- 1
# proposal scores are sampled from the auxiliary distribution
  D <- length(count_data[[1]])
    nu <- rep(0, D)
  Sigma <- diag(D)
  pca <- prcomp(Sigma)
    # center the eigenvectors to achieve the restriction of sum zero (implies D-1 space)
    pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
    pca$center <- nu

# observed data for observation i
count_data_i = count_data[[1]]

optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                      x_data_i = count_data_i, pca = pca,
                      control = list(fnscale = -1), method = "BFGS")
scores_median <- as.vector(optim_result$par)

# importance sampling
n_samples <- 10
# z_i^(t)
proposal_scores_i <- sapply(1:n_samples, function(t){
  matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
})
# calculate the weights
log_weights <- apply(proposal_scores_i, 2, function(scores){
  conditional_scores_log_clr_composition(scores, count_data_i, pca) -
    sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
})
# Seems fine until here
  weights <- list(length(count_data))
# Normalisation is an issue
# the following creates just extreme values
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        # we divide by Inf
        weights[[1]] <- exp(log_weights)/sum(exp(log_weights))
# alternative
max_log_weight <- max(log_weights)
# we substract the maximum from each weight and then take the exponential
weights <- exp(log_weights - max_log_weight)
weights <- weights / sum(weights)

```

Our initial values are:

```{r initial values, eval=TRUE}
D <- 13
Sigma <- diag(D)
pca_old <- prcomp(Sigma)
# Erstellen des Plots mit dem ersten Vektor in blau
plot(pca_old$sdev, col="blue", main="Comparison of Parameters", type="b", 
     ylim=range(c(pca_old$sdev, pca$sdev)))

# Hinzufügen des zweiten Vektors in rot
lines(pca$sdev, col="red", type="b")

# Hinzufügen einer Legende
legend("topright", 
       legend=c("Initial parameters", "Updated parameters"),
       col=c("blue", "red"), 
       lty=1,
       pch=1)
```

The last one is irrelevant and should be disgarded. Otherwise the algorithm seems to make pro




### Problems and dissection

There are several problems:
- singular covariance matrix
- Effective sampling size of 1 in importance sampling
- clr and clrInv implementation is not clear
- Overflow in the log-likelihood: that is weight values that are to large to take an exponential

```{r short, eval=FALSE}
Sigma <- diag(D)
# pca <- princomp(Sigma)
pca <- prcomp(Sigma)
# pca$loadings
pca$rotation
```

The last eigenvector is just `sqrt(1/D)`, i.e. the "Nullvector"

#### Algorithm for k = 1

```{r testrun 1, eval=TRUE}
fit_compositional_pca_iteration_one <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){
     # TODO: error checks for structure of x_data
  D <- length(x_data[[1]])  
  # create orthonormal basis
  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)
  
  # initial estimates
  nu <- rep(0, D-1)
  Sigma <- diag(D-1)
  # compute initial pca
  pca <- list()
  eigen_decomp <- eigen(Sigma)
  pca$rotation <- eigen_decomp$vectors
  pca$sdev <- eigen_decomp$values
  pca$center <- nu
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  scores_median_list <- list(length(x_data))
  log_weights_list <- list(length(x_data))

      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_ilr, gr = gradient_cslc_ilr,
                              x_data_i = x_data[[i]], pca = pca, basis_matrix = basis_matrix,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
        scores_median <- as.vector(optim_result$par)
        scores_median_list[[i]] <- scores_median
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*1), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_ilr(scores, x_data[[i]], pca, basis_matrix) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        log_weights_list[[i]] <- log_weights
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)

        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
        # if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
        #   print("non finite weights for observation: ", i)
        #   # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        # }
      }
      # if (any(!is.finite(weights[[i]]))) {
      #     print("non finite weights for observation: ", i)
      #     # print(weights[[i]])
      #    # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      # }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      return(list("mu_scores" = mu_scores, "proposal_scores" = proposal_scores, "weights" = weights, "log_weights" = log_weights_list,
      "scores_median" = scores_median_list, "pca" = pca, "x_data" = x_data))
    }
  # normalize result
  # constant <- apply(pca$rotation, 2,  function(g){
  #   L_2_norm(cbind(x_grid, g))/(max(x_grid) - min(x_grid))
  # })
  # pca$rotation <- t(t(pca$rotation)/constant)
  # pca$sdev <- pca$sdev*constant
test <- fit_compositional_pca_iteration_one(count_data)

proposal_scores <- test$proposal_scores
# check weights
weights <- test$weights
scores <- proposal_scores[[1]][,1]
x_data_i <- count_data[[1]]
pca <- test$pca
conditional_scores_log_ilr(scores, x_data[[i]], pca, basis_matrix)
log_weights <- test$log_weights


      mu_scores_bb <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }), na.rm = TRUE)

i <- 1
proposal_scores[[i]]%*%weights[[i]]

test_matrix <- sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      })
(test_row <- mean(test_matrix[,1]))
alternative <- rowMeans(test_matrix, na.rm = TRUE)

Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
  Reduce("+", lapply(1:(10*1), function(t){
    C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores_bb)%*%
      t((proposal_scores[[i]][,t] - mu_scores_bb))
  }))
}))/length(weights)

# identify NaN
# which(is.nan(weights), arr.ind = TRUE)
Na_list <- which(sapply(weights, function(x) any(is.nan(x))))
length(Na_list)
# I could replace NaN with 1 
sapply(Na_list, function(i) weights[[i]])
# => direct replacement
for(i in seq_along(weights)) {
  weights[[i]][is.nan(weights[[i]])] <- 1
}
Sigma

stabilize_weights <- function(log_weights) {
    # Find maximum log weight
    max_log_weight <- max(log_weights)
    
    # Subtract maximum and exponentiate
    stable_weights <- exp(log_weights - max_log_weight)
    
    # Normalize
    stable_weights / sum(stable_weights)
}

```


```{r testrun vs 1, eval=FALSE}
fit_compositional_pca_iteration_one_vs1 <- function(x_data,
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            eps = 0.01){

    D <- length(x_data[[1]])
  nu <- rep(0, D)
  Sigma <- diag(D)
  # compute initial pca
  pca <- prcomp(Sigma)
  pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
  pca$center <- nu
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  scores_median_list <- list(length(x_data))
  log_weights_list <- list(length(x_data))
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check to analyse `vmmin` is not finite 
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_clr_composition, gr = gradient_cslc,
                              x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
        scores_median <- as.vector(optim_result$par)
        scores_median_list[[i]] <- scores_median
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*1), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_clr_composition(scores, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        log_weights_list[[i]] <- log_weights
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
        # if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
        #   print("non finite weights for observation: ", i)
        #   # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        # }
      }
      # if (any(!is.finite(weights[[i]]))) {
      #     print("non finite weights for observation: ", i)
      #     # print(weights[[i]])
      #    # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      # }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      return(list("mu_scores" = mu_scores, "proposal_scores" = proposal_scores, "weights" = weights, "log_weights" = log_weights_list,
      "scores_median" = scores_median_list, "pca" = pca, "x_data" = x_data))
    }
  # normalize result
  # constant <- apply(pca$rotation, 2,  function(g){
  #   L_2_norm(cbind(x_grid, g))/(max(x_grid) - min(x_grid))
  # })
  # pca$rotation <- t(t(pca$rotation)/constant)
  # pca$sdev <- pca$sdev*constant
test2 <- fit_compositional_pca_iteration_one_vs1(count_data)

log_weights_2 <- test2$log_weights
proposal_scores_2 <- test2$proposal_scores
weights_2 <- test2$weights

      mu_scores_bb_2 <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores_2[[i]]%*%weights_2[[i]]
      }))
```



#### weights

With the proposed scores, we calculate the weights:

$\omega_{i t}=\frac{p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)}{p_{i}^{*}\left(\boldsymbol{\theta}_{i}^{(t)}\right)}$ for all $i=1, \ldots, n$

```{r check objective function, eval=FALSE}

density_pca <- tar_read(density_pca)
# weights are calculated for r draws from every iteration
log_weights <- density_pca$log_weights_original
# then they are centered
log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
# and normalized
weights_1 <- exp(log_weights)/sum(exp(log_weights))
plot(weights_1)
```

That is more or less what we want to see.

```{r check objective function 3, eval=FALSE}
# weights are calculated for r draws from every iteration
(log_weights <- compositional_latent_pca$log_weights)
# then they are centered
log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
# and normalized
weights_1 <- exp(log_weights)/sum(exp(log_weights))
plot(weights_1)
```

We can see that the scale of the original weights is much bigger than in the continious case.
When we compare the weights, then the difference in scale seems to be the biggest issue. Their distribution
seems to be fine.

#### Construction of Sigma

$\boldsymbol{\Sigma}^{(h+1)} = \frac{1}{\sum_{i=1}^{n}} \sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}(\boldsymbol{\theta}{i}^{(t)}-\boldsymbol{\nu}^{(h+1)})(\boldsymbol{\theta}{i}^{(t)}-\boldsymbol{\nu}^{(h+1)})^{T}$

## Strategies

### Scaling approach

This approach scales the likelihood part to reduce its influence. That helps the algorithm to converge but leads
to the problem that now the prior dominates the posterior.

```{r updated functions scaling approach, eval=TRUE}
conditional_scores_log_ilr_sc <- function(scores, x_data_i, pca, basis_matrix) {
    # Scale the data to moderate sample size
    # scaling_factor <- 1/sum(x_data_i)
    scaling_factor <- 0.0001
    
    ilr_comp <- pca$center + pca$rotation%*%scores
    clr_comp <- t(ilr_comp)%*%basis_matrix
    norm_constant <- sum(exp(clr_comp))
    
    # Compute scaled log likelihood
    log_likelihood <- scaling_factor * (sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant))
    
    # Prior remains unchanged as it's already well-scaled
    log_prior <- - sum(0.5*scores^2/(pca$sdev^2))
    
    return(log_likelihood + log_prior)
}

gradient_cslc_ilr_sc <- function(scores, x_data_i, pca, basis_matrix) {
    # scaling_factor <- 1/log(sum(x_data_i))
    scaling_factor <- 0.0001
    m_i <- sum(x_data_i)

    ilr_comp <- pca$center + pca$rotation%*%scores
    clr_comp <- t(ilr_comp)%*%basis_matrix
    composition <- clrInv(clr_comp)

    grad <- sapply(seq_along(scores), function(k) {
        e_k <- basis_matrix[k, ]
        term1 <- sum(x_data_i * e_k)
        term2 <- m_i * sum(composition * e_k)
        grad_k <- scaling_factor * (term1 - term2) - scores[k] / (pca$sdev[k]^2)
        return(grad_k)
    })

    return(grad)
}


fit_compositional_pca_ilr_sc <- function(x_data,
                                max_iter = 50,
                                r = 10,
                                lambda = 1,
                                eps = 0.01){
  start_time <- Sys.time()
  # TODO: error checks for structure of x_data
  D <- length(x_data[[1]])  
  # create orthonormal basis
  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)
  
  # initial estimates
  nu <- rep(0, D-1)
  Sigma <- diag(D-1)
  # compute initial pca
  pca <- list()
  eigen_decomp <- eigen(Sigma)
  pca$rotation <- eigen_decomp$vectors
  pca$sdev <- eigen_decomp$values
  # center needs to be a column vector
  pca$center <- matrix(nu, ncol = 1)
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  conditional_scores_list <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      cat("Iteration:", k, "\n")
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check 
        # check_optim_inputs(pca, x_data[[i]])
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_ilr_sc, gr = gradient_cslc_ilr_sc,
                              x_data_i = x_data[[i]], pca = pca, basis_matrix = basis_matrix,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            print("optim result:")
            print(optim_result)
            print("pca center:")
            print(pca$center)
        })
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        # check value of the conditional distribution
        conditional_scores_list[[i]] <- conditional_scores_log_ilr_sc(scores, x_data[[i]], pca, basis_matrix) 

        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_ilr_sc(scores, x_data[[i]], pca, basis_matrix) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        weights[[i]] <- stabilize_weights(log_weights)
      }
      monitor_global_ess(weights, k)
      mean_conditional <- mean(unlist(conditional_scores_list), na.rm = TRUE)
          cat(sprintf("Conditional score mean value %.2f:\n", 
                mean_conditional))

      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }), na.rm = TRUE)
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      # if (any(!is.finite(pca$sdev)) || any(pca$sdev <= 0)) {
      #   print("pca$sdev:")
      #   print(pca$sdev)
      #   stop(paste("Fehler in Iteration", k, ": pca$sdev enthält nicht-finite oder nicht-positive Werte."))
      # }
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      # pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value 1:", critical_value_1, "\n")
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")
      cat("critical value 2:", critical_value_2, "\n")            

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data, "time" = elapsed_time))
}

```

### Normierung der Counts

```{r norm counts, eval=FALSE}
count_data_scaled <- lapply(count_data, function(x) x/sum(x) * mean(sapply(count_data, sum)))
```

That does not appear to be super helpful

### Adjustment with a constant offset

log_posterior <- log_likelihood + κ * log_prior

### Log-Sum_exp-Trick

```{r log sum exp trick, eval=FALSE}
# What happens with the log-sum-Exp Trick?
max_log_weight <- max(log_weights)
# we substract the maximum from each weight and then take the exponential
weights <- exp(log_weights - max_log_weight)
weights <- weights / sum(weights)
plot(weights)
```

That still does not help, because the scale of the weights is to high. **Check our specifications**

Der Trick funktioniert, weil:
 $$ \log\sum_{i=1}^n \exp(x_i) = \max_i(x_i) + \log\sum_{i=1}^n \exp(x_i - \max_i(x_i)) $$
Die Subtraktion des Maximums die Exponenten in einen numerisch stabilen Bereich bringt
Die relative Größenordnung der Gewichte erhalten bleibt
Die Normalisierung weiterhin korrekte Wahrscheinlichkeiten liefert

Literaturhinweise:

Murphy, Kevin P. (2012): "Machine Learning: A Probabilistic Perspective", Kapitel 8.2.7
Bishop, Christopher M. (2006): "Pattern Recognition and Machine Learning", Abschnitt 11.6
Robert, Christian P. und Casella, George (2004): "Monte Carlo Statistical Methods", Kapitel 3
gress even though the weights are off.

### Numerical optimisation

The algorithm failes at two points:

1. non-finite values in Sigma
2. optim error

One reason is, that the conditional score function produces values of a very large scale, which leads to infinite values in the weights.
There are several options to solve that:

1. Offset into the conditional score function -> control the scale

The optim error happens earlier, that is with infinite values inside the opzimisation.

#### Rmpfr

Erhöhung der numerischen Präzision.

either from the above web pages, or much more conveniently from your Linux distribution package system:

    Debian, Ubuntu, etc . . . . . . . .: sudo apt-get install libmpfr-dev
    Fedora, Redhat, CentOS, (open)SuSE: sudo dnf install mpfr-devel

Rmpfr provides S4 classes and methods for arithmetic including transcendental ("special") functions for arbitrary precision floating point numbers. To this end, it interfaces to the LGPL ed MPFR (Multiple Precision Floating-Point Reliable) Library which itself is based on the GMP (GNU Multiple Precision) Library. 

### Eigendecomposition Sigma

The algorithm runs, but sometimes we get 
Error in eigen(Sigma) : infinite or missing values in 'x'
How can we avoid that and what is the cause for that?

```{r eigen error, eval=TRUE}
eigen
```

That obiously happens when $\Sigma$ is not finite.
`
    if (!all(is.finite(x))) 
        stop("infinite or missing values in 'x'")
`


It seems like the error message comes from a lower level function.

### Error coding

Implement error checks, like:

```{r error check, eval=FALSE}
Sigma <- tryCatch({
  # Original Sigma calculation
  Reduce("+", lapply(seq_along(weights), function(i){
    Reduce("+", lapply(1:(r*k), function(t){
      weights[[i]][t] * (proposal_scores[[i]][,t] - mu_scores) %*%
        t(proposal_scores[[i]][,t] - mu_scores)
    }))
  })) / length(weights)
}, error = function(e) {
  stop(paste("Error in computing Sigma at iteration", k, ":", e$message))
})
```

# Simulation settings

We can construct our simulation settings by building the Covraiance matrix of the multivariate process:

```{r simulation settings, eval=FALSE}
library(mvtnorm)
D <- 13  # Dimension der Komposition
basis_vectors <- lapply(1:(D-1), generate_orthonormal_basis, D)
V <- do.call(rbind, basis_vectors)

# Definiere Eigenwerte 
eigenvalues <- c(0.3, 0.21, 0.17, rep(0.05, D-4), 0)
# Konstruiere Kovarianzmatrix
Sigma <- V %*% diag(eigenvalues) %*% t(V)

n_samples <- 100
ilr_coords <- rmvnorm(n_samples, mean = rep(0,D-1), sigma = Sigma)
clr_coords <- ilr2clr(ilr_coords)

pca_std <- princomp(clr_coords)
plot_pca_rotation(pca_std$loadings)

coords <- clrInv(clr_coords)


# change matrix into list
coords_list <- apply(coords, 1, function(x) x, simplify = FALSE)

n_counts <- 500
# simulate multinomial data
x_data <- unlist(lapply(1:n_samples, function(i) {
  probs <- coords_list[[i]]
  samples <- t(rmultinom(1, n_counts, probs))
  lapply(1:nrow(samples), function(j) samples[j,])
}), recursive = FALSE)

pca_mcem <-fit_compositional_pca_ilr(x_data, max_iter = 40, r = 10, lambda = 1, eps = 0.01)
plot_pca_rotation(pca_mcem$pca$rotation)


compositions <- apply(clr_coords, 1, clrInv)
```

It seems that sample size matters pretty hard. For D=13 and m_i = 500 the algorithm does not converge with the standard settings.

```{r simulation settings 2, eval=FALSE}
n_counts <- 2000
# simulate multinomial data
x_data <- unlist(lapply(1:n_samples, function(i) {
  probs <- coords_list[[i]]
  samples <- t(rmultinom(1, n_counts, probs))
  lapply(1:nrow(samples), function(j) samples[j,])
}), recursive = FALSE)

pca_mcem <-fit_compositional_pca_ilr_bb(x_data, max_iter = 40, r = 10, lambda = 1, eps = 0.01)
plot_pca_rotation(pca_mcem$pca$rotation)


compositions <- apply(clr_coords, 1, clrInv)
```

## adjust MCEM


```{r ilr implementation, eval=TRUE}

fit_compositional_pca_ilr_bb <- function(x_data,
                                max_iter = 50,
                                r = 10,
                                lambda = 1,
                                eps = 0.01){
  start_time <- Sys.time()

  # TODO: error checks for structure of x_data
  D <- length(x_data[[1]])  
  # create orthonormal basis
  basis_vectors <- lapply(1:(D - 1), generate_orthonormal_basis, D)
  basis_matrix <- do.call(rbind, basis_vectors)
  
  # initial estimates
  nu <- rep(0, D-1)
  Sigma <- diag(D-1)
  # compute initial pca
  pca <- list()
  eigen_decomp <- eigen(Sigma)
  pca$rotation <- eigen_decomp$vectors
  pca$sdev <- eigen_decomp$values
  # center needs to be a column vector
  pca$center <- matrix(nu, ncol = 1)
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  sdev_list <- list(length(max_iter))
  center_list <- list(length(max_iter))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # decrease the spread of the proposal distribution to get more stabil weights
      lambda <- min(1, 1/sqrt(k)) 

      cat("Iteration:", k, "\n")
      # E-Step ###################
      for(i in 1:length(x_data)){
        # error check 
        # check_optim_inputs(pca, x_data[[i]])
        optim_result <- tryCatch({
          optim(rep(0, length = length(pca$sdev)), conditional_scores_log_ilr, gr = gradient_cslc_ilr,
                              x_data_i = x_data[[i]], pca = pca, basis_matrix = basis_matrix,
                              control = list(fnscale = -1), method = "BFGS")
        }, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            print("optim result:")
            print(optim_result)
            print("pca center:")
            print(pca$center)
        })
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_ilr(scores, x_data[[i]], pca, basis_matrix) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        weights[[i]] <- stabilize_weights(log_weights)
        # if (any(!is.finite(exp(log_weights)/sum(exp(log_weights))))) {
        #   print("non finite weights for observation: ", i)
        #   # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
        # }
      }
      monitor_weights(weights, k)
      # if (any(!is.finite(weights[[i]]))) {
      #     print("non finite weights for observation: ", i)
      #     # print(weights[[i]])
      #    # stop(paste("Infinite or NaN values found in weights at iteration", k, "for observation", i))
      # }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }), na.rm = TRUE)
      # update parameters
      pca_old <- pca
      pca$center <- pca$rotation%*%mu_scores
      center_list[[k]] <- pca$center
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      eigen_decomp <-  tryCatch({eigen(Sigma)}, error = function(e) {
            cat("Fehler bei optim() in Iteration", k, "für Beobachtung", i, "\n")
            cat("Fehlermeldung:", e$message, "\n")
            print("pca$sdev:")
            print(pca$sdev)
            # print("scores_median:")
            # print(scores_median)
        })
      # error check eigenvalues > 0
      negative_eigenvalues <- eigen_decomp$values < 0
        if (any(negative_eigenvalues)) {
        warning(sprintf("Warning: %d eigenvalues are negative. They have been set to zero.", sum(negative_eigenvalues)))
        }
      pca$sdev <- sqrt(pmax(eigen_decomp$values, 0))
      cat("Eigenvalues:", pca$sdev, "\n")
      sdev_list[[k]] <- pca$sdev
      # if (any(!is.finite(pca$sdev)) || any(pca$sdev <= 0)) {
      #   print("pca$sdev:")
      #   print(pca$sdev)
      #   stop(paste("Fehler in Iteration", k, ": pca$sdev enthält nicht-finite oder nicht-positive Werte."))
      # }
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      # pca$rotation <- apply(pca$rotation, 2, function(g) g - mean(g))
      
      # check convergence
      critical_value_1 <- sqrt(sum((pca_old$center - pca$center)^2))
      cat("critical value 1:", critical_value_1, "\n")
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      K_diff <- K_old - K_new
      # Berechne die Frobenius-Norm der Differenzmatrix
      critical_value_2 <- norm(K_diff, type = "F")
      cat("critical value 2:", critical_value_2, "\n")            

      if(max(critical_value_1, critical_value_2) < eps){
        constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        
        end_time <- Sys.time()
        elapsed_time <- end_time - start_time
        print(paste("The algorithm converged after:", elapsed_time, "seconds"))
        return(list("iteration" = k, "pca" = pca, "x_data" = x_data))
      }
    }
  }
  constant <- apply(pca$rotation, 2, function(g) { sqrt(sum(g^2)) })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  end_time <- Sys.time()
  elapsed_time <- end_time - start_time
  return(list("iteration" = max_iter, "pca" = pca, "x_data" = x_data, "time" = elapsed_time))
}

```

## Lambda adjustment

A decreasing lambda leads to a better convergence:

```{r lambda_adjustment, echo=FALSE}
pca_mcem_lambda <-fit_compositional_pca_ilr_bb(x_data, max_iter = 40, r = 10, lambda = 1, eps = 0.01)
plot_pca_rotation(pca_mcem_lambda$pca$rotation)

```



# Final Test
