---
title: "Simulation study CoDa"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    code_folding: hide
---


# Simulation study Greven

Get all relevant functions: (To avoid unneccassary system calls, all functions and results are stored in targets and are not evaluated here)

```{r load libraries, eval=TRUE}
library(targets)
library(ggplot2)
library(compositions)
```


```{r fit density pca, eval=FALSE}
fit_density_pca <- function(x_data, x_grid = seq(min(unlist(x_data)), max(unlist(x_data)), length = 200),
                            max_iter = 50, r = 10, lambda = 1, dim_reduction = 0.001,
                            bw = (max(x_grid) - min(x_grid))/10, eps = 0.01){
  # initial estimates
  # kernel density estimates
  densities_estimated <- lapply(1:length(x_data), function(i){
    density <- density(x_data[[i]], from = min(x_grid), to = max(x_grid), 
                       kernel = "gaussian", bw, 
                       n = length(x_grid))
    data.frame("x" = density$x, "y" = density$y)
  })
  # compute initial pca
  clr_densities_estimated <- lapply(densities_estimated, clr_trafo)
  clr_densities <- do.call("rbind", sapply(clr_densities_estimated, '[', 2))
  pca <- prcomp(na.omit(clr_densities))
  which_reduced <- rev(cumsum(rev(pca$sdev^2))/sum(pca$sdev^2) > dim_reduction)
  which_reduced <- which_reduced|c(TRUE, TRUE, rep(FALSE, length(which_reduced) - 2))
  pca$sdev <- pca$sdev[which_reduced]
  pca$rotation <- pca$rotation[,which_reduced, drop = FALSE]
  
  proposal_scores <- list(length(x_data))
  weights <- list(length(x_data))
  if(max_iter > 0){
    for(k in 1:max_iter){
      # E-Step ###################
      # draw densities conditional on observations and current pca
      for(i in 1:nrow(clr_densities)){
        # find median of the posterior score distribution
        optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                              x_grid = x_grid, x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
        scores_median <- as.vector(optim_result$par)
        # importance sampling
        proposal_scores[[i]] <- sapply(1:(r*k), function(t){
          matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
        })
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
        # increase numerical stability
        log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
        weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
      }
      # M-Step ###################
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
      
      # update pca
      pca_old <- pca
      pca$center <- center_function(cbind(x_grid, pca$center + pca$rotation%*%mu_scores))[,2]
      
      Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
        Reduce("+", lapply(1:(r*k), function(t){
          C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
            t((proposal_scores[[i]][,t] - mu_scores))
        }))
      }))/length(weights)
      eigen_decomp <- eigen(Sigma)
      pca$sdev <- sqrt(eigen_decomp$values)
      pca$rotation <- pca$rotation%*%eigen_decomp$vectors
      pca$rotation <- apply(pca$rotation, 2, function(g) center_function(cbind(x_grid, g))[,2])
      
      # check convergence
      critical_value_1 <- L_2_norm(cbind(x_grid, pca_old$center - pca$center))
      K_old <- Reduce("+", lapply(seq_along(pca_old$sdev), function(k){
        pca_old$rotation[,k]%*%t(pca_old$rotation[,k])*(pca_old$sdev[k]^2)
      }))
      K_new <- Reduce("+", lapply(seq_along(pca$sdev), function(k){
        pca$rotation[,k]%*%t(pca$rotation[,k])*(pca$sdev[k]^2)
      }))
      critical_value_2 <- L_2_norm(cbind(x_grid, sapply(1:nrow(K_old), function(k){
        L_2_norm(cbind(x_grid, K_old[k,] - K_new[k,]))
      })))
      
      if(max(critical_value_1, critical_value_2) < eps){
        # normalize result
        constant <- apply(pca$rotation, 2,  function(g){
          L_2_norm(cbind(x_grid, g))
        })
        pca$rotation <- t(t(pca$rotation)/constant)
        pca$sdev <- pca$sdev*constant
        return(list("iteration" = k, "pca" = pca, "x_grid" = x_grid, "x_data" = x_data))
      }
      which_reduced <- rev(cumsum(rev(pca$sdev^2))/sum(pca$sdev^2) > dim_reduction)
      which_reduced <- which_reduced|c(TRUE, TRUE, rep(FALSE, length(which_reduced) - 2))
      pca$sdev <- pca$sdev[which_reduced]
      pca$rotation <- pca$rotation[,which_reduced, drop = FALSE]
    }
  }
  # normalize result
  constant <- apply(pca$rotation, 2,  function(g){
    L_2_norm(cbind(x_grid, g))/(max(x_grid) - min(x_grid))
  })
  pca$rotation <- t(t(pca$rotation)/constant)
  pca$sdev <- pca$sdev*constant
  return(list("iteration" = max_iter, "pca" = pca, "x_grid" = x_grid, "x_data" = x_data))
}

################################################################################
# objective function and gradient
conditional_scores_log_density <- function(scores, x_grid, x_data_i, pca){
  clr_density <- cbind(x_grid, pca$center + pca$rotation%*%scores)
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  
  sum(clr_density[idxs, 2]) - length(idxs)*log(f_integral) - sum(0.5*scores^2/(pca$sdev^2))
}

gradient_csld <- function(scores, x_grid, x_data_i, pca){
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  density <- inverse_clr_trafo(cbind(x_grid, pca$center + pca$rotation%*%scores))
  
  sapply(seq_along(scores), function(k){
    scalar_prod <- sum(density[,2]*pca$rotation[, k]*diff(mid_points))
    sum(pca$rotation[idxs, k]) - length(idxs)*scalar_prod - scores[k]/(pca$sdev[k]^2)
  })
}

################################################################################
# helper functions
center_function <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  f_integral <- sum(f_data[,2]*diff(mid_points))
  f_data[,2] <- f_data[,2] - f_integral/(mid_points[length(mid_points)] - mid_points[1])
  f_data
}
L_2_norm <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  sqrt(sum(f_data[,2]^2*diff(mid_points)))
}

clr_trafo <- function(f_data){
  f_data[,2] <- log(f_data[,2])
  center_function(f_data)
}

inverse_clr_trafo <- function(clr_density){
  mid_points <- c(clr_density[1, 1], clr_density[-1, 1] - 0.5*diff(clr_density[,1]),
                  clr_density[nrow(clr_density),1])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

predict_latent_densities <- function(density_pca){
  predicted_scores <- sapply(seq_along(density_pca$x_data), function(i){
    optim_result <- optim(rep(0, length = length(density_pca$pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                          x_grid = density_pca$x_grid, x_data_i = density_pca$x_data[[i]], pca = density_pca$pca,
                          control = list(fnscale = -1), method = "BFGS")
    as.vector(optim_result$par)
  })
  clr_densities <- density_pca$pca$center + density_pca$pca$rotation%*%predicted_scores
  return(list("clr_densities" = clr_densities, "x_grid" = density_pca$x_grid, "predicted_scores" = predicted_scores))
}

################################################################################
# plot functions

# plot all functional principal components
plot_pca <- function(pca, x_grid, dim = 2){
  pca_mean <- data.frame("x" = x_grid, "y" = pca$center)
  pcs <- lapply(1:dim, function(i){
    data.frame("x" = x_grid, "y" = pca$rotation[,i])
  })
  y_lim <- range(rbind(pca_mean, do.call("rbind", pcs))[,2])
  plot(pca_mean, ylim = y_lim, type = "l", main = "Principal component decomposition")
  invisible(lapply(1:dim, function(i) lines(pcs[[i]], col = rainbow(dim)[i])))
  legend("bottom", legend = c("mean", paste("pc", 1:dim)), col = c("black", rainbow(dim)),
         lty = 1)
}

# obtain data to plot effect of one principal component on mean density
get_predicted_densities <- function(density_pca, idx = 1, fac = 0.2){
  clr_mean <- density_pca$pca$center
  clr_mean_minus <- clr_mean - fac*density_pca$pca$rotation[,idx]
  clr_mean_plus <- clr_mean + fac*density_pca$pca$rotation[,idx]
  
}
```

Repeat the simulations from the paper.

Build the latent densities with the given formulars and compare with Figure 5: oracle: $g(x)$

![True Principal Components](Greven_Figure5.png)

There are several parameters that can be tweaked:
- `x_grid`: grid on which the latent processes are defined
- `m_i`: number of samples to draw from the latent densities
- `n`: number of latent densities
- `max_iter`: number of iterations the algorithm is allowed to run

```{r simulation part 1, eval=FALSE}
# x_grid <- seq(-5,5,0.1)
x_grid <- seq(0,1,0.001)
# f_data <- data.frame("x" = x_grid, "y" = -0.2*x_grid^2)
f_data <- data.frame("x" = x_grid, "y" = -20*(x_grid-0.5)^2+5/3)
clr_mean <- center_function(f_data)

# pc_1 <- data.frame("x" = x_grid, "y" = 2*sin(x_grid))
pc_1 <- data.frame("x" = x_grid, "y" = -0.2*sin(10*(x_grid-0.5)))
pc_1 <- center_function(pc_1)
pc_1[,2] <- pc_1[,2]/L_2_norm(pc_1)
#####
# pc_2 <- data.frame("x" = x_grid, "y" = cos(0.3*x_grid))
pc_2 <- data.frame("x" = x_grid, "y" = 0.1*cos(2*pi*(x_grid-0.5)))
pc_2 <- center_function(pc_2)
pc_2[,2] <- pc_2[,2]/L_2_norm(pc_2)
lambda_1 <- 0.5
lambda_2 <- 0.2
```

```{r simulation target 2, eval=TRUE}
simulation_densities_results <- tar_read(simulation_densities_greven)
clr_mean <- simulation_densities_results$clr_mean
pc_1 <- simulation_densities_results$pc_1
pc_2 <- simulation_densities_results$pc_2
x_grid <- simulation_densities_results$x_grid
```

Plot the latent processes (this should be recovered by the PCA)

```{r plot latent densities, eval=TRUE}

densities <- list(
  mean = clr_mean[,2],
  pc_1 = pc_1[,2],
  pc_2 = pc_2[,2]
)

plot_data_comp <- data.frame(
  x = rep(x_grid, times = length(densities)),
  y = unlist(densities),
  line = rep(names(densities), each = length(x_grid))
)
ggplot(plot_data_comp, aes(x = x, y = y, color = line)) +
  geom_line() +
  labs(x = "x", y = "Value", color = "Line") +
  ggtitle("True Principal Components") +
  ylim(c(-2,2)) +
  theme_minimal()

```

This gives a good representation of Greven's Figure 5: oracle: $g(x)$

Now we create our stochhastic densities $f_i$ and $g_i$ and draw $m_i$ samples from them.

```{r simulation part 2, eval=FALSE}
# true densities
set.seed(12)
n_data <- 30
true_observed_clr_densities <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})
true_observed_densities <- lapply(1:n_data, function(i){
  clr_density <- data.frame(x_grid, true_observed_clr_densities[,i])
  inverse_clr_trafo(clr_density)
})
######################################
# draw data
x_data <- lapply(1:n_data, function(i){
  probs <- true_observed_densities[[i]][,2]
  x_grid <- true_observed_densities[[i]][,1]
  sample(x_grid, 40, replace = TRUE, prob = probs)
})
```

```{r simulation targets 2, eval=TRUE}
x_data <- simulation_densities_results$x_data
```



Finally, we perform the PCA with latent densities and plot the results:

```{r density estimation, eval=FALSE} 
# estimate density pca
density_pca <- fit_density_pca(x_data, max_iter = 50)

plot_pca(density_pca$pca, x_grid = density_pca$x_grid)

density_pca$iteration
```

```{r targets density estimation, eval=TRUE}
density_pca <- tar_read(density_pca)

plt <- tar_read(plot_pca_results)
plt()
# Use base-R-plotting
# par(mar = c(5, 4, 4, 2) + 0.1)
# plt

# print(format("Number of iterations: ",density_pca$iteration))
print(paste("Number of iterations:", density_pca$iteration))
```

Additional plots:

1. Observed (sampled) densities

```{r plot true observed densities, eval=TRUE}
true_observed_densities <- simulation_densities_results$true_observed_densities

plot_data_densities <- data.frame(
  x = rep(x_grid, times = length(true_observed_densities)),
  y = unlist(lapply(true_observed_densities, function(df) df[, 2])),
  line = as.factor(rep(seq_along(true_observed_densities), each = length(x_grid)))
)
ggplot(plot_data_densities, aes(x = x, y = y, color = line)) +
  geom_line() +
  labs(x = "x", y = "Value", color = "Line") +
  ggtitle("Observed Densities") +
  theme_minimal()

```

## fit density pca function

To get a better understanding of the function `fit_density_pca` we will go through it step by step.

1. Initial values are calculated on $\hat{\boldsymbol{f}}_{i}$, i.e. the kernel density estimates of the observed data.
2. The clr transformed densities are calculated: $\hat{\boldsymbol{g}}_{1}, \dots, \hat{\boldsymbol{g}}_{n}$ given 
$$
\begin{equation*}
\operatorname{clr}^{-1}(g)=\frac{\exp (g)}{\int_{I} \exp (g(x)) d x} \tag{4}
\end{equation*}
$$
3. Estimate $\boldsymbol{\nu}^{(0)}$ and $\boldsymbol{\Sigma}^{(0)}$ as the empirical mean and covariance of the coefficients $\boldsymbol{\theta}_{1} \ldots, \boldsymbol{\theta}_{n}$ of $\hat{g}_{1}, \ldots, \hat{g}_{n}$, respectively. That is 
conducting a PCA on the clr transformed densities.

```{r initial values, eval=TRUE}

center_function <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  f_integral <- sum(f_data[,2]*diff(mid_points))
  f_data[,2] <- f_data[,2] - f_integral/(mid_points[length(mid_points)] - mid_points[1])
  f_data
}

L_2_norm <- function(f_data){
  mid_points <- c(f_data[1,1], f_data[-1,1] - 0.5*diff(f_data[,1]), f_data[nrow(f_data),1])
  sqrt(sum(f_data[,2]^2*diff(mid_points)))
}

clr_trafo <- function(f_data){
  f_data[,2] <- log(f_data[,2])
  center_function(f_data)
}

inverse_clr_trafo <- function(clr_density){
  mid_points <- c(clr_density[1, 1], clr_density[-1, 1] - 0.5*diff(clr_density[,1]),
                  clr_density[nrow(clr_density),1])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

# set values as in fit_density_pca
x_grid = seq(min(unlist(x_data)), max(unlist(x_data)), length = 200)
bw = (max(x_grid) - min(x_grid))/10
  # kernel density estimates
  densities_estimated <- lapply(1:length(x_data), function(i){
    density <- density(x_data[[i]], from = min(x_grid), to = max(x_grid), 
                       kernel = "gaussian", bw, 
                       n = length(x_grid))
    data.frame("x" = density$x, "y" = density$y)
  })

clr_densities_estimated <- lapply(densities_estimated, clr_trafo)
clr_densities <- do.call("rbind", sapply(clr_densities_estimated, '[', 2))
pca <- prcomp(na.omit(clr_densities))
```

### Expectation step: Importance Sampling

We need to work trough an *importance sampling* Algorithm (see Monte Calro Methods). This enables us to calculate a weighted maximum likelihood estimate of the parameters $/nu$ and $/sigma$.

The first step is to find the mode of the posterior score distributen. We do this by maximizing the posterior score function: 
$$
\begin{align*}
& p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) \propto p\left(\mathbf{x}_{i} \mid \mathbf{z}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) p\left(\mathbf{z}_{i} \mid \boldsymbol{\Sigma}^{(h)}\right)=p\left(\mathbf{x}_{i} \mid \boldsymbol{\theta}_{i}=\boldsymbol{V}^{(h) T} \mathbf{z}_{i}+\boldsymbol{\nu}^{(h)}\right) \prod_{k=1}^{N} p\left(z_{i k} \mid \sigma_{k}^{2}{ }^{(h)}\right) \\
& =\prod_{j=1}^{m_{i}} \operatorname{clr}^{-1}\left(\sum_{k=1}^{N} \nu_{k}^{(h)} e_{k}+\boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}\right)\left(x_{i j}\right) \prod_{k=1}^{N} p\left(z_{i k} \mid \sigma_{k}^{2(h)}\right) \\
& =\frac{\exp \left(\sum_{j=1}^{m_{i}}\left(\mu^{(h)}\left(x_{i j}\right)+\sum_{k=1}^{N} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}\left(x_{i j}\right)\right)\right)}{\left(\int_{I} \exp \left(\mu^{(h)}(x)+\sum_{k=1}^{N} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}(x)\right) d x\right)^{m_{i}}} \prod_{k=1}^{N} p\left(z_{i k} \mid \sigma_{k}^{2^{(h)}}\right) \tag{9}
\end{align*}
$$

for each estimated density $i$ for $i = 1, \dots, n$.

Given the mode  $\mathbf{z}_{i}^{*}=\operatorname{argmax}_{\mathbf{z}_{i} \in \mathbb{R}^{N}} p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)$, we can calculate the weights with:

$\omega_{i t}, t=1, \ldots, r$ given as $\omega_{i t}=\frac{p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)}{p_{i}^{*}\left(\boldsymbol{\theta}_{i}^{(t)}\right)}$ for all $i=1, \ldots, n$. 

to approximate the conditional expectation: 
$$
\begin{equation*}
\mathbb{E}\left(\log \left(p\left(\boldsymbol{\theta}_{i} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right)\right) \approx \sum_{t=1}^{r} \frac{\omega_{i t}}{\sum_{t=1}^{r} \omega_{i t}} \log \left(p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right) \tag{8}
\end{equation*}
$$

The mode $\mathbf{z}_{i}^{*}$ enables us to draw from $\mathcal{N}\left(\boldsymbol{z}_{i}^{*}, \lambda \operatorname{diag}\left(\sigma_{1}^{2(h)}, \ldots, \sigma_{N}^{2}{ }^{(h)}\right)\right)$ which represents the auxiliary distribution of 
the importance sampling formular in (8).

The $r*t$ samples from  $p_{i}^{*}\left(\boldsymbol{z}_{i}\right)$ enable us to calculate the weights **$\omega_{i t}$**.
In the following code this is the $Nx(r*t)$ matrix `scores`.
```{r expectation step, eval=TRUE}
# set iteration == 1
k <- 1
# parameter to increase (why?) the number of samples in the importance sampling for each iteration
r = 10
# parameter to restrict the covariance space
lambda = 1

proposal_scores <- list(length(x_data))
weights <- list(length(x_data))

inverse_clr_trafo <- function(clr_density){
  mid_points <- c(clr_density[1, 1], clr_density[-1, 1] - 0.5*diff(clr_density[,1]),
                  clr_density[nrow(clr_density),1])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

# objective function
conditional_scores_log_density <- function(scores, x_grid, x_data_i, pca){
  clr_density <- cbind(x_grid, pca$center + pca$rotation%*%scores)
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  
  sum(clr_density[idxs, 2]) - length(idxs)*log(f_integral) - sum(0.5*scores^2/(pca$sdev^2))
}

# gradient
gradient_csld <- function(scores, x_grid, x_data_i, pca){
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  density <- inverse_clr_trafo(cbind(x_grid, pca$center + pca$rotation%*%scores))
  
  sapply(seq_along(scores), function(k){
    scalar_prod <- sum(density[,2]*pca$rotation[, k]*diff(mid_points))
    sum(pca$rotation[idxs, k]) - length(idxs)*scalar_prod - scores[k]/(pca$sdev[k]^2)
  })
}

for(i in 1:nrow(clr_densities)){
  optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                              x_grid = x_grid, x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
  scores_median <- as.vector(optim_result$par)
  # importance sampling
  proposal_scores[[i]] <- sapply(1:(r*k), function(t){
    matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
  })
  log_weights <- apply(proposal_scores[[i]], 2, function(scores){
    conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
      sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
  })
  # increase numerical stability
  log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
  weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
}

```

#### Conditional score function

This function includes the information given un the observations $x_{i}$ and the current parameters $\nu$ and $\Sigma$.

To better understand what is happening, lets digest the data. First, what is our observation $x_{i}$?

- 40 samples from $f_i$ on the Intervall $[0,1]$

We can use Kernel estimation to estimate the density of $f_i$ given the observations $x_{i}$:

```{r plot_data x_i, eval=TRUE}
summary(x_data[[1]])

density_1 <- density(x_data[[1]], from = min(x_grid), to = max(x_grid), 
                       kernel = "gaussian", bw,
                       n = length(x_grid))
plot(density_1, main = "Density of x_1")
```

Here each point on the interval has a density $f(x_{i})$ and the density is given by the Kernel function with its integral being equal to 1.

To be able to make calculations on the observed data, we need the functional clr transformation and preserve the grid:

```{r clr data x_i, eval=TRUE}
densities_estimated <- lapply(1:length(x_data), function(i){
  density <- density(x_data[[i]], from = min(x_grid), to = max(x_grid), 
                      kernel = "gaussian", bw, 
                      n = length(x_grid))
  data.frame("x" = density$x, "y" = density$y)
})

clr_densities_estimated <- lapply(densities_estimated, clr_trafo)
clr_densities <- do.call("rbind", sapply(clr_densities_estimated, '[', 2))

clr_density_1 <- clr_densities_estimated[[1]]

plot(clr_density_1, type = "l", main = "Density of clr(x_1)")
```

The clr coordinates (of all observations) are now the basis to evaluate $\mu$ and $\Sigma$ of the stochhastic process:
$$
\begin{equation*}
X_{i j} \stackrel{i . i . d .}{\sim} \operatorname{clr}^{-1}\left(G_{i}\right)=\frac{\exp \left(G_{i}\right)}{\int_{I} \exp \left(G_{i}(x)\right) d x} \quad \text { with } \quad G_{i}=\sum_{k=1}^{N} \theta_{i k} e_{k} \text { and } \boldsymbol{\theta}_{i}=\left(\theta_{i 1}, \ldots, \theta_{i N}\right) \stackrel{i . i . d .}{\sim} \mathcal{N}(\boldsymbol{\nu}, \boldsymbol{\Sigma}) \tag{5}
\end{equation*}
$$

Given the current estimates of $\nu$ and $\Sigma$ we can calculate the conditional distribution of the scores $z_{i}$: $ p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) $

Only that we replace this by the conditional distribution of our observations $x_{i}$ given the scores. To get a better understanding of what this means, we will look at all relevant conditional distributions in this process.

1. we find the mode of the posterior distribution for observation i

```{r mode calculation, eval=TRUE}
optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                        x_grid = x_grid, x_data_i = x_data[[1]], pca = pca,
                        control = list(fnscale = -1), method = "BFGS")
  scores_median <- as.vector(optim_result$par)

length(scores_median)
```

The mode is in  $\mathbb{R}^{n}$ since it is a Projection onto the principal component space given by $\boldsymbol{V}^{(h)}$. It gives the coefficients of the linear combinations of the eigenvectors $\left(\boldsymbol{v}_{1}^{(h)}, \ldots, \boldsymbol{v}_{N}^{(h)}\right)$ 
that approximate $clr(x_i)$ the best given the current parameters $\nu$ and $\Sigma$.

We can check the approximation by 

```{r approximation of x_i, eval=TRUE}

x_1_approx <- pca$center + pca$rotation%*%scores_median

# lässt sich nur an den indecies von x_i auf x_grid sehen
plot(x_1_approx, type = "l", main = "Compare clr-coordinates of x_1 and reconstructed x_1 (first iteration)")
lines(clr_density_1$y, col = "red")
lines(pca$center, col = "green")

```

with every iteration, this approximation should get better. (A problem is that the estimates here are basically zero).

Actually, nothing happens here. Maybe its just a step for constructing the (arbitrary) auxiliary distribution?
That is given that we are looking for the mode of the posterior distribution, nothing much is updated given our observations. 

With the mode, we can sample from $p_{i}^{*}\left(\boldsymbol{z}_{i}\right)$ to be $\mathcal{N}\left(\boldsymbol{z}_{i}^{*}, \lambda \operatorname{diag}\left(\sigma_{1}^{2(h)}, \ldots, \sigma_{N}^{2}{ }^{(h)}\right)\right)$

```{r sample from auxiliary distribution, eval=FALSE}
proposal_scores <- list(length(x_data))
lambda <- 1

proposal_scores[[1]] <- sapply(1:(10*1), function(t){
  matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
})

```

This gives us N by t matrices, i.e. each column is a sample of scores. Those samples are the 
"points" at which the density functions are evaluated. With that we can calculate our weights.

That is first of all, the conditional_scores_log_density function is evaluated at the given score for
sample t.

```{r calculate weights, eval=FALSE}
# apply goes over all columns of the proposal_scores list
log_weights <- apply(proposal_scores[[1]], 2, function(scores){
  conditional_scores_log_density(scores, x_grid, x_data[[1]], pca) -
    sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
})

# lets calculate an example for t = 1
 density_at_score_1 <- conditional_scores_log_density(proposal_scores[[1]][,1], x_grid, x_data[[1]], pca)

# now what happens inside that function?
# first the clr density given sigma and the sampled scores_t is calculated
x_data_i <- x_data[[1]]
scores_1 <- proposal_scores[[1]][,1]
clr_density_2 <- cbind(x_grid, pca$center + pca$rotation%*%scores_1)
scores_2 <- proposal_scores[[1]][,2]
clr_density_2 <- cbind(x_grid, pca$center + pca$rotation%*%scores_2)
plot(clr_density_1, type = "l", main = "Density of clr(x_1)")
lines(clr_density_2, type = "l", col = "red")
# then the indices for x_i are found
idxs <- sapply(x_data_i, function(x) {
  which.min((x - x_grid) ^ 2)
})
# the clr density is then evaluated at the indices
clr_density_at_x_i_1 <- clr_density_1[idxs,2]
sum(clr_density_at_x_i_1)
# the prior is substracted
sum(0.5*scores_1^2/(pca$sdev^2))

clr_density_at_x_i_2 <- clr_density_2[idxs,2]
sum(clr_density_at_x_i_2)
# the prior is substracted
sum(0.5*scores_2^2/(pca$sdev^2))

## How do those results differ for different observations x_i?
x_data_i_2 <- x_data[[2]]
scores_1 <- proposal_scores[[1]][,1]
clr_density_2 <- cbind(x_grid, pca$center + pca$rotation%*%scores_1)
scores_2 <- proposal_scores[[1]][,2]
clr_density_2 <- cbind(x_grid, pca$center + pca$rotation%*%scores_2)
plot(clr_density_1, type = "l", main = "Density of clr(x_1)")
lines(clr_density_2, type = "l", col = "red")
# then the indices for x_i are found
idxs <- sapply(x_data_i_2, function(x) {
  which.min((x - x_grid) ^ 2)
})
# the clr density is then evaluated at the indices
clr_density_at_x_i_1 <- clr_density_1[idxs,2]
sum(clr_density_at_x_i_1)
# the prior is substracted
sum(0.5*scores_1^2/(pca$sdev^2))

clr_density_at_x_i_2 <- clr_density_2[idxs,2]
sum(clr_density_at_x_i_2)
# the prior is substracted
sum(0.5*scores_2^2/(pca$sdev^2))


```

At the end of that the expected complete-data log likelihood is approximated as a weighted sum of the sampled scores resulting in the most likely score (`mu_scores`) 

### Maximization step


Given the weights $\omega_{i t}$ we can calculate the conditional expectation:
$$
\begin{equation*}
\mathbb{E}\left(\log \left(p\left(\boldsymbol{\theta}_{i} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right)\right) \approx \sum_{t=1}^{r} \frac{\omega_{i t}}{\sum_{t=1}^{r} \omega_{i t}} \log \left(p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right) \tag{8}
\end{equation*}
$$

where the distribution of $\theta_{i}$ is given by the calculated distribution of scores $z_{i}^{(t)}$.
This equals the approximation of conditional expectation of scores (n-dimensional Vektor) given the current parameters $\nu$ and $\Sigma$.

```{r maximization step, eval=TRUE}
# M-Step ###################
mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
  proposal_scores[[i]]%*%weights[[i]]
}))
```

Given the vector of scores $\theta_{i}$, we can calculate all relevant parameters of a typical principal component analysis as well as an approximation
of the original data. TODO

### Update parameters

For the next iteration of the MCEM-algorithm we update the parameters $\nu$ and $\Sigma$:

$$
\begin{aligned}
\boldsymbol{\nu}^{(h+1)} & =\frac{1}{\sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}} \sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t} \boldsymbol{\theta}_{i}^{(t)} \\
\boldsymbol{\Sigma}^{(h+1)} & =\frac{1}{\sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}} \sum_{i=1}^{n} \sum_{t=1}^{r} \omega_{i t}\left(\boldsymbol{\theta}_{i}^{(t)}-\boldsymbol{\nu}^{(h+1)}\right)\left(\boldsymbol{\theta}_{i}^{(t)}-\boldsymbol{\nu}^{(h+1)}\right)^{T}
\end{aligned}
$$

```{r update parameters, eval=FALSE}
pca$center_updated <- center_function(cbind(x_grid, pca$center + pca$rotation%*%mu_scores))[,2]

Sigma <- Reduce("+", lapply(seq_along(weights), function(i){
  Reduce("+", lapply(1:(r*k), function(t){
    C_it <- weights[[i]][t]*(proposal_scores[[i]][,t] - mu_scores)%*%
      t((proposal_scores[[i]][,t] - mu_scores))
  }))
}))/length(weights)
eigen_decomp <- eigen(Sigma)
pca$sdev <- sqrt(eigen_decomp$values)
pca$rotation <- pca$rotation%*%eigen_decomp$vectors
pca$rotation <- apply(pca$rotation, 2, function(g) center_function(cbind(x_grid, g))[,2])
      
```


### Weighted likelihood estimation

The core of the Expectation step is the approximation of the conditional expectation:
$$
\begin{equation*}
Q\left(\boldsymbol{\nu}, \boldsymbol{\Sigma} \mid \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right) \approx \sum_{i=1}^{n} \sum_{t=1}^{r} \frac{\omega_{i t}}{\sum_{t=1}^{r} \omega_{i t}} \log \left(p\left(\boldsymbol{\theta}_{i}^{(t)} \mid \boldsymbol{\nu}, \boldsymbol{\Sigma}\right)\right)+\text { const. } \tag{10}
\end{equation*}
$$

where every weight is a weighted sample t from the distribution of scores conditional on $x_1$ and the current parameters $\nu$ and $\Sigma$ over the auxiliary distribution, which is denoted as 
`conditional_scores_log_density`.

This function calculates $\theta_i$ (`clr_density`) as a function of the scores $z_i$ and $\mu$ and $V$. $\theta_i$ is evaluated on an approximationof the compact intervall $[0,1]$.
In the compositional case, this would be a D-dimensional vector.
This function is then evaluated at `idxs`, i.e. the index of the closest point in the grid to the respective data point in $x_i$.
Then Formular 9 is evaluted with $\theta$ at each index of `idxs`over the integral of all the points in the grid.
Here the clr-transformation is given by:
$$
\begin{equation*}
\operatorname{clr}^{-1}(g)=\frac{\exp (g)}{\int_{I} \exp (g(x)) d x} \tag{4}
\end{equation*}
$$

For doing the transformation from the original data to the compositional data, the open question is how we translate the product over $m_i$ times the inverse clr-transformation, 
which is:
$\mathbf{x}=\mathscr{C}\left[\exp \left(\mathbf{x}^{*}\right)\right]$ (Delgado et al. 2013, p.441).

Another question is how the information of $x_i$ is actually incorporated into the `conditional_scores_log_density` function.

```{r weighted likelihood estimation 2, eval=FALSE}

# objective function
conditional_scores_log_density <- function(scores, x_grid, x_data_i, pca){
  clr_density <- cbind(x_grid, pca$center + pca$rotation%*%scores)
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  f_integral <- sum(exp(clr_density[,2])*diff(mid_points))
  
  sum(clr_density[idxs, 2]) - length(idxs)*log(f_integral) - sum(0.5*scores^2/(pca$sdev^2))
}

```

After drawing from the auxiliary distribution, we calculate the weights $\omega_{i t}$ as the ratio of the conditional density of the scores and the draws from auxiliary distribution  $p_{i}^{*}$.

```{r weighted likelihood estimation 3, eval=FALSE}
        log_weights <- apply(proposal_scores[[i]], 2, function(scores){
          # Formular 8
          conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
            sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
        })
```

The weights are finally multiplied with the proposal scores, which is equivalent to the weighted likelihood estimation of the scores.

```{r weighted likelihood estimation 4, eval=FALSE}
      mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
        proposal_scores[[i]]%*%weights[[i]]
      }))
```

### The gradient

The gradient of the objective function is given by:

$$
\nabla \log \left(p\left(\mathbf{z}_{i} \mid \boldsymbol{x}_{i}, \boldsymbol{\nu}^{(h)}, \boldsymbol{\Sigma}^{(h)}\right)\right)=\sum_{k=1}^{N} \boldsymbol{v}_{k}^{(h)}\left(\sum_{j=1}^{m_{i}} e_{k}\left(x_{i j}\right)-m_{i}\left\langle f_{\mathbf{z}_{i}}, e_{k}\right\rangle_{\mathbb{L}_{2}}\right)-\left(\frac{z_{i l}}{\sigma_{l}^{2^{(h)}}}\right)_{l=1, \ldots, N}
$$

This is computed as: 

```{r gradient, eval=FALSE}
gradient_csld <- function(scores, x_grid, x_data_i, pca){
  idxs <- sapply(x_data_i, function(x) {
    which.min((x - x_grid) ^ 2)
  })
  mid_points <- c(x_grid[1], x_grid[-1] - 0.5*diff(x_grid), x_grid[length(x_grid)])
  density <- inverse_clr_trafo(cbind(x_grid, pca$center + pca$rotation%*%scores))
  
  sapply(seq_along(scores), function(k){
    scalar_prod <- sum(density[,2]*pca$rotation[, k]*diff(mid_points))
    sum(pca$rotation[idxs, k]) - length(idxs)*scalar_prod - scores[k]/(pca$sdev[k]^2)
  })
}
```

### Toy example

For a given sample $i$ the mode is calculated as a N-Dimensional vector:

```{r toy example, eval=TRUE}
plot(scores_median)
abline(h = 0)
```

From this mode, the scores are drawn from a normal distribution with the mode as mean and the diagonal matrix of eigenvalues from $\sigma$ as the covariance matrix.
This is stored in `proposal_scores` as a $Nxr$ matrix containing $r$ draws. Since the eigenvalues are sorted in descending order, the proposal scores are more spread for the first
components (out of N components).

```{r toy example 2, eval=TRUE}
proposal_scores[[1]] <- sapply(1:(r*k), function(t){
  matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
})

num_columns <- ncol(proposal_scores[[1]])

# Create a new plot window
plot(NULL, xlim = c(1, nrow(proposal_scores[[1]])), ylim = range(proposal_scores[[1]]),
     xlab = "Index", ylab = "Value", main = "Column Plots")

# Plot each column
for (i in 1:num_columns) {
  points(proposal_scores[[1]][, i], col = i)
}

# Add a legend
legend("topright", legend = paste("Column", 1:num_columns),
       col = 1:num_columns, lty = 1)
```

Those proposal scores are then evaluated at the at the respective indexes for the distribution of $theta_i$ given the initial parameters $\nu$ and $\Sigma$ and their 
**eigendecomposition**.
The "Nenner" of the weights is calculated as the sum of the probability of each proposal score given the auxiliary distribution.

```{r toy example 3, eval=TRUE}
log_weights <- apply(proposal_scores[[i]], 2, function(scores){
  # Formular 8
  conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
    sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
})
```

The weights are an estimator of the likelihood of the proposed scores  for each i.

Finally, in the maximisation step the conditional expectation of the scores is estimated as mean of the weighted scores.

```{r toy example 4, eval=TRUE}
mu_scores <- rowMeans(sapply(seq_along(weights), function(i){
  proposal_scores[[i]]%*%weights[[i]]
}))

plot(mu_scores)
```

Open question: Since all calculations are done on the latent transformed densities $g_{i}=\mu^{(h)}+\sum_{\underline{k=1}}^{N} \boldsymbol{z}_{i}^{T} \boldsymbol{v}_{k}^{(h)} e_{k}, i=1, \ldots, n$ and
we set initial values for $\mu^{(h)}$ and $\Sigma^{(h)}$, when do we ever take $x_i$ into account?


# Simulation study compositional data

We try to transform the MCEM algorithm to the compositional case. Firstly, we need some data. In a first approach the 
data is generated using the orthonormal bases $e_k$ from Egozcue et al. 2003 to construct the principal components.

```{r simulation compositional data, eval=TRUE}

# define number of components
n_components <- 13
x_grid <- seq(1, n_components)
# make the neutral element the center for a start
raw_data <- data.frame("x" = x_grid, "y" = rep(0, n_components))

center_function_comp <- function(comp_data){
  mean <- mean(comp_data[,2])
  comp_data[,2] <- comp_data[,2] - mean
  comp_data
}

clr_mean <- center_function_comp(raw_data)

# zwei Hauptkomponenten im clr-Raum
# PC1 is a balancing effect between the the first five elements and the sixths element
# alternative: use Egozcue et al. (2003) orthogonal basis
pc_1 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(5/(5+1)*5^(-1)), 5),-1, rep(0, 7)))
pc_1 <- center_function_comp(pc_1)
# centering resolves in the sum of all elements being zero
# Normalisierung
pc_1[,2] <- pc_1[,2]/norm(pc_1[,2],type="2")


pc_2 <- data.frame("x" = x_grid, "y" = c(rep(sqrt(12/(12+1)*12^(-1)), 12),0))
pc_2 <- center_function_comp(pc_2)
pc_2[,2] <- pc_2[,2]/norm(pc_2[,2], type="2")

lambda_1 <- 0.5
lambda_2 <- 0.2

# Schritt 2: Simulieren Sie die Scores und konstruieren Sie die Dichten
n_data <- 30
true_observed_clr_comp <- sapply(1:n_data, function(i){
  clr_mean[,2] + rnorm(1, 0, lambda_1)*pc_1[,2] + rnorm(1, 0, lambda_2)*pc_2[,2]
})

# error checking: Sum to zero
check_columns_sum_to <- function(data, integer) {
  n_cols <- ncol(data)
  
  columns_sum_to_zero <- logical(n_cols)
  
  for (i in 1:n_cols) {
    column_sum <- sum(data[, i])
    columns_sum_to_zero[i] <- (column_sum < integer)
  }
  
  return(columns_sum_to_zero)
}
check_columns_sum_to(true_observed_clr_comp, 0.001)

inverse_clr_trafo <- function(clr_density){
  f_integral <- sum(exp(clr_density[,2]))
  data.frame("x" = clr_density[,1], "y" = exp(clr_density[,2])/f_integral)
}

true_observed_comp <- lapply(1:n_data, function(i){
  # TODO: remove x_grid
  clr_density <- data.frame(x_grid, true_observed_clr_comp[,i])
  inverse_clr_trafo(clr_density)
})

# Schritt 3: Datenpunkte aus den Dichten ziehen
# Define number of counts when dealing with multinomial distributions
n_counts <- 2000
n_samples <- 40
# change the structure to a list of compositions
# x_data <- lapply(1:n_data, function(i){
#   probs <- true_observed_densities[[i]][,2]
#   t(rmultinom(n_samples, n_counts, probs))
# })
x_data <- unlist(lapply(1:n_data, function(i) {
  probs <- true_observed_comp[[i]][,2]
  # TODO: needs to be redesigned since X is modelled as a latent process G_i and not as Multinomial
  samples <- t(rmultinom(n_samples, n_counts, probs))
  lapply(1:nrow(samples), function(j) samples[j,])
}), recursive = FALSE)

# turn x_data into a matrix
# TODO: Why do I need a list at all? 
x_data_matrix <- do.call(rbind, x_data)


```

With this simplified data, we can run the MCEM algorithm, which should produce similar results to a classic pca:

```{r classic pca, eval=TRUE}
library(compositions)

x_acomp <- acomp(x_data_matrix)
x_clr <- clr(x_acomp)
# dies ist equivalent zur Summe über k bis D-1 con clr(x_acomp[,k]*e_k))
# TODO

# # Schritt 4: Dichte-PCA schätzen

# pcx <- princomp(x_acomp)
# plot(pcx)
# biplot(pcx)
# pcx$loadings
# alternative without compositions
pca <- prcomp(na.omit(x_clr))
plot(pca)
biplot(pca)

```

The key for the transformation is the adjustment of the objective function and its gradient:

```{r adjusted conditional scores function, eval=FALSE}
# TODO: use an easier example with four components or so
D <- 13
# to check computation we need a random vector of scores
# we can just choose a vector of pcx$scores
scores <- as.vector(pcx$scores[1,])
x_data_i <- x_data[[1]]

clr_comp <- pca$center + pca$rotation%*%scores
norm_constant <- sum(exp(clr_comp))

# 1 calculate the log likelihood part sum(clr_comp) -> sum(x_data_i * clr_comp) 
sum(x_data_i * clr_comp) - sum(x_data_i)*log(norm_constant) - sum(0.5*scores^2/(pca$sdev^2))
```

```{r adjusted gradient, eval=FALSE}
# TODO: remove grid, replace counts, 
gradient_csld_discrete <- function(scores, x_grid, x_data_i, pca) {
  # Calculate probabilities in the original space
  clr_density <- cbind(x_grid, pca$center + pca$rotation %*% scores)
  # TODO: replace with correct clr inverse 
  probs <- exp(clr_density[,2])
  probs <- probs / sum(probs)
  
  # Count the frequencies of observations
  counts <- table(factor(x_data_i, levels = x_grid))
  
  # Calculate the gradient
  sapply(seq_along(scores), function(k) {
    # Calculate the expected value of the k-th principal component
    expected_pc <- sum(probs * pca$rotation[, k])
    
    # Calculate the observed value of the k-th principal component
    observed_pc <- sum(counts / sum(counts) * pca$rotation[, k])
    
    # Gradient component
    gradient_k <- observed_pc - sum(counts) * expected_pc - scores[k] / (pca$sdev[k]^2)
    
    return(gradient_k)
  })
}

```


```{r weighted likelihood estimation, eval=FALSE}

# adjusted objective function
conditional_scores_log_density <- function(scores, x_data_i, pcx){
  # synthetic data with CoDa PCA results
  clr_comp <- pcx$center[1:D-1] + as.vector(scores%*%pcx$loadings)

  norm_constant <- sum(exp(clr_comp))
  
  # sum(clr_density[idxs, 2]) - length(idxs)*log(f_integral) - sum(0.5*scores^2/(pca$sdev^2))
  sum(clr_comp) - log(norm_constant) - sum(0.5*scores[1:12]^2/(pcx$sdev^2))
}

# adjusted gradient

for(i in 1:nrow(clr_densities)){
  optim_result <- optim(rep(0, length = length(pca$sdev)), conditional_scores_log_density, gr = gradient_csld,
                              x_grid = x_grid, x_data_i = x_data[[i]], pca = pca,
                              control = list(fnscale = -1), method = "BFGS")
  scores_median <- as.vector(optim_result$par)
  # importance sampling
  proposal_scores[[i]] <- sapply(1:(r*k), function(t){
    matrix(rnorm(length(scores_median), mean = scores_median, sd = lambda*pca$sdev))
  })
  log_weights <- apply(proposal_scores[[i]], 2, function(scores){
    conditional_scores_log_density(scores, x_grid, x_data[[i]], pca) -
      sum(dnorm(scores, mean = scores_median, sd = lambda*pca$sdev, log = TRUE))
  })
  # increase numerical stability
  log_weights <- log_weights - mean(log_weights, na.rm = TRUE)
  weights[[i]] <- exp(log_weights)/sum(exp(log_weights))
}

```